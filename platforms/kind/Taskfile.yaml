version: '3'

# Kind Tier - Hub + Experiments on Kind clusters
#
# Usage:
#   task kind:bootstrap              # Bootstrap hub cluster
#   task kind:destroy                # Destroy hub cluster
#   task kind:status                 # Show hub status
#   task kind:password               # Get ArgoCD password
#   task kind:list                   # List experiments
#   task kind:conduct -- <name>      # Run experiment (interactive)
#   task kind:up -- <name>           # Deploy experiment
#   task kind:down -- <name>         # Destroy experiment clusters

silent: true

vars:
  HUB_CLUSTER: hub
  ARGOCD_NAMESPACE: argocd
  # ENVIRONMENT CONFIG: If changing Kind network, also update:
  #   - platforms/hub/app-of-apps/kind/manifests/metallb-config/kustomization.yaml (POOL_RANGE)
  #   - platforms/hub/app-of-apps/kind/values/dns-stack.yaml (loadBalancerIP)
  #   - platforms/hub/app-of-apps/kind/values/argocd.yaml (loadBalancerIP)
  #   - platforms/hub/bootstrap/argocd-values-kind.yaml (loadBalancerIP)
  DNS_IP: 172.19.255.200
  # OpenBao persistence (use $HOME instead of ~ for reliable expansion)
  OPENBAO_KEYS_FILE: $HOME/.illmlab/openbao-keys.json
  OPENBAO_NAMESPACE: openbao
  # Infrastructure containers
  CPK_CONTAINER: cloud-provider-kind

tasks:
  # =============================================================================
  # Hub Lifecycle
  # =============================================================================
  bootstrap:
    desc: Bootstrap hub on Kind (full setup)
    cmds:
      - |
        echo "=== Preparing persistent storage ==="
        mkdir -p ~/.illmlab/openbao-data
        chmod 777 ~/.illmlab/openbao-data
      - |
        echo "=== Creating Kind cluster ==="
        kind create cluster --name {{.HUB_CLUSTER}} --config platforms/kind/cluster-config.yaml --wait 60s || true
        kubectl config use-context kind-{{.HUB_CLUSTER}}
      - |
        echo "=== Installing ArgoCD ==="
        helm repo add argo https://argoproj.github.io/argo-helm 2>/dev/null || true
        helm repo update argo
        helm upgrade --install argocd argo/argo-cd \
          -n {{.ARGOCD_NAMESPACE}} --create-namespace \
          -f platforms/hub/bootstrap/argocd-values-kind.yaml
      - |
        echo "=== Applying hub application ==="
        kubectl apply -f platforms/hub/bootstrap/hub-application.yaml
      - |
        echo "=== Waiting for services ==="
        echo "Waiting for dns-stack to be healthy..."
        until kubectl get application dns-stack -n argocd -o jsonpath='{.status.health.status}' 2>/dev/null | grep -q "Healthy"; do
          sleep 5
        done
        echo "dns-stack healthy!"

        echo "Waiting for k8s_gateway LoadBalancer..."
        until kubectl get svc dns-stack-k8s-gateway -n dns-system -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null | grep -q "{{.DNS_IP}}"; do
          sleep 2
        done
        echo "k8s_gateway ready at {{.DNS_IP}}"
      - |
        echo "=== Initializing OpenBao ==="
        echo "Waiting for OpenBao pod to be running..."
        # Wait for namespace to exist
        until kubectl get ns {{.OPENBAO_NAMESPACE}} 2>/dev/null; do
          sleep 2
        done
        # Wait for pod to be running (not Ready - it needs init/unseal first)
        until kubectl get pods -n {{.OPENBAO_NAMESPACE}} -l app.kubernetes.io/name=openbao -o jsonpath='{.items[0].status.phase}' 2>/dev/null | grep -q "Running"; do
          sleep 2
        done
        echo "OpenBao pod running!"

        # Get OpenBao address
        OPENBAO_POD=$(kubectl get pods -n {{.OPENBAO_NAMESPACE}} -l app.kubernetes.io/name=openbao -o jsonpath='{.items[0].metadata.name}')

        # Check if already initialized
        INIT_STATUS=$(kubectl exec -n {{.OPENBAO_NAMESPACE}} $OPENBAO_POD -- bao status -format=json 2>/dev/null | jq -r '.initialized' || echo "false")

        KEYS_FILE="{{.OPENBAO_KEYS_FILE}}"

        if [ "$INIT_STATUS" = "false" ]; then
          echo "Initializing OpenBao (first time setup)..."
          INIT_OUTPUT=$(kubectl exec -n {{.OPENBAO_NAMESPACE}} $OPENBAO_POD -- bao operator init -key-shares=1 -key-threshold=1 -format=json)
          echo "$INIT_OUTPUT" > "$KEYS_FILE"
          chmod 600 "$KEYS_FILE"
          echo "Keys saved to $KEYS_FILE"
        else
          echo "OpenBao already initialized"
        fi

        # Check if sealed
        SEALED=$(kubectl exec -n {{.OPENBAO_NAMESPACE}} $OPENBAO_POD -- bao status -format=json 2>/dev/null | jq -r '.sealed')

        if [ "$SEALED" = "true" ]; then
          if [ -f "$KEYS_FILE" ]; then
            echo "Unsealing OpenBao..."
            UNSEAL_KEY=$(jq -r '.unseal_keys_b64[0]' "$KEYS_FILE")
            kubectl exec -n {{.OPENBAO_NAMESPACE}} $OPENBAO_POD -- bao operator unseal "$UNSEAL_KEY" > /dev/null
            echo "OpenBao unsealed!"
          else
            echo "ERROR: OpenBao is sealed but no keys file found at $KEYS_FILE"
            exit 1
          fi
        else
          echo "OpenBao already unsealed"
        fi

        # Show status
        ROOT_TOKEN=$(jq -r '.root_token' "$KEYS_FILE" 2>/dev/null || echo "unknown")
        echo "OpenBao ready! Root token: $ROOT_TOKEN"
      - |
        echo "=== Configuring OpenBao ==="
        KEYS_FILE="{{.OPENBAO_KEYS_FILE}}"
        ROOT_TOKEN=$(jq -r '.root_token' "$KEYS_FILE")
        OPENBAO_POD=$(kubectl get pods -n {{.OPENBAO_NAMESPACE}} -l app.kubernetes.io/name=openbao -o jsonpath='{.items[0].metadata.name}')

        # Enable KV v2 secrets engine (if not already enabled)
        kubectl exec -n {{.OPENBAO_NAMESPACE}} $OPENBAO_POD -- env BAO_TOKEN="$ROOT_TOKEN" bao secrets enable -path=secret -version=2 kv 2>/dev/null || echo "KV engine already enabled"

        # Check if Cloudflare token exists in OpenBao
        CF_EXISTS=$(kubectl exec -n {{.OPENBAO_NAMESPACE}} $OPENBAO_POD -- env BAO_TOKEN="$ROOT_TOKEN" bao kv get secret/cloudflare 2>/dev/null && echo "yes" || echo "no")

        if [ "$CF_EXISTS" = "no" ]; then
          echo ""
          echo "Cloudflare API token not found in OpenBao."
          echo "To store it, run:"
          echo "  kubectl exec -n openbao \$(kubectl get pods -n openbao -l app.kubernetes.io/name=openbao -o jsonpath='{.items[0].metadata.name}') -- \\"
          echo "    env BAO_TOKEN=\"$ROOT_TOKEN\" bao kv put secret/cloudflare api-token=\"YOUR_TOKEN\""
          echo ""
        else
          echo "Cloudflare token already stored in OpenBao"
        fi

        # Wait for external-secrets namespace to exist
        echo "Waiting for external-secrets namespace..."
        until kubectl get ns external-secrets 2>/dev/null; do
          sleep 2
        done

        # Create/update openbao-token secret for ESO
        echo "Configuring ESO authentication..."
        kubectl create secret generic openbao-token \
          -n external-secrets \
          --from-literal=token="$ROOT_TOKEN" \
          --dry-run=client -o yaml | kubectl apply -f -
        echo "ESO configured with OpenBao token"
      - |
        echo "=== Configuring OpenBao PKI (Internal CA) ==="
        KEYS_FILE="{{.OPENBAO_KEYS_FILE}}"
        ROOT_TOKEN=$(jq -r '.root_token' "$KEYS_FILE")
        OPENBAO_POD=$(kubectl get pods -n {{.OPENBAO_NAMESPACE}} -l app.kubernetes.io/name=openbao -o jsonpath='{.items[0].metadata.name}')

        # Enable PKI secrets engine
        kubectl exec -n {{.OPENBAO_NAMESPACE}} $OPENBAO_POD -- \
          env BAO_TOKEN="$ROOT_TOKEN" bao secrets enable -path=pki pki 2>/dev/null || echo "PKI engine already enabled"

        # Configure max TTL (10 years for root CA)
        kubectl exec -n {{.OPENBAO_NAMESPACE}} $OPENBAO_POD -- \
          env BAO_TOKEN="$ROOT_TOKEN" bao secrets tune -max-lease-ttl=87600h pki

        # Check if root CA already exists
        CA_EXISTS=$(kubectl exec -n {{.OPENBAO_NAMESPACE}} $OPENBAO_POD -- \
          env BAO_TOKEN="$ROOT_TOKEN" bao read pki/cert/ca 2>/dev/null && echo "yes" || echo "no")

        if [ "$CA_EXISTS" = "no" ]; then
          echo "Generating internal root CA..."
          kubectl exec -n {{.OPENBAO_NAMESPACE}} $OPENBAO_POD -- \
            env BAO_TOKEN="$ROOT_TOKEN" bao write pki/root/generate/internal \
            common_name="illmlab-internal-ca" \
            ttl=87600h \
            key_bits=4096
        else
          echo "Root CA already exists"
        fi

        # Configure CA and CRL URLs
        kubectl exec -n {{.OPENBAO_NAMESPACE}} $OPENBAO_POD -- \
          env BAO_TOKEN="$ROOT_TOKEN" bao write pki/config/urls \
          issuing_certificates="http://openbao.openbao.svc.cluster.local:8200/v1/pki/ca" \
          crl_distribution_points="http://openbao.openbao.svc.cluster.local:8200/v1/pki/crl"

        # Create role for internal certificates
        # Use allow_any_name=true to support all internal service name patterns
        # (e.g., argocd-server, argocd-server.argocd, argocd-server.argocd.svc)
        kubectl exec -n {{.OPENBAO_NAMESPACE}} $OPENBAO_POD -- \
          env BAO_TOKEN="$ROOT_TOKEN" bao write pki/roles/internal-tls \
          allow_any_name=true \
          enforce_hostnames=true \
          max_ttl=720h \
          key_bits=2048

        echo "OpenBao PKI configured!"

        # Create token for cert-manager (scoped to PKI)
        echo "Creating cert-manager PKI token..."
        PKI_POLICY='path "pki/*" { capabilities = ["create", "read", "update", "delete", "list"] }'
        kubectl exec -n {{.OPENBAO_NAMESPACE}} $OPENBAO_POD -- \
          env BAO_TOKEN="$ROOT_TOKEN" sh -c "echo '$PKI_POLICY' | bao policy write pki-policy -" 2>/dev/null || true

        PKI_TOKEN=$(kubectl exec -n {{.OPENBAO_NAMESPACE}} $OPENBAO_POD -- \
          env BAO_TOKEN="$ROOT_TOKEN" bao token create -policy=pki-policy -ttl=87600h -format=json | jq -r '.auth.client_token')

        # Create secret for cert-manager
        kubectl create secret generic openbao-pki-token \
          -n cert-manager \
          --from-literal=token="$PKI_TOKEN" \
          --dry-run=client -o yaml | kubectl apply -f -
        echo "cert-manager PKI token configured"

        # Export CA certificate for clients to trust
        echo "Exporting internal CA certificate..."
        CA_CERT=$(kubectl exec -n {{.OPENBAO_NAMESPACE}} $OPENBAO_POD -- \
          env BAO_TOKEN="$ROOT_TOKEN" bao read -field=certificate pki/cert/ca)

        # Create CA secret in argocd namespace for webhook-relay
        kubectl create secret generic openbao-internal-ca \
          -n argocd \
          --from-literal=ca.crt="$CA_CERT" \
          --dry-run=client -o yaml | kubectl apply -f -
        echo "Internal CA certificate exported"
      - |
        echo "=== Configuring host DNS ==="
        if grep -q "{{.DNS_IP}}" /etc/resolv.conf 2>/dev/null; then
          echo "Already configured"
        else
          if sudo -n true 2>/dev/null; then
            if grep -q "nameserver" /etc/resolv.conf; then
              sudo sed -i '0,/nameserver/s/nameserver/nameserver {{.DNS_IP}}\nnameserver/' /etc/resolv.conf
            else
              echo "nameserver {{.DNS_IP}}" | sudo tee -a /etc/resolv.conf
            fi
            echo "DNS configured"
          else
            echo "Skipped (needs sudo). Run manually:"
            echo "  echo 'nameserver {{.DNS_IP}}' | sudo tee -a /etc/resolv.conf"
          fi
        fi
      - |
        echo ""
        echo "=== Bootstrap Complete ==="
        kubectl get applications -n {{.ARGOCD_NAMESPACE}}
        echo ""
        echo "ArgoCD UI: https://argocd-server.argocd.k8s.local"
        echo "Password:  $(kubectl -n {{.ARGOCD_NAMESPACE}} get secret argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d)"

  destroy:
    desc: Destroy Kind hub cluster
    cmds:
      - kind delete cluster --name {{.HUB_CLUSTER}}

  status:
    desc: Show Kind hub status
    cmds:
      - |
        echo "=== Kind Hub Cluster ==="
        kubectl config use-context kind-{{.HUB_CLUSTER}} 2>/dev/null || { echo "Hub cluster not found"; exit 1; }
        echo ""
        echo "=== ArgoCD Applications ==="
        kubectl get applications -n {{.ARGOCD_NAMESPACE}} 2>/dev/null || echo "ArgoCD not installed"

  password:
    desc: Get ArgoCD admin password
    cmds:
      - kubectl --context kind-{{.HUB_CLUSTER}} -n {{.ARGOCD_NAMESPACE}} get secret argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d && echo

  secret:
    desc: "Store secret in OpenBao: task kind:secret -- <path> <key>=<value> [key2=value2...]"
    cmds:
      - |
        if [ -z "{{.CLI_ARGS}}" ]; then
          echo "Usage: task kind:secret -- <path> <key>=<value> [key2=value2...]"
          echo ""
          echo "Examples:"
          echo "  task kind:secret -- cloudflare api-token=abc123"
          echo "  task kind:secret -- github token=xyz user=myuser"
          exit 1
        fi

        KEYS_FILE="{{.OPENBAO_KEYS_FILE}}"

        if [ ! -f "$KEYS_FILE" ]; then
          echo "ERROR: OpenBao keys file not found at $KEYS_FILE"
          echo "Run 'task kind:bootstrap' first."
          exit 1
        fi

        ROOT_TOKEN=$(jq -r '.root_token' "$KEYS_FILE")
        OPENBAO_POD=$(kubectl --context kind-{{.HUB_CLUSTER}} get pods -n {{.OPENBAO_NAMESPACE}} -l app.kubernetes.io/name=openbao -o jsonpath='{.items[0].metadata.name}')

        # Parse arguments: first is path, rest are key=value pairs
        ARGS=({{.CLI_ARGS}})
        SECRET_PATH="${ARGS[0]}"
        unset ARGS[0]

        # Build key=value arguments
        KV_ARGS=""
        for arg in "${ARGS[@]}"; do
          KV_ARGS="$KV_ARGS $arg"
        done

        echo "Storing secret at: secret/$SECRET_PATH"
        kubectl --context kind-{{.HUB_CLUSTER}} exec -n {{.OPENBAO_NAMESPACE}} $OPENBAO_POD -- \
          env BAO_TOKEN="$ROOT_TOKEN" bao kv put "secret/$SECRET_PATH" $KV_ARGS

        echo "Secret stored successfully!"

  cert-backup:
    desc: "Manually backup Let's Encrypt certificate to OpenBao (usually automatic via PushSecret)"
    cmds:
      - |
        KEYS_FILE="{{.OPENBAO_KEYS_FILE}}"
        if [ ! -f "$KEYS_FILE" ]; then
          echo "ERROR: OpenBao keys file not found at $KEYS_FILE"
          exit 1
        fi

        # Check if the Let's Encrypt cert exists and is ready
        CERT_STATUS=$(kubectl --context kind-{{.HUB_CLUSTER}} get certificate -n argocd argocd-server-tls-letsencrypt -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' 2>/dev/null)
        if [ "$CERT_STATUS" != "True" ]; then
          echo "ERROR: Let's Encrypt certificate not ready"
          echo "Status: $CERT_STATUS"
          echo "Wait for cert-manager to issue the certificate first."
          exit 1
        fi

        # Extract cert and key from the secret
        echo "Extracting certificate from cluster..."
        TLS_CRT=$(kubectl --context kind-{{.HUB_CLUSTER}} get secret -n argocd argocd-server-tls-letsencrypt -o jsonpath='{.data.tls\.crt}')
        TLS_KEY=$(kubectl --context kind-{{.HUB_CLUSTER}} get secret -n argocd argocd-server-tls-letsencrypt -o jsonpath='{.data.tls\.key}')

        if [ -z "$TLS_CRT" ] || [ -z "$TLS_KEY" ]; then
          echo "ERROR: Could not extract certificate data"
          exit 1
        fi

        # Store in OpenBao (base64 encoded to preserve formatting)
        ROOT_TOKEN=$(jq -r '.root_token' "$KEYS_FILE")
        OPENBAO_POD=$(kubectl --context kind-{{.HUB_CLUSTER}} get pods -n {{.OPENBAO_NAMESPACE}} -l app.kubernetes.io/name=openbao -o jsonpath='{.items[0].metadata.name}')

        echo "Storing certificate in OpenBao..."
        kubectl --context kind-{{.HUB_CLUSTER}} exec -n {{.OPENBAO_NAMESPACE}} $OPENBAO_POD -- \
          env BAO_TOKEN="$ROOT_TOKEN" bao kv put secret/tls/argocd \
          "tls.crt=$TLS_CRT" \
          "tls.key=$TLS_KEY"

        echo "Certificate backed up to OpenBao at secret/tls/argocd"
        echo ""
        echo "ESO will now sync this to argocd-server-tls secret."
        echo "On next cluster cycle, the cert will be restored automatically."

  cert-status:
    desc: "Show TLS certificate status"
    cmds:
      - |
        echo "=== Let's Encrypt Certificate (staging) ==="
        kubectl --context kind-{{.HUB_CLUSTER}} get certificate -n argocd argocd-server-tls-letsencrypt 2>/dev/null || echo "Not found"

        echo ""
        echo "=== ArgoCD TLS Secret (ESO-managed) ==="
        kubectl --context kind-{{.HUB_CLUSTER}} get externalsecret -n argocd argocd-server-tls 2>/dev/null || echo "Not found"
        kubectl --context kind-{{.HUB_CLUSTER}} get secret -n argocd argocd-server-tls 2>/dev/null || echo "Not found"

        echo ""
        echo "=== Certificate Details ==="
        kubectl --context kind-{{.HUB_CLUSTER}} get secret -n argocd argocd-server-tls -o json 2>/dev/null | \
          jq -r '.data["tls.crt"]' | base64 -d | \
          openssl x509 -noout -subject -issuer -dates 2>/dev/null || echo "No valid certificate"

  # =============================================================================
  # Experiment Lifecycle
  # =============================================================================
  list:
    desc: List available experiments
    cmds:
      - tree experiments/scenarios -L 1 -d --noreport 2>/dev/null || ls -d experiments/scenarios/*/

  up:
    desc: "Deploy experiment: task kind:up -- <name>"
    cmds:
      - task: _ensure-cloud-provider-kind
      - |
        if [ -z "{{.CLI_ARGS}}" ]; then
          echo "Usage: task kind:up -- <experiment-name>"
          echo ""
          echo "Deploys experiment clusters and apps."
          echo "Use 'task kind:down -- <name>' to clean up when done."
          exit 1
        fi

        EXP_NAME="{{.CLI_ARGS}}"
        EXP_PATH="experiments/scenarios/$EXP_NAME"

        if [ ! -d "$EXP_PATH" ]; then
          echo "ERROR: Experiment not found at $EXP_PATH"
          exit 1
        fi

        echo "=============================================="
        echo "  DEPLOYING EXPERIMENT: $EXP_NAME"
        echo "=============================================="
        echo ""

        # Step 1: Discover clusters
        echo "=== Step 1/4: Discovering clusters ==="
        CLUSTERS=""
        for cluster_dir in "$EXP_PATH"/*/; do
          if [ -f "$cluster_dir/cluster.yaml" ]; then
            cluster_name=$(basename "$cluster_dir")
            CLUSTERS="$CLUSTERS $cluster_name"
            echo "  Found: $cluster_name"
          fi
        done

        if [ -z "$CLUSTERS" ]; then
          echo "ERROR: No clusters found (no */cluster.yaml files)"
          exit 1
        fi

        # Step 2: Create kind clusters
        echo ""
        echo "=== Step 2/4: Creating experiment clusters ==="
        PIDS=""
        for cluster in $CLUSTERS; do
          full_name="${EXP_NAME}-${cluster}"
          if kind get clusters 2>/dev/null | grep -q "^${full_name}$"; then
            echo "  Cluster '$full_name' already exists"
          else
            echo "  Creating cluster '$full_name'..."
            kind create cluster --name "$full_name" --wait 60s &
            PIDS="$PIDS $!"
          fi
        done

        if [ -n "$PIDS" ]; then
          echo "  Waiting for cluster creation..."
          for pid in $PIDS; do
            wait $pid || { echo "ERROR: Cluster creation failed"; exit 1; }
          done
          echo "  All clusters created!"
          # Restart cloud-provider-kind to pick up new cluster credentials
          docker restart cloud-provider-kind >/dev/null 2>&1 || true
        fi

        # Step 3: Register clusters with hub ArgoCD
        echo ""
        echo "=== Step 3/4: Registering clusters with ArgoCD ==="
        for cluster in $CLUSTERS; do
          full_name="${EXP_NAME}-${cluster}"
          echo "  Registering: $full_name"

          # Update kubeconfig to use container IP (required for ArgoCD access)
          CONTAINER_NAME="${full_name}-control-plane"
          CLUSTER_IP=$(docker inspect "$CONTAINER_NAME" | jq -r '.[0].NetworkSettings.Networks.kind.IPAddress')
          kubectl config set-cluster "kind-${full_name}" --server="https://${CLUSTER_IP}:6443" >/dev/null

          # Create service account on target cluster
          kubectl --context "kind-${full_name}" create serviceaccount argocd-manager -n kube-system 2>/dev/null || true
          kubectl --context "kind-${full_name}" create clusterrolebinding argocd-manager \
            --clusterrole=cluster-admin --serviceaccount=kube-system:argocd-manager 2>/dev/null || true

          # Create token secret
          cat <<EOF | kubectl --context "kind-${full_name}" apply -f -
        apiVersion: v1
        kind: Secret
        metadata:
          name: argocd-manager-token
          namespace: kube-system
          annotations:
            kubernetes.io/service-account.name: argocd-manager
        type: kubernetes.io/service-account-token
        EOF
          sleep 2

          # Remove stale cluster registration if it exists (handles recreated clusters)
          # First try kubectl (always works), then argocd CLI (if available)
          kubectl --context kind-{{.HUB_CLUSTER}} delete secret -n argocd -l argocd.argoproj.io/secret-type=cluster,name=$cluster 2>/dev/null || true
          kubectl --context kind-{{.HUB_CLUSTER}} delete secret argocd-cluster-$cluster -n argocd 2>/dev/null || true
          if argocd cluster get "$cluster" >/dev/null 2>&1; then
            echo "    Removing stale CLI registration..."
            echo "y" | argocd cluster rm "$cluster" >/dev/null 2>&1 || true
          fi

          # Add cluster to ArgoCD - try CLI first, fall back to kubectl secret
          if argocd cluster add "kind-${full_name}" --name "$cluster" \
            --kubeconfig ~/.kube/config --insecure --yes 2>/dev/null; then
            echo "    Registered via CLI"
          else
            # Fallback: create cluster secret directly
            echo "    CLI unavailable, creating secret directly..."
            TARGET_CA=$(kubectl --context "kind-${full_name}" get secret argocd-manager-token -n kube-system -o jsonpath='{.data.ca\.crt}')
            TARGET_TOKEN=$(kubectl --context "kind-${full_name}" get secret argocd-manager-token -n kube-system -o jsonpath='{.data.token}' | base64 -d)
            cat <<CLUSTERSECRET | kubectl --context kind-{{.HUB_CLUSTER}} apply -f -
        apiVersion: v1
        kind: Secret
        metadata:
          name: argocd-cluster-${cluster}
          namespace: argocd
          labels:
            argocd.argoproj.io/secret-type: cluster
        type: Opaque
        stringData:
          name: "${cluster}"
          server: "https://${CLUSTER_IP}:6443"
          config: |
            {
              "bearerToken": "${TARGET_TOKEN}",
              "tlsClientConfig": {
                "insecure": false,
                "caData": "${TARGET_CA}"
              }
            }
        CLUSTERSECRET
            echo "    Registered via secret"
          fi
        done

        # Step 3b: Load local images (if local-images.txt exists)
        LOCAL_IMAGES_FILE="$EXP_PATH/local-images.txt"
        if [ -f "$LOCAL_IMAGES_FILE" ]; then
          echo ""
          echo "=== Loading local Docker images ==="
          while read -r image; do
            [ -z "$image" ] && continue
            [[ "$image" =~ ^# ]] && continue
            if docker image inspect "$image" >/dev/null 2>&1; then
              for cluster in $CLUSTERS; do
                full_name="${EXP_NAME}-${cluster}"
                echo "  Loading $image into $full_name..."
                kind load docker-image "$image" --name "$full_name" 2>/dev/null || echo "    (failed)"
              done
            else
              echo "  WARNING: Image '$image' not found locally - build it first"
            fi
          done < "$LOCAL_IMAGES_FILE"
        fi

        # Step 4: Deploy ArgoCD apps
        echo ""
        echo "=== Step 4/4: Deploying apps via ArgoCD ==="
        for cluster in $CLUSTERS; do
          cluster_dir="$EXP_PATH/$cluster"
          argocd_dir="$cluster_dir/argocd"

          if [ -d "$argocd_dir" ] && [ -f "$argocd_dir/app.yaml" ]; then
            echo "  Applying: $cluster/argocd/app.yaml"
            kubectl --context kind-{{.HUB_CLUSTER}} apply -f "$argocd_dir/app.yaml"
          fi
        done

        echo ""
        echo "  Waiting for apps to sync..."
        sleep 30
      - |
        EXP_NAME="{{.CLI_ARGS}}"
        EXP_PATH="experiments/scenarios/$EXP_NAME"

        echo ""
        echo "=============================================="
        echo "  EXPERIMENT DEPLOYED: $EXP_NAME"
        echo "=============================================="

        # Show access information
        for cluster_dir in "$EXP_PATH"/*/; do
          if [ -f "$cluster_dir/cluster.yaml" ]; then
            cluster=$(basename "$cluster_dir")
            full_name="${EXP_NAME}-${cluster}"
            CONTAINER_NAME="${full_name}-control-plane"
            CLUSTER_IP=$(docker inspect "$CONTAINER_NAME" 2>/dev/null | jq -r '.[0].NetworkSettings.Networks.kind.IPAddress')

            echo ""
            echo "Cluster: $full_name"
            echo "  Context: kind-${full_name}"
            echo "  API: https://${CLUSTER_IP}:6443"

            # Check for LoadBalancer services first, then NodePort
            GRAFANA_LB=$(kubectl --context "kind-${full_name}" get svc -A -o json 2>/dev/null | \
              jq -r '.items[] | select(.metadata.name | contains("grafana")) | select(.spec.type=="LoadBalancer") | .status.loadBalancer.ingress[0].ip // empty' | head -1)
            PROM_LB=$(kubectl --context "kind-${full_name}" get svc -A -o json 2>/dev/null | \
              jq -r '.items[] | select(.metadata.name | contains("prometheus") and (.metadata.name | contains("operator") | not)) | select(.spec.type=="LoadBalancer") | .status.loadBalancer.ingress[0].ip // empty' | head -1)

            if [ -n "$GRAFANA_LB" ]; then
              echo "  Grafana: http://${GRAFANA_LB}:80 (admin/admin)"
            fi
            if [ -n "$PROM_LB" ]; then
              echo "  Prometheus: http://${PROM_LB}:9090"
            fi

            # Fallback to NodePort if no LoadBalancer
            if [ -z "$GRAFANA_LB" ]; then
              GRAFANA_PORT=$(kubectl --context "kind-${full_name}" get svc -A -o json 2>/dev/null | \
                jq -r '.items[] | select(.metadata.name | contains("grafana")) | .spec.ports[0].nodePort // empty' | head -1)
              if [ -n "$GRAFANA_PORT" ]; then
                echo "  Grafana: http://${CLUSTER_IP}:${GRAFANA_PORT} (admin/admin)"
              fi
            fi
            if [ -z "$PROM_LB" ]; then
              PROM_PORT=$(kubectl --context "kind-${full_name}" get svc -A -o json 2>/dev/null | \
                jq -r '.items[] | select(.metadata.name | contains("prometheus") and (.metadata.name | contains("operator") | not)) | select(.spec.type=="NodePort") | .spec.ports[0].nodePort // empty' | head -1)
              if [ -n "$PROM_PORT" ]; then
                echo "  Prometheus: http://${CLUSTER_IP}:${PROM_PORT}"
              fi
            fi
          fi
        done

        echo ""
        echo "To clean up: task kind:down -- $EXP_NAME"
        echo ""

  conduct:
    desc: "Run experiment: task kind:conduct -- <name> [USERS=10] [DURATION=60s]"
    cmds:
      - task: _ensure-cloud-provider-kind
      - |
        if [ -z "{{.CLI_ARGS}}" ]; then
          echo "Usage: task kind:conduct -- <experiment-name> [USERS=10] [DURATION=60s]"
          exit 1
        fi

        EXP_NAME="{{.CLI_ARGS}}"
        EXP_PATH="experiments/scenarios/$EXP_NAME"
        WORKFLOW_FILE="$EXP_PATH/workflow/experiment.yaml"

        if [ ! -d "$EXP_PATH" ]; then
          echo "ERROR: Experiment not found at $EXP_PATH"
          exit 1
        fi

        # Check for tutorial.yaml or workflow file
        TUTORIAL_FILE="$EXP_PATH/tutorial.yaml"
        if [ ! -f "$TUTORIAL_FILE" ] && [ ! -f "$WORKFLOW_FILE" ]; then
          echo "ERROR: No tutorial.yaml or workflow/experiment.yaml found"
          exit 1
        fi

        USERS={{.USERS | default "10"}}
        DURATION={{.DURATION | default "60s"}}

        echo "=============================================="
        echo "  CONDUCTING EXPERIMENT: $EXP_NAME"
        echo "=============================================="
        echo "  Users: $USERS"
        echo "  Duration: $DURATION"
        echo ""

        # Step 1: Discover clusters from experiment folders
        echo "=== Step 1/6: Discovering clusters ==="
        CLUSTERS=""
        HAS_ORCHESTRATOR=false
        for cluster_dir in "$EXP_PATH"/*/; do
          if [ -f "$cluster_dir/cluster.yaml" ]; then
            cluster_name=$(basename "$cluster_dir")
            CLUSTERS="$CLUSTERS $cluster_name"
            echo "  Found: $cluster_name"
            if [ "$cluster_name" = "orchestrator" ]; then
              HAS_ORCHESTRATOR=true
            fi
          fi
        done

        if [ -z "$CLUSTERS" ]; then
          echo "ERROR: No clusters found (no */cluster.yaml files)"
          exit 1
        fi

        if [ "$HAS_ORCHESTRATOR" = "true" ]; then
          echo "  Mode: Orchestrator pattern (workflow runs on orchestrator)"
        else
          echo "  Mode: Hub pattern (workflow runs on hub)"
        fi

        # Step 2: Create kind clusters IN PARALLEL
        echo ""
        echo "=== Step 2/6: Creating experiment clusters (parallel) ==="
        PIDS=""
        for cluster in $CLUSTERS; do
          full_name="${EXP_NAME}-${cluster}"
          if kind get clusters 2>/dev/null | grep -q "^${full_name}$"; then
            echo "  Cluster '$full_name' already exists"
          else
            echo "  Creating cluster '$full_name'..."
            kind create cluster --name "$full_name" --wait 60s &
            PIDS="$PIDS $!"
          fi
        done

        # Wait for all cluster creations
        if [ -n "$PIDS" ]; then
          echo "  Waiting for parallel cluster creation..."
          for pid in $PIDS; do
            wait $pid || { echo "ERROR: Cluster creation failed (pid $pid)"; exit 1; }
          done
          echo "  All clusters created!"
          # Restart cloud-provider-kind to pick up new cluster credentials
          docker restart cloud-provider-kind >/dev/null 2>&1 || true
        fi

        # Step 3: Bootstrap orchestrator if present
        echo ""
        echo "=== Step 3/6: Bootstrapping orchestrator ==="
        if [ "$HAS_ORCHESTRATOR" = "true" ]; then
          ORCH_NAME="${EXP_NAME}-orchestrator"
          ORCH_CONTEXT="kind-${ORCH_NAME}"

          echo "  Installing ArgoCD on orchestrator..."
          helm repo add argo https://argoproj.github.io/argo-helm 2>/dev/null || true
          helm repo update argo >/dev/null
          helm upgrade --install argocd argo/argo-cd \
            --kube-context "$ORCH_CONTEXT" \
            -n argocd --create-namespace \
            --set server.service.type=NodePort \
            --set configs.params."server\.insecure"=true \
            --wait --timeout 3m

          echo "  Installing Argo Workflows on orchestrator..."
          helm upgrade --install argo-workflows argo/argo-workflows \
            --kube-context "$ORCH_CONTEXT" \
            -n argo-workflows --create-namespace \
            --set controller.containerRuntimeExecutor=emissary \
            --set controller.workflowDefaults.spec.serviceAccountName=argo-workflow \
            --set server.enabled=true \
            --set server.authMode=server \
            --set server.serviceType=NodePort \
            --set workflow.serviceAccount.create=true \
            --set workflow.serviceAccount.name=argo-workflow \
            --set workflow.rbac.create=true \
            --wait --timeout 3m

          echo "  Orchestrator bootstrapped!"
        else
          echo "  No orchestrator cluster - using hub"
        fi

        # Step 4: Register clusters with ArgoCD
        echo ""
        echo "=== Step 4/6: Registering clusters with ArgoCD ==="

        # Determine which cluster runs ArgoCD
        if [ "$HAS_ORCHESTRATOR" = "true" ]; then
          ARGOCD_CONTEXT="kind-${EXP_NAME}-orchestrator"
          ARGOCD_CLUSTER="${EXP_NAME}-orchestrator"
        else
          ARGOCD_CONTEXT="kind-{{.HUB_CLUSTER}}"
          ARGOCD_CLUSTER="{{.HUB_CLUSTER}}"
        fi

        for cluster in $CLUSTERS; do
          # Skip registering orchestrator with itself
          if [ "$cluster" = "orchestrator" ] && [ "$HAS_ORCHESTRATOR" = "true" ]; then
            continue
          fi

          full_name="${EXP_NAME}-${cluster}"
          echo "  Registering: $full_name with ArgoCD on $ARGOCD_CLUSTER"

          CLUSTER_CONTEXT="kind-${full_name}"
          CONTAINER_NAME="${full_name}-control-plane"
          CLUSTER_IP=$(docker inspect "$CONTAINER_NAME" | jq -r '.[0].NetworkSettings.Networks.kind.IPAddress')
          CLUSTER_SERVER="https://${CLUSTER_IP}:6443"

          # Remove any stale CLI-based cluster registration (prevents duplicates)
          if argocd cluster get "$cluster" >/dev/null 2>&1; then
            echo "    Removing stale CLI registration..."
            echo "y" | argocd cluster rm "$cluster" >/dev/null 2>&1 || true
          fi

          kubectl --context "$CLUSTER_CONTEXT" create serviceaccount argocd-manager -n kube-system 2>/dev/null || true
          kubectl --context "$CLUSTER_CONTEXT" create clusterrolebinding argocd-manager --clusterrole=cluster-admin --serviceaccount=kube-system:argocd-manager 2>/dev/null || true

          cat > /tmp/sa-token.yaml << 'ENDOFFILE'
        apiVersion: v1
        kind: Secret
        metadata:
          name: argocd-manager-token
          namespace: kube-system
          annotations:
            kubernetes.io/service-account.name: argocd-manager
        type: kubernetes.io/service-account-token
        ENDOFFILE
          kubectl --context "$CLUSTER_CONTEXT" apply -f /tmp/sa-token.yaml
          sleep 2
          TOKEN=$(kubectl --context "$CLUSTER_CONTEXT" -n kube-system get secret argocd-manager-token -o jsonpath='{.data.token}' | base64 -d)

          cat > /tmp/cluster-secret.yaml << ENDOFFILE
        apiVersion: v1
        kind: Secret
        metadata:
          name: cluster-${cluster}
          namespace: argocd
          labels:
            argocd.argoproj.io/secret-type: cluster
        type: Opaque
        stringData:
          name: "${cluster}"
          server: "${CLUSTER_SERVER}"
          config: |
            {
              "bearerToken": "${TOKEN}",
              "tlsClientConfig": {
                "insecure": true
              }
            }
        ENDOFFILE
          kubectl --context "$ARGOCD_CONTEXT" apply -f /tmp/cluster-secret.yaml
          echo "    Registered: $cluster -> $CLUSTER_SERVER"

          # Wait for ArgoCD to recognize the cluster (use kubectl, not argocd CLI)
          echo "    Waiting for cluster to be ready..."
          sleep 5  # Give ArgoCD a moment to process the secret
        done

        # Step 5: Deploy ArgoCD apps
        echo ""
        echo "=== Step 5/6: Deploying apps via ArgoCD ==="
        for cluster in $CLUSTERS; do
          # Skip orchestrator (it doesn't get app deployments, it runs them)
          if [ "$cluster" = "orchestrator" ]; then
            continue
          fi

          full_name="${EXP_NAME}-${cluster}"
          cluster_dir="$EXP_PATH/$cluster"
          argocd_dir="$cluster_dir/argocd"

          if [ -d "$argocd_dir" ] && [ -f "$argocd_dir/app.yaml" ]; then
            echo "  Applying: app.yaml -> $full_name"
            kubectl --context "$ARGOCD_CONTEXT" apply -f "$argocd_dir/app.yaml"

            APP_NAME=$(grep 'name:' "$argocd_dir/app.yaml" | head -1 | awk '{print $2}')
            if [ -n "$APP_NAME" ]; then
              # Try argocd CLI for refresh/sync (optional - works if port-forward is active)
              # Falls back to auto-sync if CLI unavailable
              if argocd app get "$APP_NAME" --hard-refresh >/dev/null 2>&1; then
                echo "  Refreshing: $APP_NAME (via CLI)"
                sleep 3
                echo "  Syncing: $APP_NAME (via CLI)"
                argocd app sync "$APP_NAME" --force --prune --timeout 300 2>&1 | tail -3 || true
              else
                echo "  ArgoCD CLI unavailable - using auto-sync (this is fine)"
                echo "  Waiting for ArgoCD to detect and sync the app..."
                sleep 10
              fi

              # Wait for app to be healthy using kubectl (doesn't require argocd CLI)
              echo "  Waiting for app to be healthy..."
              for i in $(seq 1 60); do
                HEALTH=$(kubectl --context "$ARGOCD_CONTEXT" get application "$APP_NAME" -n argocd -o jsonpath='{.status.health.status}' 2>/dev/null || echo "Unknown")
                SYNC=$(kubectl --context "$ARGOCD_CONTEXT" get application "$APP_NAME" -n argocd -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "Unknown")
                if [ "$HEALTH" = "Healthy" ] && [ "$SYNC" = "Synced" ]; then
                  echo "    App is Healthy and Synced!"
                  break
                fi
                echo "    Status: $SYNC / $HEALTH ($i/60)"
                sleep 5
              done
            fi
          fi
        done
      - |
        EXP_NAME="{{.CLI_ARGS}}"
        EXP_PATH="experiments/scenarios/$EXP_NAME"
        TUTORIAL_FILE="$EXP_PATH/tutorial.yaml"
        WORKFLOW_FILE="$EXP_PATH/workflow/experiment.yaml"

        # Check if this is a tutorial (interactive mode)
        if [ -f "$TUTORIAL_FILE" ]; then
          echo ""
          echo "=== Step 6/6: Interactive Tutorial ==="
          echo ""

          # Get target cluster context
          TARGET_CONTEXT="kind-${EXP_NAME}-target"

          # Wait for LoadBalancer IPs to be assigned
          echo "Waiting for services to get LoadBalancer IPs..."
          for i in $(seq 1 90); do
            PROM_IP=$(kubectl --context "$TARGET_CONTEXT" get svc -n observability kube-prometheus-stack-prometheus -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
            GRAFANA_IP=$(kubectl --context "$TARGET_CONTEXT" get svc -n observability kube-prometheus-stack-grafana -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
            METRICS_IP=$(kubectl --context "$TARGET_CONTEXT" get svc -n demo metrics-app -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
            STATION_IP=$(kubectl --context "$TARGET_CONTEXT" get svc -n station station-monitor -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)

            if [ -n "$PROM_IP" ] && [ -n "$GRAFANA_IP" ] && [ -n "$METRICS_IP" ]; then
              break
            fi
            echo "  Waiting... ($i/90)"
            sleep 3
          done

          if [ -z "$PROM_IP" ] || [ -z "$GRAFANA_IP" ]; then
            echo "Warning: Could not get LoadBalancer IPs"
            echo "  Use port-forward instead:"
            echo "  kubectl --context $TARGET_CONTEXT port-forward -n observability svc/kube-prometheus-stack-prometheus 9090:9090"
            echo "  kubectl --context $TARGET_CONTEXT port-forward -n observability svc/kube-prometheus-stack-grafana 3000:80"
          fi

          echo ""
          echo "=============================================="
          echo "  $(grep '^title:' "$TUTORIAL_FILE" | cut -d'"' -f2)"
          echo "=============================================="
          echo ""
          echo "  Prometheus:      http://${PROM_IP}:9090"
          echo "  Grafana:         http://${GRAFANA_IP}:80  (admin/admin)"
          echo "  Metrics App:     http://${METRICS_IP}:80"
          if [ -n "$STATION_IP" ]; then
          echo "  Station Monitor: http://${STATION_IP}:80  [SECTOR 12]"
          fi
          echo ""
          echo "----------------------------------------------"

          # Display instructions from tutorial.yaml
          sed -n '/^instructions:/,/^[a-z]/p' "$TUTORIAL_FILE" | head -n -1 | sed 's/^instructions: |//' | sed 's/^  //'

          echo ""
          echo "----------------------------------------------"
          echo ""
          echo "Quick load generation (copy & paste):"
          echo ""
          echo "  for i in {1..50}; do curl -s http://${METRICS_IP}/; curl -s http://${METRICS_IP}/slow; curl -s http://${METRICS_IP}/error; sleep 0.3; done"
          echo ""
          echo "----------------------------------------------"
          echo ""
          echo "Press ENTER when you're done exploring to clean up..."
          echo "(Or Ctrl+C to exit without cleanup)"
          echo ""

          # Wait for user input
          read -r DUMMY

          # Cleanup
          echo ""
          echo "=== Cleaning up ArgoCD resources ==="
          for cluster_dir in "$EXP_PATH"/*/; do
            if [ -f "$cluster_dir/cluster.yaml" ]; then
              cluster=$(basename "$cluster_dir")
              full_name="${EXP_NAME}-${cluster}"
              # Remove finalizer first (cluster is being deleted, no need for resource cleanup)
              kubectl --context kind-{{.HUB_CLUSTER}} patch app "$full_name" -n argocd -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
              # Delete ArgoCD app
              kubectl --context kind-{{.HUB_CLUSTER}} delete app "$full_name" -n argocd --wait=false 2>/dev/null || true
              # Delete cluster secrets (both naming patterns)
              kubectl --context kind-{{.HUB_CLUSTER}} delete secret "cluster-${cluster}" -n argocd 2>/dev/null || true
              kubectl --context kind-{{.HUB_CLUSTER}} delete secret "argocd-cluster-${cluster}" -n argocd 2>/dev/null || true
            fi
          done

          echo "=== Cleaning up experiment clusters ==="
          for cluster_dir in "$EXP_PATH"/*/; do
            if [ -f "$cluster_dir/cluster.yaml" ]; then
              cluster=$(basename "$cluster_dir")
              full_name="${EXP_NAME}-${cluster}"
              echo "  Deleting cluster: $full_name"
              kind delete cluster --name "$full_name"
            fi
          done
          kubectl config use-context kind-{{.HUB_CLUSTER}} 2>/dev/null || true
          echo ""
          echo "=============================================="
          echo "  TUTORIAL COMPLETE"
          echo "=============================================="

        else
          # Automated workflow mode
          echo ""
          echo "=== Step 6/6: Running workflow ==="

          # Determine orchestrator mode
          HAS_ORCHESTRATOR=false
          if [ -d "$EXP_PATH/orchestrator" ] && [ -f "$EXP_PATH/orchestrator/cluster.yaml" ]; then
            HAS_ORCHESTRATOR=true
          fi

          # Determine which cluster runs the workflow
          if [ "$HAS_ORCHESTRATOR" = "true" ]; then
            WORKFLOW_CONTEXT="kind-${EXP_NAME}-orchestrator"
            echo "  Running on: orchestrator"
          else
            WORKFLOW_CONTEXT="kind-{{.HUB_CLUSTER}}"
            echo "  Running on: hub"
          fi

          kubectl config use-context "$WORKFLOW_CONTEXT"

          TARGET_CONTAINER="${EXP_NAME}-target-control-plane"
          TARGET_IP=$(docker inspect "$TARGET_CONTAINER" | jq -r '.[0].NetworkSettings.Networks.kind.IPAddress')
          TARGET_URL="http://${TARGET_IP}:30080"
          echo "  Target URL: $TARGET_URL"

          USERS={{.USERS | default "10"}}
          DURATION={{.DURATION | default "60s"}}

          WORKFLOW_NAME=$(argo submit "$WORKFLOW_FILE" \
            -p users="$USERS" \
            -p duration="$DURATION" \
            -p target-url="$TARGET_URL" \
            -o name -n argo-workflows 2>/dev/null || \
            kubectl create -f "$WORKFLOW_FILE" -o name)
          echo "  Workflow: $WORKFLOW_NAME"

          echo "  Waiting for workflow completion..."
          argo wait "$WORKFLOW_NAME" -n argo-workflows 2>/dev/null || \
            kubectl wait "$WORKFLOW_NAME" --for=condition=Completed --timeout=30m -n argo-workflows

          echo ""
          echo "=== Workflow Results ==="
          argo get "$WORKFLOW_NAME" -n argo-workflows 2>/dev/null || \
            kubectl get "$WORKFLOW_NAME" -n argo-workflows -o yaml

          # Cleanup
          echo ""
          echo "=== Cleaning up ArgoCD resources ==="
          for cluster_dir in "$EXP_PATH"/*/; do
            if [ -f "$cluster_dir/cluster.yaml" ]; then
              cluster=$(basename "$cluster_dir")
              full_name="${EXP_NAME}-${cluster}"
              # Remove finalizer first (cluster is being deleted, no need for resource cleanup)
              kubectl --context kind-{{.HUB_CLUSTER}} patch app "$full_name" -n argocd -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
              # Delete ArgoCD app
              kubectl --context kind-{{.HUB_CLUSTER}} delete app "$full_name" -n argocd --wait=false 2>/dev/null || true
              # Delete cluster secrets (both naming patterns)
              kubectl --context kind-{{.HUB_CLUSTER}} delete secret "cluster-${cluster}" -n argocd 2>/dev/null || true
              kubectl --context kind-{{.HUB_CLUSTER}} delete secret "argocd-cluster-${cluster}" -n argocd 2>/dev/null || true
            fi
          done

          echo "=== Cleaning up experiment clusters ==="
          for cluster_dir in "$EXP_PATH"/*/; do
            if [ -f "$cluster_dir/cluster.yaml" ]; then
              cluster=$(basename "$cluster_dir")
              full_name="${EXP_NAME}-${cluster}"
              echo "  Deleting cluster: $full_name"
              kind delete cluster --name "$full_name"
            fi
          done

          kubectl config use-context kind-{{.HUB_CLUSTER}}

          echo ""
          echo "=============================================="
          echo "  EXPERIMENT COMPLETE"
          echo "=============================================="
        fi

  down:
    desc: "Destroy experiment clusters: task kind:down -- <name>"
    cmds:
      - |
        if [ -z "{{.CLI_ARGS}}" ]; then
          echo "Usage: task kind:down -- <experiment-name>"
          exit 1
        fi

        EXP_NAME="{{.CLI_ARGS}}"
        EXP_PATH="experiments/scenarios/$EXP_NAME"

        if [ ! -d "$EXP_PATH" ]; then
          echo "ERROR: Experiment not found at $EXP_PATH"
          exit 1
        fi

        echo "Destroying experiment: $EXP_NAME"

        # Step 1: Clean up ArgoCD resources on hub (prevents stale apps blocking future deployments)
        echo "=== Cleaning up ArgoCD resources ==="
        for cluster_dir in "$EXP_PATH"/*/; do
          if [ -f "$cluster_dir/cluster.yaml" ]; then
            cluster_name=$(basename "$cluster_dir")
            full_name="${EXP_NAME}-${cluster_name}"
            argocd_app="${full_name}"

            # Delete ArgoCD application (if exists)
            if kubectl --context kind-{{.HUB_CLUSTER}} get app "$argocd_app" -n argocd >/dev/null 2>&1; then
              echo "  Deleting ArgoCD app: $argocd_app"
              # Remove finalizer first (cluster is being deleted, no need for resource cleanup)
              kubectl --context kind-{{.HUB_CLUSTER}} patch app "$argocd_app" -n argocd -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
              kubectl --context kind-{{.HUB_CLUSTER}} delete app "$argocd_app" -n argocd --wait=false 2>/dev/null || true
            fi

            # Delete cluster secrets from ArgoCD (both naming patterns)
            kubectl --context kind-{{.HUB_CLUSTER}} delete secret "cluster-${cluster_name}" -n argocd 2>/dev/null || true
            kubectl --context kind-{{.HUB_CLUSTER}} delete secret "argocd-cluster-${cluster_name}" -n argocd 2>/dev/null || true
          fi
        done

        # Step 2: Delete Kind clusters
        echo "=== Deleting Kind clusters ==="
        for cluster_dir in "$EXP_PATH"/*/; do
          if [ -f "$cluster_dir/cluster.yaml" ]; then
            cluster_name=$(basename "$cluster_dir")
            full_name="${EXP_NAME}-${cluster_name}"
            if kind get clusters 2>/dev/null | grep -q "^${full_name}$"; then
              echo "  Deleting: $full_name"
              kind delete cluster --name "$full_name"
            fi
          fi
        done
        echo "Done."

  # =============================================================================
  # Internal Helpers
  # =============================================================================
  _ensure-cloud-provider-kind:
    internal: true
    cmds:
      - |
        if docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^{{.CPK_CONTAINER}}$"; then
          echo "  [cloud-provider-kind] Already running"
          exit 0
        fi
        docker rm -f {{.CPK_CONTAINER}} 2>/dev/null || true
        echo "  [cloud-provider-kind] Starting..."
        docker run -d \
          --name {{.CPK_CONTAINER}} \
          --network kind \
          --restart unless-stopped \
          -v /var/run/docker.sock:/var/run/docker.sock \
          registry.k8s.io/cloud-provider-kind/cloud-controller-manager:v0.6.0
        sleep 2
        docker ps --format '{{"{{"}}.Names{{"}}"}}' | grep -q "^{{.CPK_CONTAINER}}$" && echo "  [cloud-provider-kind] Running" || exit 1
