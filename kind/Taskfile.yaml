version: '3'

# Kind Tier - Hub + Experiments on Kind clusters
#
# Usage:
#   task kind:bootstrap              # Bootstrap hub cluster
#   task kind:destroy                # Destroy hub cluster
#   task kind:status                 # Show hub status
#   task kind:password               # Get ArgoCD password
#   task kind:list                   # List experiments
#   task kind:conduct -- <name>      # Run experiment
#   task kind:teardown -- <name>     # Destroy experiment clusters

silent: true

vars:
  HUB_CLUSTER: hub
  ARGOCD_NAMESPACE: argocd
  DNS_IP: 172.19.255.200
  # Infrastructure containers
  CPK_CONTAINER: cloud-provider-kind
  DNSMASQ_CONTAINER: kind-dnsmasq
  DNSMASQ_DOMAIN: k8s.local

tasks:
  # =============================================================================
  # Hub Lifecycle
  # =============================================================================
  bootstrap:
    desc: Bootstrap hub on Kind (full setup)
    cmds:
      - |
        echo "=== Creating Kind cluster ==="
        kind create cluster --name {{.HUB_CLUSTER}} --wait 60s || true
        kubectl config use-context kind-{{.HUB_CLUSTER}}
      - |
        echo "=== Installing ArgoCD ==="
        helm repo add argo https://argoproj.github.io/argo-helm 2>/dev/null || true
        helm repo update argo
        helm upgrade --install argocd argo/argo-cd \
          -n {{.ARGOCD_NAMESPACE}} --create-namespace \
          -f hub/bootstrap/argocd-values-kind.yaml
      - |
        echo "=== Applying hub application ==="
        kubectl apply -f hub/bootstrap/hub-application.yaml
      - |
        echo "=== Waiting for services ==="
        echo "Waiting for dns-stack to be healthy..."
        until kubectl get application dns-stack -n argocd -o jsonpath='{.status.health.status}' 2>/dev/null | grep -q "Healthy"; do
          sleep 5
        done
        echo "dns-stack healthy!"

        echo "Waiting for k8s_gateway LoadBalancer..."
        until kubectl get svc dns-stack-k8s-gateway -n dns-system -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null | grep -q "{{.DNS_IP}}"; do
          sleep 2
        done
        echo "k8s_gateway ready at {{.DNS_IP}}"
      - |
        echo "=== Configuring host DNS ==="
        if grep -q "{{.DNS_IP}}" /etc/resolv.conf 2>/dev/null; then
          echo "Already configured"
        else
          if sudo -n true 2>/dev/null; then
            if grep -q "nameserver" /etc/resolv.conf; then
              sudo sed -i '0,/nameserver/s/nameserver/nameserver {{.DNS_IP}}\nnameserver/' /etc/resolv.conf
            else
              echo "nameserver {{.DNS_IP}}" | sudo tee -a /etc/resolv.conf
            fi
            echo "DNS configured"
          else
            echo "Skipped (needs sudo). Run manually:"
            echo "  echo 'nameserver {{.DNS_IP}}' | sudo tee -a /etc/resolv.conf"
          fi
        fi
      - |
        echo ""
        echo "=== Bootstrap Complete ==="
        kubectl get applications -n {{.ARGOCD_NAMESPACE}}
        echo ""
        echo "ArgoCD UI: https://argocd-server.argocd.k8s.local"
        echo "Password:  $(kubectl -n {{.ARGOCD_NAMESPACE}} get secret argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d)"

  destroy:
    desc: Destroy Kind hub cluster
    cmds:
      - kind delete cluster --name {{.HUB_CLUSTER}}

  status:
    desc: Show Kind hub status
    cmds:
      - |
        echo "=== Kind Hub Cluster ==="
        kubectl config use-context kind-{{.HUB_CLUSTER}} 2>/dev/null || { echo "Hub cluster not found"; exit 1; }
        echo ""
        echo "=== ArgoCD Applications ==="
        kubectl get applications -n {{.ARGOCD_NAMESPACE}} 2>/dev/null || echo "ArgoCD not installed"

  password:
    desc: Get ArgoCD admin password
    cmds:
      - kubectl --context kind-{{.HUB_CLUSTER}} -n {{.ARGOCD_NAMESPACE}} get secret argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d && echo

  # =============================================================================
  # Experiment Lifecycle
  # =============================================================================
  list:
    desc: List available experiments
    cmds:
      - tree experiments/scenarios -L 1 -d --noreport 2>/dev/null || ls -d experiments/scenarios/*/

  conduct:
    desc: "Run experiment: task kind:conduct -- <name> [USERS=10] [DURATION=60s]"
    cmds:
      - task: _ensure-cloud-provider-kind
      - task: _ensure-dnsmasq
      - |
        if [ -z "{{.CLI_ARGS}}" ]; then
          echo "Usage: task kind:conduct -- <experiment-name> [USERS=10] [DURATION=60s]"
          exit 1
        fi

        EXP_NAME="{{.CLI_ARGS}}"
        EXP_PATH="experiments/scenarios/$EXP_NAME"
        WORKFLOW_FILE="$EXP_PATH/workflow/experiment.yaml"

        if [ ! -d "$EXP_PATH" ]; then
          echo "ERROR: Experiment not found at $EXP_PATH"
          exit 1
        fi

        if [ ! -f "$WORKFLOW_FILE" ]; then
          echo "ERROR: Workflow not found at $WORKFLOW_FILE"
          exit 1
        fi

        USERS={{.USERS | default "10"}}
        DURATION={{.DURATION | default "60s"}}

        echo "=============================================="
        echo "  CONDUCTING EXPERIMENT: $EXP_NAME"
        echo "=============================================="
        echo "  Users: $USERS"
        echo "  Duration: $DURATION"
        echo ""

        # Step 1: Discover clusters from experiment folders
        echo "=== Step 1/6: Discovering clusters ==="
        CLUSTERS=""
        HAS_ORCHESTRATOR=false
        for cluster_dir in "$EXP_PATH"/*/; do
          if [ -f "$cluster_dir/cluster.yaml" ]; then
            cluster_name=$(basename "$cluster_dir")
            CLUSTERS="$CLUSTERS $cluster_name"
            echo "  Found: $cluster_name"
            if [ "$cluster_name" = "orchestrator" ]; then
              HAS_ORCHESTRATOR=true
            fi
          fi
        done

        if [ -z "$CLUSTERS" ]; then
          echo "ERROR: No clusters found (no */cluster.yaml files)"
          exit 1
        fi

        if [ "$HAS_ORCHESTRATOR" = "true" ]; then
          echo "  Mode: Orchestrator pattern (workflow runs on orchestrator)"
        else
          echo "  Mode: Hub pattern (workflow runs on hub)"
        fi

        # Step 2: Create kind clusters IN PARALLEL
        echo ""
        echo "=== Step 2/6: Creating experiment clusters (parallel) ==="
        PIDS=""
        for cluster in $CLUSTERS; do
          full_name="${EXP_NAME}-${cluster}"
          if kind get clusters 2>/dev/null | grep -q "^${full_name}$"; then
            echo "  Cluster '$full_name' already exists"
          else
            echo "  Creating cluster '$full_name'..."
            kind create cluster --name "$full_name" --wait 60s &
            PIDS="$PIDS $!"
          fi
        done

        # Wait for all cluster creations
        if [ -n "$PIDS" ]; then
          echo "  Waiting for parallel cluster creation..."
          for pid in $PIDS; do
            wait $pid || { echo "ERROR: Cluster creation failed (pid $pid)"; exit 1; }
          done
          echo "  All clusters created!"
        fi

        # Step 3: Bootstrap orchestrator if present
        echo ""
        echo "=== Step 3/6: Bootstrapping orchestrator ==="
        if [ "$HAS_ORCHESTRATOR" = "true" ]; then
          ORCH_NAME="${EXP_NAME}-orchestrator"
          ORCH_CONTEXT="kind-${ORCH_NAME}"

          echo "  Installing ArgoCD on orchestrator..."
          helm repo add argo https://argoproj.github.io/argo-helm 2>/dev/null || true
          helm repo update argo >/dev/null
          helm upgrade --install argocd argo/argo-cd \
            --kube-context "$ORCH_CONTEXT" \
            -n argocd --create-namespace \
            --set server.service.type=NodePort \
            --set configs.params."server\.insecure"=true \
            --wait --timeout 3m

          echo "  Installing Argo Workflows on orchestrator..."
          helm upgrade --install argo-workflows argo/argo-workflows \
            --kube-context "$ORCH_CONTEXT" \
            -n argo-workflows --create-namespace \
            --set controller.containerRuntimeExecutor=emissary \
            --set controller.workflowDefaults.spec.serviceAccountName=argo-workflow \
            --set server.enabled=true \
            --set server.authMode=server \
            --set server.serviceType=NodePort \
            --set workflow.serviceAccount.create=true \
            --set workflow.serviceAccount.name=argo-workflow \
            --set workflow.rbac.create=true \
            --wait --timeout 3m

          echo "  Orchestrator bootstrapped!"
        else
          echo "  No orchestrator cluster - using hub"
        fi

        # Step 4: Register clusters with ArgoCD
        echo ""
        echo "=== Step 4/6: Registering clusters with ArgoCD ==="

        # Determine which cluster runs ArgoCD
        if [ "$HAS_ORCHESTRATOR" = "true" ]; then
          ARGOCD_CONTEXT="kind-${EXP_NAME}-orchestrator"
          ARGOCD_CLUSTER="${EXP_NAME}-orchestrator"
        else
          ARGOCD_CONTEXT="kind-{{.HUB_CLUSTER}}"
          ARGOCD_CLUSTER="{{.HUB_CLUSTER}}"
        fi

        for cluster in $CLUSTERS; do
          # Skip registering orchestrator with itself
          if [ "$cluster" = "orchestrator" ] && [ "$HAS_ORCHESTRATOR" = "true" ]; then
            continue
          fi

          full_name="${EXP_NAME}-${cluster}"
          echo "  Registering: $full_name with ArgoCD on $ARGOCD_CLUSTER"

          CLUSTER_CONTEXT="kind-${full_name}"
          CONTAINER_NAME="${full_name}-control-plane"
          CLUSTER_IP=$(docker inspect "$CONTAINER_NAME" | jq -r '.[0].NetworkSettings.Networks.kind.IPAddress')
          CLUSTER_SERVER="https://${CLUSTER_IP}:6443"

          kubectl --context "$CLUSTER_CONTEXT" create serviceaccount argocd-manager -n kube-system 2>/dev/null || true
          kubectl --context "$CLUSTER_CONTEXT" create clusterrolebinding argocd-manager --clusterrole=cluster-admin --serviceaccount=kube-system:argocd-manager 2>/dev/null || true

          cat > /tmp/sa-token.yaml << 'ENDOFFILE'
        apiVersion: v1
        kind: Secret
        metadata:
          name: argocd-manager-token
          namespace: kube-system
          annotations:
            kubernetes.io/service-account.name: argocd-manager
        type: kubernetes.io/service-account-token
        ENDOFFILE
          kubectl --context "$CLUSTER_CONTEXT" apply -f /tmp/sa-token.yaml
          sleep 2
          TOKEN=$(kubectl --context "$CLUSTER_CONTEXT" -n kube-system get secret argocd-manager-token -o jsonpath='{.data.token}' | base64 -d)

          cat > /tmp/cluster-secret.yaml << ENDOFFILE
        apiVersion: v1
        kind: Secret
        metadata:
          name: cluster-${cluster}
          namespace: argocd
          labels:
            argocd.argoproj.io/secret-type: cluster
        type: Opaque
        stringData:
          name: "${cluster}"
          server: "${CLUSTER_SERVER}"
          config: |
            {
              "bearerToken": "${TOKEN}",
              "tlsClientConfig": {
                "insecure": true
              }
            }
        ENDOFFILE
          kubectl --context "$ARGOCD_CONTEXT" apply -f /tmp/cluster-secret.yaml
          echo "    Registered: $cluster -> $CLUSTER_SERVER"
        done

        # Step 5: Deploy ArgoCD apps
        echo ""
        echo "=== Step 5/6: Deploying apps via ArgoCD ==="
        for cluster in $CLUSTERS; do
          # Skip orchestrator (it doesn't get app deployments, it runs them)
          if [ "$cluster" = "orchestrator" ]; then
            continue
          fi

          full_name="${EXP_NAME}-${cluster}"
          cluster_dir="$EXP_PATH/$cluster"
          argocd_dir="$cluster_dir/argocd"

          if [ -d "$argocd_dir" ] && [ -f "$argocd_dir/app.yaml" ]; then
            echo "  Applying: app.yaml -> $full_name"
            kubectl --context "$ARGOCD_CONTEXT" apply -f "$argocd_dir/app.yaml"
          fi
        done

        echo "  Waiting for sync..."
        sleep 15
      - task: _update-dns
      - |
        EXP_NAME="{{.CLI_ARGS}}"
        EXP_PATH="experiments/scenarios/$EXP_NAME"
        WORKFLOW_FILE="$EXP_PATH/workflow/experiment.yaml"

        # Determine orchestrator mode
        HAS_ORCHESTRATOR=false
        if [ -d "$EXP_PATH/orchestrator" ] && [ -f "$EXP_PATH/orchestrator/cluster.yaml" ]; then
          HAS_ORCHESTRATOR=true
        fi

        echo ""
        echo "=== Step 6/6: Running workflow ==="

        # Determine which cluster runs the workflow
        if [ "$HAS_ORCHESTRATOR" = "true" ]; then
          WORKFLOW_CONTEXT="kind-${EXP_NAME}-orchestrator"
          echo "  Running on: orchestrator"
        else
          WORKFLOW_CONTEXT="kind-{{.HUB_CLUSTER}}"
          echo "  Running on: hub"
        fi

        kubectl config use-context "$WORKFLOW_CONTEXT"

        TARGET_CONTAINER="${EXP_NAME}-target-control-plane"
        TARGET_IP=$(docker inspect "$TARGET_CONTAINER" | jq -r '.[0].NetworkSettings.Networks.kind.IPAddress')
        TARGET_URL="http://${TARGET_IP}:30080"
        echo "  Target URL: $TARGET_URL"

        USERS={{.USERS | default "10"}}
        DURATION={{.DURATION | default "60s"}}

        WORKFLOW_NAME=$(argo submit "$WORKFLOW_FILE" \
          -p users="$USERS" \
          -p duration="$DURATION" \
          -p target-url="$TARGET_URL" \
          -o name -n argo-workflows 2>/dev/null || \
          kubectl create -f "$WORKFLOW_FILE" -o name)
        echo "  Workflow: $WORKFLOW_NAME"

        echo "  Waiting for workflow completion..."
        argo wait "$WORKFLOW_NAME" -n argo-workflows 2>/dev/null || \
          kubectl wait "$WORKFLOW_NAME" --for=condition=Completed --timeout=30m -n argo-workflows

        echo ""
        echo "=== Workflow Results ==="
        argo get "$WORKFLOW_NAME" -n argo-workflows 2>/dev/null || \
          kubectl get "$WORKFLOW_NAME" -n argo-workflows -o yaml

        # Cleanup
        echo ""
        echo "=== Cleaning up experiment clusters ==="
        for cluster_dir in "$EXP_PATH"/*/; do
          if [ -f "$cluster_dir/cluster.yaml" ]; then
            cluster=$(basename "$cluster_dir")
            full_name="${EXP_NAME}-${cluster}"
            echo "  Deleting cluster: $full_name"
            kind delete cluster --name "$full_name"
          fi
        done

        kubectl config use-context kind-{{.HUB_CLUSTER}}

        echo ""
        echo "=============================================="
        echo "  EXPERIMENT COMPLETE"
        echo "=============================================="

  teardown:
    desc: "Destroy experiment clusters: task kind:teardown -- <name>"
    cmds:
      - |
        if [ -z "{{.CLI_ARGS}}" ]; then
          echo "Usage: task kind:teardown -- <experiment-name>"
          exit 1
        fi

        EXP_NAME="{{.CLI_ARGS}}"
        EXP_PATH="experiments/scenarios/$EXP_NAME"

        if [ ! -d "$EXP_PATH" ]; then
          echo "ERROR: Experiment not found at $EXP_PATH"
          exit 1
        fi

        echo "Destroying experiment clusters for: $EXP_NAME"
        for cluster_dir in "$EXP_PATH"/*/; do
          if [ -f "$cluster_dir/cluster.yaml" ]; then
            cluster_name=$(basename "$cluster_dir")
            full_name="${EXP_NAME}-${cluster_name}"
            if kind get clusters 2>/dev/null | grep -q "^${full_name}$"; then
              echo "  Deleting: $full_name"
              kind delete cluster --name "$full_name"
            fi
          fi
        done
        echo "Done."

  # =============================================================================
  # Internal Helpers
  # =============================================================================
  _ensure-cloud-provider-kind:
    internal: true
    cmds:
      - |
        if docker ps --format '{{.Names}}' | grep -q "^{{.CPK_CONTAINER}}$"; then
          echo "  [cloud-provider-kind] Already running"
          exit 0
        fi
        docker rm -f {{.CPK_CONTAINER}} 2>/dev/null || true
        echo "  [cloud-provider-kind] Starting..."
        docker run -d \
          --name {{.CPK_CONTAINER}} \
          --network kind \
          --restart unless-stopped \
          -v /var/run/docker.sock:/var/run/docker.sock \
          registry.k8s.io/cloud-provider-kind/cloud-controller-manager:v0.6.0
        sleep 2
        docker ps --format '{{.Names}}' | grep -q "^{{.CPK_CONTAINER}}$" && echo "  [cloud-provider-kind] Running" || exit 1

  _ensure-dnsmasq:
    internal: true
    cmds:
      - |
        if docker ps --format '{{.Names}}' | grep -q "^{{.DNSMASQ_CONTAINER}}$"; then
          echo "  [dnsmasq] Already running"
          exit 0
        fi
        docker rm -f {{.DNSMASQ_CONTAINER}} 2>/dev/null || true
        mkdir -p /tmp/kind-dnsmasq
        cat > /tmp/kind-dnsmasq/dnsmasq.conf << 'EOF'
        no-resolv
        no-hosts
        server=8.8.8.8
        server=8.8.4.4
        address=/k8s.local/127.0.0.1
        EOF
        echo "  [dnsmasq] Starting..."
        docker run -d \
          --name {{.DNSMASQ_CONTAINER}} \
          --network kind \
          --restart unless-stopped \
          --cap-add NET_ADMIN \
          -p 127.0.0.1:5353:53/udp \
          -v /tmp/kind-dnsmasq:/etc/dnsmasq.d:ro \
          andyshinn/dnsmasq:latest \
          --conf-dir=/etc/dnsmasq.d,*.conf --no-daemon --log-facility=-
        sleep 2
        docker ps --format '{{.Names}}' | grep -q "^{{.DNSMASQ_CONTAINER}}$" && echo "  [dnsmasq] Running" || exit 1

  _update-dns:
    internal: true
    cmds:
      - |
        if ! docker ps --format '{{.Names}}' | grep -q "^{{.DNSMASQ_CONTAINER}}$"; then
          exit 0
        fi
        echo "  [dnsmasq] Updating DNS entries..."
        mkdir -p /tmp/kind-dnsmasq
        cat > /tmp/kind-dnsmasq/dnsmasq.conf << 'EOF'
        no-resolv
        no-hosts
        server=8.8.8.8
        server=8.8.4.4
        EOF
        for cluster in $(kind get clusters 2>/dev/null); do
          kubectl --context "kind-${cluster}" get svc -A -o json 2>/dev/null | \
            jq -r '.items[] | select(.spec.type=="LoadBalancer") | select(.status.loadBalancer.ingress != null) | "address=/\(.metadata.name).\(.metadata.namespace).{{.DNSMASQ_DOMAIN}}/\(.status.loadBalancer.ingress[0].ip)"' \
            >> /tmp/kind-dnsmasq/dnsmasq.conf
        done
        docker restart {{.DNSMASQ_CONTAINER}} > /dev/null
        echo "  [dnsmasq] Updated"
