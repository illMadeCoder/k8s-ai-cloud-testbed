{"id":"illm-k8s-ai-lab-085","title":"Cycle wait uses talosctl version without --insecure, times out in maintenance mode","description":"After STATE wipe, node enters maintenance mode with different TLS. The cycle task polls with talosctl version (authenticated) which always fails. Needs --insecure flag to detect the node is back in maintenance mode.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-28T17:34:16.386550015-05:00","created_by":"illm","updated_at":"2026-01-28T17:35:15.634192475-05:00","closed_at":"2026-01-28T17:35:15.634192475-05:00","close_reason":"Fixed: cycle wait now uses talosctl version --insecure to detect maintenance mode node","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-08q","title":"otel-demo services crash: OTel schema conflict","description":"Services fail with: Failed to initialize tracer: failed to create resource: conflicting Schema URL: https://opentelemetry.io/schemas/1.26.0 and https://opentelemetry.io/schemas/1.24.0. Need to align OTel SDK versions.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-12T20:09:43.504031341-05:00","created_by":"illm","updated_at":"2026-01-12T20:15:08.112598336-05:00","closed_at":"2026-01-12T20:15:08.112598336-05:00","close_reason":"Fixed by upgrading OTel SDK to v1.33.0","labels":["otel-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-0si","title":"Stale old node object persists after teardown/deploy cycle","description":"After STATE wipe and redeploy, the old node object (talos-jid-knd) remains with NotReady,SchedulingDisabled status. The new node (talos-0lj-hy1) is Ready. The stale node doesn't cause functional issues but is confusing. Could auto-delete NotReady nodes during deploy step 3.","status":"closed","priority":3,"issue_type":"bug","created_at":"2026-01-28T14:43:19.289705199-05:00","created_by":"illm","updated_at":"2026-01-28T15:01:53.09852084-05:00","closed_at":"2026-01-28T15:01:53.09852084-05:00","close_reason":"Fixed in deploy task: Step 3 now deletes stale NotReady/SchedulingDisabled nodes after the new node is Ready","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-0so","title":"Test observability-cost-tutorial on Talos","description":"Test observability-cost-tutorial experiment on Talos platform. Goal: 'task talos:up -- observability-cost-tutorial' works with zero manual intervention. Cycle the experiment, identify and fix any toil. Create child beads for each issue discovered during testing.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T08:03:00.484077037-05:00","created_by":"illm","updated_at":"2026-01-13T08:25:50.747852387-05:00","closed_at":"2026-01-13T08:25:50.747852387-05:00","close_reason":"observability-cost-tutorial deploys successfully on Talos with all workloads healthy","labels":["talos-testing"]}
{"id":"illm-k8s-ai-lab-0vl","title":"Crossplane providers overwhelming N100 - reduce provider count","description":"ROOT CAUSE FOUND: Providers OOM killed at 256Mi limit (using 258MB). Fixed by increasing limits to 512Mi in commit caee555. Still need MRAPs for long-term CRD reduction.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-17T16:27:00.59944283-05:00","created_by":"illm","updated_at":"2026-01-24T12:35:27.996341824-05:00","closed_at":"2026-01-24T12:35:27.996341824-05:00","close_reason":"Fixed: Reduced to 2 providers (helm, kubernetes), memory usage now ~340Mi total","labels":["crossplane","resources","talos"]}
{"id":"illm-k8s-ai-lab-18z","title":"Create Grafana Agent config for target clusters","description":"Create Helm values for Grafana Agent on target clusters with remote_write to hub Mimir. Lightweight (~100MB) observability.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-17T12:46:21.842940748-05:00","created_by":"illm","updated_at":"2026-01-17T12:46:21.842940748-05:00"}
{"id":"illm-k8s-ai-lab-1cr","title":"Target cluster missing external-secrets CRDs","description":"ExternalSecrets fail on target cluster because external-secrets-operator isn't installed. Need to either install ESO on target or use different secret approach for tutorial.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-15T13:33:05.897483823-05:00","created_by":"illm","updated_at":"2026-01-15T13:37:44.917640719-05:00","closed_at":"2026-01-15T13:37:44.917640719-05:00","close_reason":"Fixed by reverting ExternalSecrets to regular Secrets"}
{"id":"illm-k8s-ai-lab-1ex","title":"Hub app deletion stuck on finalizer during reset","description":"When deleting the hub app-of-apps, the resources-finalizer.argocd.argoproj.io finalizer prevents deletion even after 10+ minutes. Child apps don't have owner references so cascade delete doesn't work as expected. Manual finalizer removal required.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-24T15:01:47.451643224-05:00","created_by":"illm","updated_at":"2026-01-24T17:28:34.160147662-05:00","closed_at":"2026-01-24T17:28:34.160147662-05:00","close_reason":"Addressed by talos-hub Taskfile automation (task talos-hub:reset handles webhook cleanup, finalizer removal, and OpenBao re-init)","labels":["argocd","toil"]}
{"id":"illm-k8s-ai-lab-26k","title":"OpenBao needs re-initialization after Talos namespace reset","description":"On Talos hub, OpenBao uses local-path PVC storage. When the namespace is deleted during reset, all vault data is lost and OpenBao needs re-initialization. Keys from previous installs (in ~/.illmlab/openbao-keys.json) won't work. Consider using persistent hostPath mount like Kind does, or implement backup/restore process.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-24T15:16:51.150222183-05:00","created_by":"illm","updated_at":"2026-01-24T17:28:34.16831777-05:00","closed_at":"2026-01-24T17:28:34.16831777-05:00","close_reason":"Addressed by talos-hub Taskfile automation (task talos-hub:reset handles webhook cleanup, finalizer removal, and OpenBao re-init)","labels":["openbao","talos","toil"]}
{"id":"illm-k8s-ai-lab-30m","title":"Kyverno webhook blocks Helm install after teardown/deploy recovery","description":"After recovering from a failed cycle, Kyverno's mutating webhook is registered but pods aren't ready. Helm pre-upgrade hooks fail with 'connection refused'. Deploy needs to clean stale webhooks before the Helm step.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-28T17:38:34.297855963-05:00","created_by":"illm","updated_at":"2026-01-28T17:43:01.393741544-05:00","closed_at":"2026-01-28T17:43:01.393741544-05:00","close_reason":"Added webhook cleanup to deploy Step 4 before Helm install","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-39i","title":"ArgoCD OCI helm charts not working","description":"ArgoCD helm pull fails for OCI registries. May need ArgoCD config update or use YAML-based install instead.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-15T13:26:36.683603747-05:00","created_by":"illm","updated_at":"2026-01-15T17:49:50.778618288-05:00","closed_at":"2026-01-15T17:49:50.778618288-05:00","close_reason":"Fixed: Added GatewayClass to manifests, added grpcurl test pod, documented OCI workaround"}
{"id":"illm-k8s-ai-lab-3o4","title":"logging-comparison log-generator uses local image","description":"log-generator deployment uses 'log-generator:latest' which is a local image. Needs to be pushed to ttl.sh registry for Talos deployment.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-12T21:01:40.886143803-05:00","created_by":"illm","updated_at":"2026-01-12T21:02:03.002954628-05:00","closed_at":"2026-01-12T21:02:03.002954628-05:00","close_reason":"Closed","labels":["logging-comparison","toil"]}
{"id":"illm-k8s-ai-lab-4dw","title":"Test seaweedfs-tutorial on Talos","description":"Test seaweedfs-tutorial experiment on Talos platform. Goal: 'task talos:up -- seaweedfs-tutorial' works with zero manual intervention. Cycle the experiment, identify and fix any toil. Create child beads for each issue discovered during testing.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T08:03:03.00657484-05:00","created_by":"illm","updated_at":"2026-01-13T08:48:18.823497158-05:00","closed_at":"2026-01-13T08:48:18.823497158-05:00","close_reason":"SeaweedFS deploys successfully on Talos with PVC storage","labels":["talos-testing"]}
{"id":"illm-k8s-ai-lab-4dw.1","title":"SeaweedFS uses hostPath which fails on Talos read-only FS","description":"SeaweedFS helm chart default uses hostPath volumes at /ssd/ which fails on Talos (read-only filesystem). Need to configure persistence with PVC using local-path StorageClass instead.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-13T08:34:07.004796123-05:00","created_by":"illm","updated_at":"2026-01-13T08:48:18.801513116-05:00","closed_at":"2026-01-13T08:48:18.801513116-05:00","close_reason":"SeaweedFS deploys successfully on Talos with PVC storage","labels":["talos-testing"],"dependencies":[{"issue_id":"illm-k8s-ai-lab-4dw.1","depends_on_id":"illm-k8s-ai-lab-4dw","type":"parent-child","created_at":"2026-01-13T08:34:07.005996283-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-4dw.2","title":"SeaweedFS dashboard assumes monitoring namespace exists","description":"dashboard.yaml hardcoded to monitoring namespace. Either exclude dashboard/alerts from standalone deployment or add monitoring stack. For now, excluding with directory.exclude.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-13T08:41:04.851752414-05:00","created_by":"illm","updated_at":"2026-01-13T08:48:18.819881892-05:00","closed_at":"2026-01-13T08:48:18.819881892-05:00","close_reason":"SeaweedFS deploys successfully on Talos with PVC storage","labels":["talos-testing"],"dependencies":[{"issue_id":"illm-k8s-ai-lab-4dw.2","depends_on_id":"illm-k8s-ai-lab-4dw","type":"parent-child","created_at":"2026-01-13T08:41:04.861102428-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-4i9","title":"OOM during Crossplane v2 upgrade - reduce provider count","description":"Installing 25+ Crossplane providers simultaneously caused WSL to run out of memory (7.6GB total). Solution: In v2 family provider architecture, individual providers (like provider-aws-s3) only install CRDs - they don't run pods. We should install only family providers and let CRDs be installed on-demand.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-17T14:08:04.546335651-05:00","created_by":"illm","updated_at":"2026-01-17T14:11:09.497258534-05:00","closed_at":"2026-01-17T14:11:09.497258534-05:00","close_reason":"Fixed by reducing providers from 26 to 12"}
{"id":"illm-k8s-ai-lab-5cs","title":"Jaeger values.yaml has duplicate COLLECTOR_OTLP_ENABLED","description":"The allInOne.extraEnv adds COLLECTOR_OTLP_ENABLED but it already exists in the chart defaults, causing duplicate key error.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-12T21:46:17.63634854-05:00","created_by":"illm","updated_at":"2026-01-12T21:46:32.223955212-05:00","closed_at":"2026-01-12T21:46:32.223955212-05:00","close_reason":"Closed","labels":["toil","tracing-comparison"]}
{"id":"illm-k8s-ai-lab-5h2","title":"K8s gRPC health probes don't support TLS","description":"Kubernetes gRPC health probes don't support TLS. The grpc-healthcheck-demo was using port 9001 (TLS) which caused continuous TLS handshake failures. Fixed by using port 9000 (insecure) for health probes.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-15T13:49:32.060489683-05:00","created_by":"illm","updated_at":"2026-01-15T13:49:46.30314114-05:00","closed_at":"2026-01-15T13:49:46.30314114-05:00","close_reason":"Fixed by using insecure port 9000 for health probes","labels":["gateway-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-5yd","title":"ESO webhooks block namespace deletion during reset","description":"During talos-hub:reset, external-secrets webhooks remain after ESO is deleted, blocking tailscale namespace deletion. Need to delete external-secrets webhooks in Step 0.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-25T20:38:50.729243009-05:00","created_by":"illm","updated_at":"2026-01-26T08:32:15.892716377-05:00","closed_at":"2026-01-26T08:32:15.892716377-05:00","close_reason":"Fixed by adding external-secrets to webhook cleanup in Taskfile Step 0","labels":["external-secrets","networking","toil"]}
{"id":"illm-k8s-ai-lab-618","title":"ArgoCD terminatingReplicas comparison error on Talos","description":"Kubernetes 1.32+ adds terminatingReplicas to ReplicaSet/Deployment status. ArgoCD schema doesn't recognize it, causing ComparisonError.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-12T21:40:47.81587937-05:00","created_by":"illm","updated_at":"2026-01-12T21:41:26.548860557-05:00","closed_at":"2026-01-12T21:41:26.548860557-05:00","close_reason":"Closed","labels":["toil","tracing-comparison"]}
{"id":"illm-k8s-ai-lab-666","title":"Add local-path-provisioner to Talos bootstrap","description":"## Summary\nAdded local-path-provisioner to Talos platform bootstrap to support PVC-based workloads.\n\n## Changes\n- `platforms/talos/manifests/local-path-provisioner.yaml` - New manifest with:\n  - PodSecurity labels (privileged) on namespace\n  - Default StorageClass annotation\n- `platforms/talos/Taskfile.yaml`:\n  - Updated bootstrap task to install both kube-vip and local-path-provisioner\n  - Added local-path-storage to preserved namespaces\n\n## Context\nDiscovered during loki-tutorial deployment that Talos had no StorageClass, causing Loki PVC to stay Pending.\n\n## Commit\n`65ebb02` - feat: Add local-path-provisioner to Talos bootstrap\n","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-12T07:35:33.567206Z","updated_at":"2026-01-12T07:35:33.567206Z","closed_at":"2026-01-12T07:35:33.567206Z"}
{"id":"illm-k8s-ai-lab-68h","title":"Stale ArgoCD apps block new lab deployments","description":"When clusters are deleted via 'kind delete cluster', the ArgoCD apps remain in hub cluster and claim shared resources (ClusterRoles, ClusterRoleBindings, Namespaces). This causes new lab deployments to fail with 'is part of applications X and Y' errors. \n\nStale apps found: loki-tutorial-target, slo-tutorial-target, observability-cost-tutorial-target\n\nWorkaround: Manually delete stale ArgoCD apps before running new labs.\n\nRoot cause: kind:down task doesn't clean up ArgoCD apps when deleting clusters.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T13:13:39.350247155-05:00","created_by":"illm","updated_at":"2026-01-10T13:24:33.757053858-05:00","closed_at":"2026-01-10T13:24:33.757053858-05:00","close_reason":"Closed","labels":["config","prometheus-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-6kp","title":"Test http-baseline on Talos","description":"Test http-baseline experiment on Talos platform. Goal: 'task talos:up -- http-baseline' works with zero manual intervention. Cycle the experiment, identify and fix any toil. Create child beads for each issue discovered during testing.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T08:02:57.890385649-05:00","created_by":"illm","updated_at":"2026-01-13T08:18:24.112057794-05:00","closed_at":"2026-01-13T08:18:24.112057794-05:00","close_reason":"http-baseline deploys successfully on Talos","labels":["talos-testing"]}
{"id":"illm-k8s-ai-lab-6kp.1","title":"Taskfile sed pattern doesn't handle comments in destination","description":"The talos:up sed pattern uses name: target$ which requires target at end of line. But http-baseline and other experiments have comments after target. Fix sed pattern to handle this.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-13T08:17:18.333322693-05:00","created_by":"illm","updated_at":"2026-01-13T08:18:24.10634152-05:00","closed_at":"2026-01-13T08:18:24.10634152-05:00","close_reason":"http-baseline deploys successfully on Talos","labels":["talos-testing"],"dependencies":[{"issue_id":"illm-k8s-ai-lab-6kp.1","depends_on_id":"illm-k8s-ai-lab-6kp","type":"parent-child","created_at":"2026-01-13T08:17:18.335151488-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-6t6","title":"grpcbin uses port 9000 for plaintext gRPC, not 9001","description":"All grpc-tutorial services were targeting port 9001 but grpcbin uses port 9000 for plaintext gRPC. This caused all gRPC tests to timeout. Fixed by changing targetPort to 9000.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-15T18:33:38.102675599-05:00","created_by":"illm","updated_at":"2026-01-15T18:33:44.193501852-05:00","closed_at":"2026-01-15T18:33:44.193501852-05:00","close_reason":"Fixed: Changed all service targetPort from 9001 to 9000","labels":["config","gateway-tutorial"]}
{"id":"illm-k8s-ai-lab-7eb","title":"Multi-namespace experiments need all namespaces PSS labeled","description":"Experiments like logging-comparison create multiple namespaces (loki, elasticsearch, monitoring, logging-comparison). The talos:up task only labels the main destination namespace, but DaemonSets like promtail/fluent-bit in sub-namespaces are blocked by PSS.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-12T21:03:00.810780882-05:00","created_by":"illm","updated_at":"2026-01-12T21:05:09.280020901-05:00","closed_at":"2026-01-12T21:05:09.280020901-05:00","close_reason":"Closed","labels":["logging-comparison","toil"]}
{"id":"illm-k8s-ai-lab-7p5","title":"Tier 3: Prune dead code, update docs, align roadmap with reality","description":"Epic: Remove unused code and bring documentation in line with actual state.\n\nSubtasks:\n1. Remove or archive disabled Istio manifests (platform/hub/apps/disabled/) — decision made in ADR-014 to defer to Phase 7.\n2. Remove Vault component if replaced by OpenBao.\n3. Clean up Kind-specific hub config if only Talos is used (platform/hub/ vs platform/hub-talos/).\n4. Assess GitLab CI — is it active or dead? Remove if not used.\n5. Update ADR-013 status from \"Proposed\" to \"Accepted\" (Crossplane v2 is running).\n6. Update roadmap.md to reflect actual Phase 3 completion state.\n7. Audit experiment scenarios — which reference components that are not deployed?\n8. Consolidate scripts/ into Taskfile if appropriate.\n\nRelated existing beads:\n- aql (P2): kube-vip not respecting loadBalancerIP — config cleanup candidate\n- r8c (P1): Control-plane taint returns after reboot — config cleanup candidate\n\nNo blockers — this is independent cleanup work.\n\nSuccess criteria: No dead/unreachable code in tree. Docs accurately describe what exists. ADR statuses current.","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-02T17:31:17.796774899-05:00","created_by":"illm","updated_at":"2026-02-02T17:31:17.796774899-05:00","labels":["cleanup docs"]}
{"id":"illm-k8s-ai-lab-86u","title":"cloud-provider-kind needs restart after new cluster creation","description":"When a new Kind cluster is created, cloud-provider-kind container cannot connect to it due to TLS certificate issues. LoadBalancer services remain in 'pending' state.\n\nError: 'tls: failed to verify certificate: x509: certificate signed by unknown authority'\n\nWorkaround: Restart cloud-provider-kind container after cluster creation.\n\nRoot cause: cloud-provider-kind doesn't automatically pick up credentials for new clusters.\n\nPossible fixes:\n1. Restart cloud-provider-kind after cluster creation in kind:up task\n2. Configure cloud-provider-kind to auto-reload when new clusters appear","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T14:04:56.331007316-05:00","created_by":"illm","updated_at":"2026-01-10T14:05:33.809441658-05:00","closed_at":"2026-01-10T14:05:33.809441658-05:00","close_reason":"Closed","labels":["networking","timing","toil"]}
{"id":"illm-k8s-ai-lab-89a","title":"OpenBao secrets lost after reinit - ESO syncs fail","description":"After OpenBao is reinitialized during cluster reset, all stored secrets are lost. ExternalSecrets fail with SecretSyncedError. Affected: argocd-server-tls, argocd-webhook-secret, cluster-talos-target, cloudflare-api-token, crossplane credentials, tailscale-operator-oauth. Need to either: 1) Backup OpenBao before reset, 2) Recreate secrets after reset, 3) Use persistent storage that survives reset","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-26T08:31:33.76776628-05:00","created_by":"illm","updated_at":"2026-01-26T08:41:13.104480244-05:00","closed_at":"2026-01-26T08:41:13.104480244-05:00","close_reason":"Fixed by using local-path-retain StorageClass with Retain reclaim policy. PV survives PVC deletion so OpenBao secrets persist across cluster resets.","labels":["eso","openbao","toil"]}
{"id":"illm-k8s-ai-lab-8ax","title":"talos:up PSS labels applied after DaemonSet pods rejected","description":"Step 4 labels namespaces AFTER Step 3 waits for sync, but DaemonSet pods are created during sync. Need to restart DaemonSets after labeling or label namespaces earlier.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-12T21:12:21.093166496-05:00","created_by":"illm","updated_at":"2026-01-12T21:12:52.438533524-05:00","closed_at":"2026-01-12T21:12:52.438533524-05:00","close_reason":"Closed","labels":["toil"]}
{"id":"illm-k8s-ai-lab-8xl","title":"cloud-gateway-comparison: AWS ALB IC vs Azure AGIC vs in-cluster","description":"Compare cloud-native gateways (AWS ALB IC, Azure AGIC) vs in-cluster (Envoy Gateway). Metrics: latency, throughput, cost, propagation time, features, observability, blast radius. Deploys to AWS/Azure via Crossplane + Talos baseline.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-15T09:20:19.498112321-05:00","created_by":"illm","updated_at":"2026-01-15T09:24:07.789812155-05:00","closed_at":"2026-01-15T09:24:07.789812155-05:00","close_reason":"Wrong tracking mechanism - these are roadmap items, not toil","dependencies":[{"issue_id":"illm-k8s-ai-lab-8xl","depends_on_id":"illm-k8s-ai-lab-age","type":"blocks","created_at":"2026-01-15T09:20:24.705355839-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-98l","title":"Implement budget tracking","description":"Create experiment-budget-limits ConfigMap and composition function for per-XRD + global budget enforcement.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-17T12:46:29.614265972-05:00","created_by":"illm","updated_at":"2026-01-17T12:46:29.614265972-05:00","dependencies":[{"issue_id":"illm-k8s-ai-lab-98l","depends_on_id":"illm-k8s-ai-lab-t4h","type":"blocks","created_at":"2026-01-17T12:46:41.398942013-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-999","title":"otel-demo images expire after 2h (ttl.sh)","description":"The otel-demo services use ttl.sh ephemeral images with 2h TTL. Images expire, causing ImagePullBackOff. Need to either: (1) use persistent registry like ghcr.io, or (2) build images as part of 'up' command.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-12T20:02:19.689543454-05:00","created_by":"illm","updated_at":"2026-01-12T20:26:35.171492802-05:00","closed_at":"2026-01-12T20:26:35.171492802-05:00","close_reason":"Fixed with 24h TTL and imagePullPolicy: Always","labels":["otel-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-9n0","title":"Kyverno webhooks block namespace deletion during reset","description":"During talos-hub:reset, Kyverno webhooks (validate.kyverno.svc-fail) remain after Kyverno is deleted, blocking resource deletion. Need to delete ALL Kyverno webhooks in Step 0, not just cert-manager and kyverno ones.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-25T20:38:48.358055295-05:00","created_by":"illm","updated_at":"2026-01-26T08:32:15.991137006-05:00","closed_at":"2026-01-26T08:32:15.991137006-05:00","close_reason":"Fixed by adding kyverno to webhook cleanup in Taskfile Step 0","labels":["kyverno","networking","toil"]}
{"id":"illm-k8s-ai-lab-9p4","title":"Crossplane v2 Upgrade","description":"Upgrade from Crossplane v1.18.1 to v2.x to enable namespace-scoped XRs, direct K8s resource composition, and MRDs. See ADR-013.","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-17T12:46:06.891449705-05:00","created_by":"illm","updated_at":"2026-01-17T14:31:35.908843763-05:00","closed_at":"2026-01-17T14:31:35.908843763-05:00","close_reason":"Implemented via phased ArgoCD apps (6 waves: core, base, azure, aws, gcp, configs)"}
{"id":"illm-k8s-ai-lab-9s1","title":"loki-tutorial promtail blocked by PodSecurity baseline","description":"Two issues: (1) PodSecurity baseline blocks hostPath - needs namespace label, (2) Promtail can't find logs - Talos uses different log paths than standard Linux. Core loki infra works, promtail needs Talos-specific config.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-12T20:29:42.928600681-05:00","created_by":"illm","updated_at":"2026-01-12T20:36:20.525164441-05:00","closed_at":"2026-01-12T20:36:20.525164441-05:00","close_reason":"Superseded by general Talos PSS issue","labels":["loki-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-a7k","title":"Envoy Gateway v1.2.4 crashes on GRPCRoute RequestMirror","description":"Envoy Gateway v1.2.4 panics when processing GRPCRoute with RequestMirror filter. Error: interface conversion: interface {} is []v1.HTTPRouteFilter, not []v1.GRPCRouteFilter. Workaround: remove RequestMirror from GRPCRoutes.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-15T13:41:00.941997688-05:00","created_by":"illm","updated_at":"2026-01-15T17:48:26.400396086-05:00","closed_at":"2026-01-15T17:48:26.400396086-05:00","close_reason":"Workaround applied: GRPCRoute RequestMirror commented out in part5-grpc-gateway-api.yaml with reference to upstream bug","labels":["gateway-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-a7q","title":"Tailscale operator leaves stale StatefulSets after cluster reset","description":"After cluster reset/redeploy, Tailscale operator creates StatefulSets but the corresponding secrets don't exist. Operator logs: 'Tailscale proxy secret doesn't exist, but the corresponding StatefulSet already does. Something is wrong, please delete the StatefulSet.' Requires manual cleanup of all ts-* StatefulSets and secrets in tailscale namespace before operator can reconcile properly.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-26T09:57:42.492771261-05:00","created_by":"illm","updated_at":"2026-01-26T11:58:13.337439852-05:00","closed_at":"2026-01-26T11:58:13.337439852-05:00","close_reason":"Fixed in commit 847bfc6","labels":["tailscale","timing","toil"]}
{"id":"illm-k8s-ai-lab-age","title":"gateway-comparison: nginx vs Traefik vs Envoy Gateway","description":"Side-by-side comparison of gateway implementations with same demo app and routes. Compare config complexity, resource usage, features. Deploys to Talos.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-15T09:20:19.082344247-05:00","created_by":"illm","updated_at":"2026-01-15T09:24:07.785749757-05:00","closed_at":"2026-01-15T09:24:07.785749757-05:00","close_reason":"Wrong tracking mechanism - these are roadmap items, not toil","dependencies":[{"issue_id":"illm-k8s-ai-lab-age","depends_on_id":"illm-k8s-ai-lab-iku","type":"blocks","created_at":"2026-01-15T09:20:24.574174163-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-ahu","title":"hello-app app.yaml uses direct server URL instead of name: target","description":"hello-app's app.yaml uses server: https://kubernetes.default.svc instead of name: target. The talos:up task expects name: target to substitute.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-13T08:09:11.881063331-05:00","created_by":"illm","updated_at":"2026-01-13T08:12:57.836698276-05:00","closed_at":"2026-01-13T08:12:57.836698276-05:00","close_reason":"Fixed app.yaml to use name: target instead of server URL","labels":["hello-app","toil"]}
{"id":"illm-k8s-ai-lab-ajb","title":"ArgoCD CreateNamespace fails after namespace deletion","description":"After deleting a namespace and resetting, ArgoCD's CreateNamespace=true sync option sometimes fails with 'namespace not found' error even though the option should create the namespace. Manual namespace creation required as workaround.","status":"open","priority":2,"issue_type":"bug","created_at":"2026-01-24T16:20:29.006758493-05:00","created_by":"illm","updated_at":"2026-01-24T16:20:29.006758493-05:00","labels":["argocd","toil"]}
{"id":"illm-k8s-ai-lab-aql","title":"kube-vip not respecting loadBalancerIP field","description":"During Talos hub migration, discovered that kube-vip-cloud-provider may not be respecting the loadBalancerIP spec field. OpenBao requested 192.168.1.242 but got 192.168.1.240 (same as DNS). Currently works because different ports (DNS:53, OpenBao:8200), but should investigate proper IP allocation.","status":"open","priority":2,"issue_type":"bug","created_at":"2026-01-17T16:24:11.384627435-05:00","created_by":"illm","updated_at":"2026-01-17T16:24:11.384627435-05:00","labels":["config","networking","talos"]}
{"id":"illm-k8s-ai-lab-aua","title":"Deploy SeaweedFS on hub cluster","description":"Deploy SeaweedFS S3-compatible storage for experiment artifacts (k6 results, Grafana snapshots, logs, heap dumps).","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T12:46:14.384661933-05:00","created_by":"illm","updated_at":"2026-01-24T15:28:35.49619668-05:00","closed_at":"2026-01-24T15:28:35.49619668-05:00","close_reason":"SeaweedFS deployed and working with bucket creation automation","dependencies":[{"issue_id":"illm-k8s-ai-lab-aua","depends_on_id":"illm-k8s-ai-lab-9p4","type":"blocks","created_at":"2026-01-17T12:46:40.959386413-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-av5","title":"Create generic workflow templates","description":"Create Argo Workflow templates: tutorial-interactive, benchmark-k6, benchmark-comparison, soak-test.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-17T12:46:26.105493757-05:00","created_by":"illm","updated_at":"2026-01-17T12:46:26.105493757-05:00","dependencies":[{"issue_id":"illm-k8s-ai-lab-av5","depends_on_id":"illm-k8s-ai-lab-t4h","type":"blocks","created_at":"2026-01-17T12:46:41.175653645-05:00","created_by":"illm"},{"issue_id":"illm-k8s-ai-lab-av5","depends_on_id":"illm-k8s-ai-lab-aua","type":"blocks","created_at":"2026-01-17T12:46:41.286803027-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-b4d","title":"Test hello-app on Talos","description":"Test hello-app experiment on Talos platform. Goal: 'task talos:up -- hello-app' works with zero manual intervention. Cycle the experiment, identify and fix any toil. Create child beads for each issue discovered during testing.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T08:02:56.628679475-05:00","created_by":"illm","updated_at":"2026-01-13T08:12:58.140405786-05:00","closed_at":"2026-01-13T08:12:58.140405786-05:00","close_reason":"hello-app deploys successfully on Talos with task talos:up -- hello-app","labels":["talos-testing"]}
{"id":"illm-k8s-ai-lab-bji","title":"Test multi-cloud-demo on Talos","description":"Test multi-cloud-demo experiment on Talos platform. Goal: 'task talos:up -- multi-cloud-demo' works with zero manual intervention. Cycle the experiment, identify and fix any toil. Create child beads for each issue discovered during testing.","notes":"Blocked by illm-k8s-ai-lab-bji.1 - needs structural redesign","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T08:02:59.196152983-05:00","created_by":"illm","updated_at":"2026-01-13T20:44:52.468141217-05:00","closed_at":"2026-01-13T20:44:52.468141217-05:00","close_reason":"Deprioritized - multi-cloud-demo not on current roadmap.","labels":["talos-testing"]}
{"id":"illm-k8s-ai-lab-bji.1","title":"multi-cloud-demo needs structural redesign for Talos","description":"Experiment has: 1) server URL instead of name: target, 2) wrong paths (crossplane, k6 don't exist), 3) depends on Crossplane on hub cluster. May need separate target/loadgen apps like other experiments.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T08:22:39.345450998-05:00","created_by":"illm","updated_at":"2026-01-13T20:44:48.160479121-05:00","closed_at":"2026-01-13T20:44:48.160479121-05:00","close_reason":"Deprioritized - not on current roadmap. Experiment needs redesign but not urgent.","labels":["talos-testing"],"dependencies":[{"issue_id":"illm-k8s-ai-lab-bji.1","depends_on_id":"illm-k8s-ai-lab-bji","type":"parent-child","created_at":"2026-01-13T08:22:39.347823399-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-bvh","title":"OpenBao PVC stuck terminating blocks ESO sync","description":"During cluster reset, OpenBao PVC gets stuck in Terminating state. OpenBao regenerates with new root token but ESO still has old token, causing all ExternalSecrets to fail with 403. Need to: 1) Force delete stuck PVC 2) Reinitialize OpenBao 3) Update ESO token","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-26T07:42:48.885097197-05:00","created_by":"illm","updated_at":"2026-01-26T08:32:15.778278372-05:00","closed_at":"2026-01-26T08:32:15.778278372-05:00","close_reason":"Fixed by patching PVC finalizers during cleanup and adding PVC cleanup to Taskfile Step 5","labels":["eso","openbao","toil"]}
{"id":"illm-k8s-ai-lab-by6","title":"Envoy Gateway helm chart repo 404 error","description":"The https://gateway.envoyproxy.io/charts URL returns 404. Need to find correct helm repo URL or use OCI format properly.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-15T13:23:05.8751578-05:00","created_by":"illm","updated_at":"2026-01-15T13:24:09.672280533-05:00","closed_at":"2026-01-15T13:24:09.672280533-05:00","close_reason":"Split Envoy Gateway into separate single-source ArgoCD Application"}
{"id":"illm-k8s-ai-lab-chc","title":"STATE wipe puts Talos into maintenance mode, needs apply-config before bootstrap","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-28T14:28:23.613360442-05:00","created_by":"illm","updated_at":"2026-01-28T15:01:06.573748045-05:00","closed_at":"2026-01-28T15:01:06.573748045-05:00","close_reason":"Fixed in deploy task: Step 1 applies machine config if node is in maintenance mode; Helm guard clears stale pending-install releases before upgrade","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-cl1","title":"SeaweedFS S3 endpoint stuck in notReadyAddresses after reboot","status":"open","priority":1,"issue_type":"bug","created_at":"2026-01-26T20:50:19.185044466-05:00","created_by":"illm","updated_at":"2026-01-26T20:50:19.185044466-05:00","labels":["networking","timing","toil"]}
{"id":"illm-k8s-ai-lab-cqc","title":"Namespace termination blocked by pods with finalizers","description":"During talos-hub:reset, namespaces get stuck in Terminating state because pods still exist. The script removes resource finalizers but doesn't force delete pods. Need to add force pod deletion or wait longer for pods to terminate gracefully.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-24T16:12:43.838641883-05:00","created_by":"illm","updated_at":"2026-01-24T17:28:34.131248005-05:00","closed_at":"2026-01-24T17:28:34.131248005-05:00","close_reason":"Addressed by talos-hub Taskfile automation (task talos-hub:reset handles webhook cleanup, finalizer removal, and OpenBao re-init)","labels":["k8s","toil"]}
{"id":"illm-k8s-ai-lab-d8f","title":"kong/httpbin image cannot run as non-root on port 80","description":"kong/httpbin:0.1.0 binds to port 80 which requires root. With runAsNonRoot: true, the container fails to start. Fixed by switching to mccutchen/go-httpbin which supports PORT env var.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-15T13:44:49.887606948-05:00","created_by":"illm","updated_at":"2026-01-15T13:45:03.22224403-05:00","closed_at":"2026-01-15T13:45:03.22224403-05:00","close_reason":"Fixed by switching to mccutchen/go-httpbin","labels":["gateway-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-d8v","title":"Kibana OOMKilled - needs more memory","description":"Kibana container is OOMKilled with 512Mi limit. Needs 1Gi for stable operation.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-12T21:04:53.501749078-05:00","created_by":"illm","updated_at":"2026-01-12T21:05:09.270943794-05:00","closed_at":"2026-01-12T21:05:09.270943794-05:00","close_reason":"Closed","labels":["logging-comparison","toil"]}
{"id":"illm-k8s-ai-lab-dcm","title":"SeaweedFS PostSync hook doesn't trigger on redeploy","description":"The PostSync hook for bucket creation didn't trigger during the redeploy cycle. The job had to be manually applied. The hook worked on the first deploy but not on subsequent syncs. May need to investigate ArgoCD hook behavior or change to a different approach (initContainer, CronJob).","status":"open","priority":2,"issue_type":"bug","created_at":"2026-01-24T16:39:21.228125268-05:00","created_by":"illm","updated_at":"2026-01-24T16:39:21.228125268-05:00","labels":["argocd","seaweedfs","toil"]}
{"id":"illm-k8s-ai-lab-dgz","title":"otel-demo pods missing securityContext for restricted PSS","description":"Pods warn about PodSecurity restricted mode: need securityContext.allowPrivilegeEscalation=false, capabilities.drop=[ALL], runAsNonRoot=true, seccompProfile","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-12T20:26:42.277331295-05:00","created_by":"illm","updated_at":"2026-01-13T16:34:49.844647847-05:00","closed_at":"2026-01-13T16:34:49.844647847-05:00","close_reason":"Added securityContext to all otel-demo pods for restricted PSS compliance","labels":["config","otel-tutorial"]}
{"id":"illm-k8s-ai-lab-dj1","title":"Envoy Gateway OCI chart URL incorrect in ArgoCD app","description":"ArgoCD app.yaml uses wrong OCI format: oci://docker.io/envoyproxy gateway-helm. Should be oci://docker.io/envoyproxy/gateway-helm or use the standard helm repo https://gateway.envoyproxy.io/charts","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-15T13:20:05.182003208-05:00","created_by":"illm","updated_at":"2026-01-15T13:20:30.566665018-05:00","closed_at":"2026-01-15T13:20:30.566665018-05:00","close_reason":"Fixed by using HTTPS helm repo URL instead of OCI format"}
{"id":"illm-k8s-ai-lab-dq9","title":"Control plane crashes during Crossplane CRD installation","description":"During provider installation, the ~800 CRDs overwhelm the API server, causing kube-scheduler and kube-controller-manager to lose their leader leases. Control plane crashes in CrashLoopBackOff while CRDs are being registered. Related to ewz but distinct - this is CRD installation load, not post-reboot.","status":"open","priority":1,"issue_type":"bug","created_at":"2026-01-27T06:38:50.705022539-05:00","created_by":"illm","updated_at":"2026-01-27T06:38:50.705022539-05:00","labels":["crossplane","timing","toil"]}
{"id":"illm-k8s-ai-lab-ei7","title":"Talos API server becomes unresponsive during hub migration","description":"During hub migration to Talos N100, the Kubernetes API server became unresponsive. Node is pingable (192.168.1.178) but kubectl and talosctl cannot connect to port 6443. Possible causes: resource exhaustion from deploying many apps simultaneously, kube-vip conflict, or N100 resource limits.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-17T16:07:51.568853175-05:00","created_by":"illm","updated_at":"2026-01-17T16:24:18.412741687-05:00","closed_at":"2026-01-17T16:24:18.412741687-05:00","close_reason":"API server recovered after ~3 minutes. Possible temporary overload during heavy app deployment.","labels":["resources","talos","timing"]}
{"id":"illm-k8s-ai-lab-ewz","title":"Scheduler stuck after reboot - not acquiring lease","status":"open","priority":0,"issue_type":"bug","created_at":"2026-01-26T21:12:06.49079538-05:00","created_by":"illm","updated_at":"2026-01-26T21:12:06.49079538-05:00","labels":["timing","toil"]}
{"id":"illm-k8s-ai-lab-fek","title":"Cycle wait: talosctl version --insecure exits 1 in maintenance mode","description":"talosctl version --insecure returns exit code 1 in maintenance mode (API is not implemented in maintenance mode) even though the node IS reachable. The cycle wait loop uses exit code to detect availability, so it times out after 10 minutes. Fix: check for output content (e.g. grep for Tag:) instead of exit code.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-28T17:57:28.530974146-05:00","created_by":"illm","updated_at":"2026-01-28T18:00:27.105564596-05:00","closed_at":"2026-01-28T18:00:27.105564596-05:00","close_reason":"Fixed: check for Tag: in output instead of exit code","labels":["toil"]}
{"id":"illm-k8s-ai-lab-fh1","title":"Test cicd-fundamentals on Talos","description":"Test cicd-fundamentals experiment on Talos platform. Goal: 'task talos:up -- cicd-fundamentals' works with zero manual intervention. Cycle the experiment, identify and fix any toil. Create child beads for each issue discovered during testing.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T08:03:01.728938433-05:00","created_by":"illm","updated_at":"2026-01-13T08:26:37.234423197-05:00","closed_at":"2026-01-13T08:26:37.234423197-05:00","close_reason":"cicd-fundamentals is documentation-only, no deployable app for Talos","labels":["talos-testing"]}
{"id":"illm-k8s-ai-lab-fh1.1","title":"cicd-fundamentals is documentation-only, no deployable app","description":"This experiment describes a CI/CD pipeline concept using GitHub Actions and ArgoCD Image Updater. There's no target/argocd/app.yaml to deploy. Either convert to deployable experiment or mark as N/A for Talos.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-13T08:26:36.912659227-05:00","created_by":"illm","updated_at":"2026-01-13T20:39:07.790244819-05:00","closed_at":"2026-01-13T20:39:07.790244819-05:00","close_reason":"Documentation-only experiment describing CI/CD architecture. No deployable workload - references hello-app which is a separate experiment. N/A for Talos testing.","labels":["talos-testing"],"dependencies":[{"issue_id":"illm-k8s-ai-lab-fh1.1","depends_on_id":"illm-k8s-ai-lab-fh1","type":"parent-child","created_at":"2026-01-13T08:26:36.913930107-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-gak","title":"Envoy Gateway Helm chart doesn't create GatewayClass","description":"The Envoy Gateway Helm chart (v1.2.4 via GitHub source) doesn't create the 'eg' GatewayClass. This prevents Gateway resources from being programmed. Had to create it manually. Should add GatewayClass to manifests or values.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-15T17:33:57.761702306-05:00","created_by":"illm","updated_at":"2026-01-15T17:49:50.764853001-05:00","closed_at":"2026-01-15T17:49:50.764853001-05:00","close_reason":"Fixed: Added GatewayClass to manifests, added grpcurl test pod, documented OCI workaround","labels":["config","gateway-tutorial"]}
{"id":"illm-k8s-ai-lab-h2o","title":"Tier 1: Stop the bleeding — ArgoCD green, remove broken compositions","description":"Epic: Get ArgoCD to a clean state and triage existing beads.\n\nSubtasks:\n1. Fix crossplane-xrds OutOfSync — remove or convert non-functional legacy compositions (caches, databases, experiment-clusters, queues) that fail Pipeline mode validation. Keep XRD definitions.\n2. Investigate and fix other OutOfSync apps: cert-manager, hub, seaweedfs.\n3. Triage all 13 open beads against current 5-day-old cluster — close anything already resolved.\n4. Decide on AWS/Azure provider installation vs removing untestable compositions.\n\nRelated existing beads:\n- dq9 (P1): Control plane crashes during Crossplane CRD install — may be resolved or needs revalidation\n- jne (P2): StatefulSet spec changes require deletion — affects Mimir/Loki redeploy\n- dcm (P2): SeaweedFS PostSync hook doesn't trigger — affects seaweedfs OutOfSync\n- ajb (P2): ArgoCD CreateNamespace fails after deletion\n\nSuccess criteria: All ArgoCD apps either Synced+Healthy or intentionally excluded. No spurious sync errors.","status":"in_progress","priority":1,"issue_type":"task","created_at":"2026-02-02T17:30:44.063285184-05:00","created_by":"illm","updated_at":"2026-02-02T17:33:03.946949086-05:00","labels":["cleanup"]}
{"id":"illm-k8s-ai-lab-ibc","title":"SeaweedFS idx volume must be persistent","description":"SeaweedFS volume server crashed with 'idx file does not exist' after pod restarts. Root cause: Helm chart defaults idx volume to emptyDir, losing index files on restart while data files persist. Fixed by configuring idx as PVC in seaweedfs.yaml.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-25T20:04:40.618887707-05:00","created_by":"illm","updated_at":"2026-01-25T20:04:46.78196279-05:00","closed_at":"2026-01-25T20:04:46.78196279-05:00","close_reason":"Fixed by changing idx volume from emptyDir to PVC in platform/hub/values/seaweedfs.yaml","labels":["config","seaweedfs","toil"]}
{"id":"illm-k8s-ai-lab-ibx","title":"Namespaces need explicit PSS labels for operators requiring elevated permissions","description":"Several operators (Tailscale, potentially others) require privileged PodSecurityStandards. Without explicit namespace labels, pods fail with PodSecurity violations. Fixed for tailscale namespace in platform/hub/manifests/tailscale-config/namespace.yaml. May need to audit other namespaces: kyverno, external-secrets, cert-manager, etc.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-26T09:57:56.419250386-05:00","created_by":"illm","updated_at":"2026-01-26T12:00:03.603314045-05:00","closed_at":"2026-01-26T12:00:03.603314045-05:00","close_reason":"Audited all namespaces. Tailscale fixed in commit c734e9b. Other namespaces (cert-manager, crossplane-system, external-secrets) have no PSS violations - they work with restricted profile.","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-iku","title":"gateway-tutorial: Ingress → Gateway API evolution","description":"Deep tutorial covering Ingress basics, hitting limitations, migrating to Gateway API, and Gateway API deep dive. Deploys to Kind/Talos.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-15T09:20:18.673748521-05:00","created_by":"illm","updated_at":"2026-01-15T09:24:07.777746595-05:00","closed_at":"2026-01-15T09:24:07.777746595-05:00","close_reason":"Wrong tracking mechanism - these are roadmap items, not toil"}
{"id":"illm-k8s-ai-lab-j4c","title":"PVC finalizers block namespace deletion during reset","description":"PVCs with kubernetes.io/pvc-protection finalizer block namespace deletion. Need to patch PVC finalizers explicitly in cleanup step.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-25T20:38:52.657686193-05:00","created_by":"illm","updated_at":"2026-01-26T08:32:16.08575496-05:00","closed_at":"2026-01-26T08:32:16.08575496-05:00","close_reason":"Fixed by adding explicit PVC finalizer cleanup in Taskfile Step 5","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-jne","title":"StatefulSet spec changes require deletion before redeploy","description":"When Helm chart values change StatefulSet spec fields beyond allowed (replicas, template, etc), ArgoCD sync fails with 'spec: Forbidden: updates to statefulset spec for fields other than...' error. Workaround: delete the StatefulSet with --cascade=orphan, clear ArgoCD operation state, then sync again. Affects Mimir and Loki on redeployment.","status":"open","priority":2,"issue_type":"bug","created_at":"2026-01-24T17:28:20.886348008-05:00","created_by":"illm","updated_at":"2026-01-24T17:28:20.886348008-05:00","labels":["argocd","helm","toil"]}
{"id":"illm-k8s-ai-lab-jze","title":"Deploy Mimir on hub cluster","description":"Deploy Mimir for metrics aggregation from all experiment clusters via Grafana Agent remote_write.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T12:46:17.932558007-05:00","created_by":"illm","updated_at":"2026-01-24T16:48:18.655994425-05:00","closed_at":"2026-01-24T16:48:18.655994425-05:00","close_reason":"Mimir deployed as part of LGTM observability stack with SeaweedFS backend","dependencies":[{"issue_id":"illm-k8s-ai-lab-jze","depends_on_id":"illm-k8s-ai-lab-9p4","type":"blocks","created_at":"2026-01-17T12:46:41.070334998-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-kso","title":"Jaeger image tag 1.62 doesn't exist","description":"jaegertracing/all-in-one:1.62 not found on Docker Hub. Need to update to a valid version.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-12T21:50:35.346086307-05:00","created_by":"illm","updated_at":"2026-01-12T21:50:49.937757209-05:00","closed_at":"2026-01-12T21:50:49.937757209-05:00","close_reason":"Closed","labels":["toil","tracing-comparison"]}
{"id":"illm-k8s-ai-lab-lp8","title":"Stale Helm pending-install blocks ArgoCD upgrade on deploy","description":"After a failed or interrupted deploy, Helm can leave ArgoCD in pending-install state. helm upgrade --install then fails. Deploy task needs to detect and clear stale releases.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-28T14:30:54.913596063-05:00","created_by":"illm","updated_at":"2026-01-28T15:01:06.587256308-05:00","closed_at":"2026-01-28T15:01:06.587256308-05:00","close_reason":"Fixed in deploy task: Step 1 applies machine config if node is in maintenance mode; Helm guard clears stale pending-install releases before upgrade","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-moe","title":"Kyverno webhooks block pod creation when admission controller is down","description":"When Kyverno admission controller is unhealthy (CrashLoopBackOff), its mutating/validating webhooks remain registered and block all pod creation with 'failed calling webhook: dial tcp: connection refused'. Workaround: manually delete kyverno-*-webhook-cfg resources. Consider setting failurePolicy: Ignore for non-critical environments.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-26T09:57:49.182199998-05:00","created_by":"illm","updated_at":"2026-01-26T11:58:13.326389159-05:00","closed_at":"2026-01-26T11:58:13.326389159-05:00","close_reason":"Fixed in commit 847bfc6","labels":["kyverno","networking","toil"]}
{"id":"illm-k8s-ai-lab-n0e","title":"tracing-comparison uses non-existent ghcr.io images","description":"Same as otel-tutorial - otel-demo services use ghcr.io images that don't exist.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-12T21:31:20.370836536-05:00","created_by":"illm","updated_at":"2026-01-12T21:32:06.06718126-05:00","closed_at":"2026-01-12T21:32:06.06718126-05:00","close_reason":"Closed","labels":["toil","tracing-comparison"]}
{"id":"illm-k8s-ai-lab-nd0","title":"otel-tutorial missing demo app images","description":"## Issue\notel-tutorial deployment fails due to missing container images.\n\n## Failing Images\n- `ghcr.io/illmadecoder/otel-demo-user-service:latest` - 403 Forbidden\n- `ghcr.io/illmadecoder/otel-demo-order-service:latest` - 403 Forbidden  \n- `ghcr.io/illmadecoder/otel-demo-payment-service:latest` - 403 Forbidden\n\n## Working Components\n- Grafana: Running\n- Prometheus: Running\n- OTEL Collector: Running\n- Tempo: Running\n\n## Action Required\nBuild and push demo app images, or replace with existing public OTEL demo images.\n\n## File\n`experiments/scenarios/otel-tutorial/manifests/otel-demo-app.yaml`\n","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-12T07:52:09.999549Z","updated_at":"2026-01-12T20:26:35.325050174-05:00","closed_at":"2026-01-12T20:26:35.325050174-05:00","close_reason":"Superseded by illm-k8s-ai-lab-999 fix"}
{"id":"illm-k8s-ai-lab-ong","title":"Tier 2: Stabilize observability stack — all Progressing apps to Healthy","description":"Epic: Get grafana, loki, mimir, tempo, seaweedfs from Progressing to Healthy.\n\nSubtasks:\n1. Diagnose why each is stuck in Progressing (PVC issues? resource limits? missing config? readiness probes?).\n2. Fix SeaweedFS — both the app and its config are Progressing + OutOfSync.\n3. Fix Loki, Mimir, Tempo — the metrics/logs/traces pipeline.\n4. Fix Grafana — visualization depends on the above backends.\n5. Validate end-to-end: can a pod's metrics/logs/traces flow through to Grafana?\n\nRelated existing beads:\n- cl1 (P1): SeaweedFS S3 endpoint stuck in notReadyAddresses — directly related\n- dcm (P2): SeaweedFS PostSync hook doesn't trigger — bucket creation issue\n- jne (P2): StatefulSet spec changes require deletion — affects Mimir/Loki\n- s9m (P1): Kyverno CrashLoopBackOff — may cause admission webhook interference\n- ewz (P0): Scheduler stuck after reboot — could be root cause of Progressing state\n\nSuccess criteria: All 5 observability apps show Synced+Healthy. Grafana can display metrics from Mimir, logs from Loki, traces from Tempo.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-02T17:30:58.283482322-05:00","created_by":"illm","updated_at":"2026-02-02T17:30:58.283482322-05:00","labels":["cleanup observability"],"dependencies":[{"issue_id":"illm-k8s-ai-lab-ong","depends_on_id":"illm-k8s-ai-lab-h2o","type":"blocks","created_at":"2026-02-02T17:31:26.776435972-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-pcj","title":"Talos enforces Pod Security Standards - DaemonSets blocked","description":"Talos enforces PSS baseline by default. DaemonSets like promtail, node-exporter that need hostPath/hostNetwork/hostPID are blocked. Fix: label namespaces with pod-security.kubernetes.io/enforce=privileged. Consider adding this to talos:up task or ArgoCD app.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-12T20:36:20.437720096-05:00","created_by":"illm","updated_at":"2026-01-12T20:37:33.480990498-05:00","closed_at":"2026-01-12T20:37:33.480990498-05:00","close_reason":"Fixed by auto-labeling namespace in talos:up task","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-png","title":"Scheduler and controller-manager failing startup probes","status":"closed","priority":0,"issue_type":"bug","created_at":"2026-01-26T22:02:54.161135379-05:00","created_by":"illm","updated_at":"2026-01-26T22:11:21.739456076-05:00","closed_at":"2026-01-26T22:11:21.739456076-05:00","close_reason":"Duplicate of illm-k8s-ai-lab-ewz - scheduler issue","labels":["timing","toil"]}
{"id":"illm-k8s-ai-lab-r5e","title":"OpenBao PVC→hostPath migration requires manual StatefulSet delete","description":"One-time issue: switching OpenBao from PVC (local-path-retain) to hostPath requires deleting the StatefulSet (volumeClaimTemplates are immutable) and PVC/PV. ArgoCD then recreates with hostPath. Not a recurring issue — only happens during the migration.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-28T15:18:13.94031291-05:00","created_by":"illm","updated_at":"2026-01-28T15:18:24.7114629-05:00","closed_at":"2026-01-28T15:18:24.7114629-05:00","close_reason":"One-time migration completed. StatefulSet, PVC, and PVs deleted manually. ArgoCD recreated with hostPath. Won't recur.","labels":["config"]}
{"id":"illm-k8s-ai-lab-r8c","title":"Control-plane taint returns after reboot","status":"open","priority":1,"issue_type":"bug","created_at":"2026-01-26T21:47:11.646643734-05:00","created_by":"illm","updated_at":"2026-01-26T21:47:11.646643734-05:00","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-rle","title":"Duplicate cluster secrets cause ArgoCD sync failures","description":"Two different secret naming patterns are used:\n- kind:up creates 'argocd-cluster-{cluster}' (line ~504)\n- kind:conduct creates 'cluster-{cluster}' (line ~794)\n- kind:down only deletes 'cluster-{cluster}'\n\nThis leaves orphaned 'argocd-cluster-*' secrets that cause 'there are 2 clusters with the same name' errors.\n\nFix: Standardize on one naming pattern and clean up both in down task.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T13:41:50.937981592-05:00","created_by":"illm","updated_at":"2026-01-10T13:45:51.370671733-05:00","closed_at":"2026-01-10T13:45:51.370671733-05:00","close_reason":"Closed","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-s9m","title":"Kyverno admission controller CrashLoopBackOff on etcd timeouts","description":"Kyverno admission controller enters CrashLoopBackOff due to etcd/API server timeouts. Logs show 'etcdserver: request timed out' and 'failed to renew lease'.\n\nInvestigation findings:\n- Resource usage is low (7m CPU, 40Mi memory) - not resource exhaustion\n- No Istio sidecar (istio-injection=disabled on namespace)\n- Likely intermittent API server load during cluster startup\n- Currently stable with 36 restarts but running\n\nMitigations applied:\n- failurePolicy: Ignore prevents cluster-wide pod blocking (commit 847bfc6)\n\nRemaining:\n- Monitor for recurrence\n- Consider increasing liveness probe timeouts if issue persists","status":"open","priority":1,"issue_type":"bug","created_at":"2026-01-26T09:57:35.316491598-05:00","created_by":"illm","updated_at":"2026-01-26T11:58:53.394692311-05:00","labels":["kyverno","timing","toil"]}
{"id":"illm-k8s-ai-lab-t4h","title":"Deploy PostgreSQL on hub cluster","description":"Deploy PostgreSQL for experiment results storage. Schema includes: run metadata, metrics summaries, budget tracking, JSONB columns.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-17T12:46:10.871937093-05:00","created_by":"illm","updated_at":"2026-01-17T12:46:10.871937093-05:00","dependencies":[{"issue_id":"illm-k8s-ai-lab-t4h","depends_on_id":"illm-k8s-ai-lab-9p4","type":"blocks","created_at":"2026-01-17T12:46:40.857343699-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-tel","title":"gateway-tutorial requires grpcurl for Part 5","description":"Part 5 gRPC tests require grpcurl to be installed. The tutorial assumes grpcurl is available but doesn't include installation instructions. Should either add grpcurl install steps or use a grpcurl pod for testing.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-15T17:39:29.586369657-05:00","created_by":"illm","updated_at":"2026-01-15T17:49:50.772898456-05:00","closed_at":"2026-01-15T17:49:50.772898456-05:00","close_reason":"Fixed: Added GatewayClass to manifests, added grpcurl test pod, documented OCI workaround","labels":["gateway-tutorial"]}
{"id":"illm-k8s-ai-lab-u10","title":"Kyverno webhooks block namespace deletion after uninstall","description":"When deleting namespaces after removing Kyverno, the validating webhooks (validate.kyverno.svc-fail) block resource deletion because the webhook service no longer exists. Need to clean up MutatingWebhookConfiguration and ValidatingWebhookConfiguration resources.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-24T15:07:33.074785248-05:00","created_by":"illm","updated_at":"2026-01-24T17:28:34.15253213-05:00","closed_at":"2026-01-24T17:28:34.15253213-05:00","close_reason":"Addressed by talos-hub Taskfile automation (task talos-hub:reset handles webhook cleanup, finalizer removal, and OpenBao re-init)","labels":["kyverno","toil"]}
{"id":"illm-k8s-ai-lab-vek","title":"ArgoCD deployments missing app.kubernetes.io/instance label","description":"After cluster reset and ArgoCD redeploy, deployment templates are missing app.kubernetes.io/instance label required by service selectors. This causes pods to not be registered as endpoints. Workaround: manually patch deployments to add the label.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-26T08:30:15.293829645-05:00","created_by":"illm","updated_at":"2026-01-26T12:01:38.153694672-05:00","closed_at":"2026-01-26T12:01:38.153694672-05:00","close_reason":"Fixed in commit 797208a - added podLabels with instance label to all ArgoCD components","labels":["argocd","toil"]}
{"id":"illm-k8s-ai-lab-viw","title":"ArgoCD pods missing container port names after Istio injection","description":"When ArgoCD pods are injected with Istio sidecars, the container port names are lost. Services using targetPort by name (repo-server, http, grpc) can't find endpoints. Workaround: patch services to use numeric targetPort instead of named ports.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-26T08:30:08.73530669-05:00","created_by":"illm","updated_at":"2026-01-26T12:01:38.313519971-05:00","closed_at":"2026-01-26T12:01:38.313519971-05:00","close_reason":"Transient issue - pods have proper port names, endpoints are working. Issue was during initial startup race condition.","labels":["argocd","istio","toil"]}
{"id":"illm-k8s-ai-lab-vxu","title":"Crossplane cloud provider credentials lost - need to restore","description":"Crossplane cloud provider credentials were lost when OpenBao was reinitialized.\n\nSTATUS: Azure credentials restored to secret/cloud/azure\n- ExternalSecret now syncs successfully\n\nREMAINING ISSUE: Providers have package dependency conflict:\n'cannot resolve package dependencies: node provider-family-azure already exists'\nThis is a separate Crossplane packaging issue, not credentials.\n\nAWS and GCP credentials still need to be created and stored.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-26T10:02:43.185017453-05:00","created_by":"illm","updated_at":"2026-01-26T14:27:31.340068822-05:00","closed_at":"2026-01-26T14:27:31.340068822-05:00","close_reason":"All cloud credentials restored to OpenBao:\n- AWS: secret/cloud/aws (accessKeyId, secretAccessKey)\n- Azure: secret/cloud/azure (clientId, clientSecret, tenantId, subscriptionId)\n- GCP: secret/cloud/gcp (credentials JSON)\n\nNote: Crossplane providers still have package conflict issue (separate from credentials).","labels":["config","crossplane"]}
{"id":"illm-k8s-ai-lab-wod","title":"Talos prometheus-tutorial deployment validation - 2026-01-12","description":"## Deployment Test Results\n\nFull tear-down and redeploy of prometheus-tutorial on Talos cluster.\n\n### Timing Summary\n| Phase | Duration |\n|-------|----------|\n| Tear down | 0s |\n| Reset cluster | 21s |\n| Create ArgoCD app | 0s |\n| Trigger sync | 0s |\n| Wait for sync | 24s |\n| Wait for pods ready | 6s |\n| **Total deploy** | **30s** |\n| URL validation | 5s |\n\n### URL Validation Results\n| Service | URL | HTTP Code | Response Time |\n|---------|-----|-----------|---------------|\n| Grafana | 192.168.1.241:80 | 302 | 0.018s |\n| Prometheus | 192.168.1.242:9090 | 200 | 0.017s |\n| metrics-app | 192.168.1.240:80 | 200 | 0.017s |\n| station-monitor | 192.168.1.243:80 | 200 | 0.078s |\n\n### Issues Fixed During Session\n1. **RBAC permissions for kube-vip** - Cloud provider needed `configmaps` (create/update) and `events` (create/patch) permissions\n2. **ArgoCD ComparisonError** - Added `ignoreDifferences` for ReplicaSet/Deployment status fields\n\n### Commits\n- `92ce6ee` - fix: Add ignoreDifferences for ReplicaSet/Deployment status fields\n- `496c699` - fix: Add missing RBAC permissions for kube-vip cloud provider\n\n### Performance Notes\n- Sync phase (24s) is the bottleneck - waiting for ArgoCD to reconcile\n- All URL response times excellent (\u003c0.1s)\n- station-monitor slightly slower (0.078s vs ~0.017s for others)\n","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-12T07:11:21.367117Z","updated_at":"2026-01-12T07:11:21.367117Z","closed_at":"2026-01-12T07:11:21.367117Z"}
{"id":"illm-k8s-ai-lab-wup","title":"Envoy Gateway v1.2.4 CRD schema changes","description":"Envoy Gateway v1.2.4 changed CRD schemas: retryOn must use triggers array inside object, allowOrigins must be plain strings not type/value objects. Fixed manifests to match new schema.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-15T13:52:23.4655385-05:00","created_by":"illm","updated_at":"2026-01-15T13:52:38.867035014-05:00","closed_at":"2026-01-15T13:52:38.867035014-05:00","close_reason":"Fixed by updating to v1.2.4 schema","labels":["gateway-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-x1z","title":"ArgoCD app stuck in terminating state after cluster deletion","description":"When tearing down experiments, we delete the ArgoCD app with --wait=false then immediately delete the Kind cluster. But the app has a resources-finalizer that requires ArgoCD to clean up resources on the target cluster. Since the cluster is already deleted, the finalizer can never complete and the app is stuck forever in Terminating state.\n\nObserved: deletionTimestamp set but app not deleted due to finalizer.\n\nImpact: Subsequent deploys see 'resource is currently being deleted' warning and may not work correctly.\n\nFix options:\n1. Wait for app deletion before cluster deletion (slow)\n2. Remove finalizer before deleting app when cluster is being destroyed\n3. Use cascade=orphan to skip resource cleanup","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T13:34:12.184556089-05:00","created_by":"illm","updated_at":"2026-01-10T13:45:51.154939305-05:00","closed_at":"2026-01-10T13:45:51.154939305-05:00","close_reason":"Closed","labels":["config","timing","toil"]}
{"id":"illm-k8s-ai-lab-y7o","title":"Rebuild otel-demo images with non-root user","description":"otel-demo container images run as root. Need to rebuild with USER directive in Dockerfile to support runAsNonRoot for full restricted PSS compliance.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-13T16:44:26.352319843-05:00","created_by":"illm","updated_at":"2026-01-13T20:32:27.159340479-05:00","closed_at":"2026-01-13T20:32:27.159340479-05:00","close_reason":"Rebuilt images with numeric UID (USER 1000) to satisfy Kubernetes runAsNonRoot verification. All pods now running with restricted PSS on Talos.","labels":["config","otel-tutorial"]}
{"id":"illm-k8s-ai-lab-y7w","title":"nginx-ingress snippets require annotations-risk-level","description":"nginx-ingress v1.9+ silently ignores snippet annotations without annotations-risk-level: Critical. This caused header-based-routing and header-manipulation ingresses to not get addresses. Added config fix.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-15T16:44:08.561735017-05:00","created_by":"illm","updated_at":"2026-01-15T17:41:20.037028683-05:00","closed_at":"2026-01-15T17:41:20.037028683-05:00","close_reason":"Fixed during gateway-tutorial validation - added annotations-risk-level to nginx-ingress config and changed grpcbin from gRPC to TCP probes","labels":["config","gateway-tutorial"]}
{"id":"illm-k8s-ai-lab-z07","title":"Test tsdb-comparison on Talos","description":"Test tsdb-comparison experiment on Talos platform. Goal: 'task talos:up -- tsdb-comparison' works with zero manual intervention. Cycle the experiment, identify and fix any toil. Create child beads for each issue discovered during testing.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T08:03:04.322198693-05:00","created_by":"illm","updated_at":"2026-01-13T09:21:19.089449253-05:00","closed_at":"2026-01-13T09:21:19.089449253-05:00","close_reason":"tsdb-comparison deploys successfully on Talos with all apps Healthy","labels":["talos-testing"]}
{"id":"illm-k8s-ai-lab-z07.1","title":"tsdb-comparison apps need ignoreDifferences for terminatingReplicas","description":"Kubernetes 1.32+ adds terminatingReplicas field. Need to add ignoreDifferences to prometheus, mimir, and workload apps.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-13T08:53:04.594401497-05:00","created_by":"illm","updated_at":"2026-01-13T09:21:19.083036006-05:00","closed_at":"2026-01-13T09:21:19.083036006-05:00","close_reason":"tsdb-comparison deploys successfully on Talos with all apps Healthy","labels":["talos-testing"],"dependencies":[{"issue_id":"illm-k8s-ai-lab-z07.1","depends_on_id":"illm-k8s-ai-lab-z07","type":"parent-child","created_at":"2026-01-13T08:53:04.595745045-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-z5z","title":"grpcbin doesn't support gRPC health protocol","description":"moul/grpcbin image doesn't implement grpc.health.v1.Health. K8s native gRPC health probes fail. Currently using TCP probes as workaround. Should create custom gRPC demo image that implements health protocol for proper tutorial.","status":"closed","priority":3,"issue_type":"bug","created_at":"2026-01-15T16:25:54.711056073-05:00","created_by":"illm","updated_at":"2026-01-15T17:41:20.051443669-05:00","closed_at":"2026-01-15T17:41:20.051443669-05:00","close_reason":"Fixed during gateway-tutorial validation - added annotations-risk-level to nginx-ingress config and changed grpcbin from gRPC to TCP probes","labels":["gateway-tutorial"]}
