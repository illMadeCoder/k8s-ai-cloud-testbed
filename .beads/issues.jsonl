{"id":"illm-k8s-ai-lab-085","title":"Cycle wait uses talosctl version without --insecure, times out in maintenance mode","description":"After STATE wipe, node enters maintenance mode with different TLS. The cycle task polls with talosctl version (authenticated) which always fails. Needs --insecure flag to detect the node is back in maintenance mode.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-28T17:34:16.386550015-05:00","created_by":"illm","updated_at":"2026-01-28T17:35:15.634192475-05:00","closed_at":"2026-01-28T17:35:15.634192475-05:00","close_reason":"Fixed: cycle wait now uses talosctl version --insecure to detect maintenance mode node","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-08q","title":"otel-demo services crash: OTel schema conflict","description":"Services fail with: Failed to initialize tracer: failed to create resource: conflicting Schema URL: https://opentelemetry.io/schemas/1.26.0 and https://opentelemetry.io/schemas/1.24.0. Need to align OTel SDK versions.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-12T20:09:43.504031341-05:00","created_by":"illm","updated_at":"2026-01-12T20:15:08.112598336-05:00","closed_at":"2026-01-12T20:15:08.112598336-05:00","close_reason":"Fixed by upgrading OTel SDK to v1.33.0","labels":["otel-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-0mh","title":"elk-tutorial: Create experiment.yaml and component CRs","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-06T18:28:19.297621491-05:00","created_by":"illm","updated_at":"2026-02-06T18:31:04.654120639-05:00","closed_at":"2026-02-06T18:31:04.654120639-05:00","close_reason":"Created elasticsearch, kibana, fluent-bit component CRs and experiment.yaml deploying ELK stack to GKE"}
{"id":"illm-k8s-ai-lab-0mr","title":"tracing-comparison: Create experiment.yaml","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-06T18:28:22.115336943-05:00","created_by":"illm","updated_at":"2026-02-06T18:31:07.893093815-05:00","closed_at":"2026-02-06T18:31:07.893093815-05:00","close_reason":"Created jaeger component CR and experiment.yaml deploying Tempo+Jaeger to GKE for comparison"}
{"id":"illm-k8s-ai-lab-0og","title":"gateway-comparison: Create experiment.yaml and component CRs","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-06T18:28:26.342518924-05:00","created_by":"illm","updated_at":"2026-02-06T18:31:12.722048146-05:00","closed_at":"2026-02-06T18:31:12.722048146-05:00","close_reason":"Created traefik component CR and experiment.yaml deploying 3-gateway comparison to GKE"}
{"id":"illm-k8s-ai-lab-0si","title":"Stale old node object persists after teardown/deploy cycle","description":"After STATE wipe and redeploy, the old node object (talos-jid-knd) remains with NotReady,SchedulingDisabled status. The new node (talos-0lj-hy1) is Ready. The stale node doesn't cause functional issues but is confusing. Could auto-delete NotReady nodes during deploy step 3.","status":"closed","priority":3,"issue_type":"bug","created_at":"2026-01-28T14:43:19.289705199-05:00","created_by":"illm","updated_at":"2026-01-28T15:01:53.09852084-05:00","closed_at":"2026-01-28T15:01:53.09852084-05:00","close_reason":"Fixed in deploy task: Step 3 now deletes stale NotReady/SchedulingDisabled nodes after the new node is Ready","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-0so","title":"Test observability-cost-tutorial on Talos","description":"Test observability-cost-tutorial experiment on Talos platform. Goal: 'task talos:up -- observability-cost-tutorial' works with zero manual intervention. Cycle the experiment, identify and fix any toil. Create child beads for each issue discovered during testing.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T08:03:00.484077037-05:00","created_by":"illm","updated_at":"2026-01-13T08:25:50.747852387-05:00","closed_at":"2026-01-13T08:25:50.747852387-05:00","close_reason":"observability-cost-tutorial deploys successfully on Talos with all workloads healthy","labels":["talos-testing"]}
{"id":"illm-k8s-ai-lab-0u1","title":"Hub VM custom queries returned empty for gw-comp-b5jbs","description":"During gw-comp-b5jbs completion, the operator's hub VictoriaMetrics custom queries returned empty for all 20 metrics. cadvisor scrape only got 5 pods. Missing: envoy-default data-plane proxy pods, k6 load test results. Possible cause: metrics-agent/metrics-egress backhaul not working, or VictoriaMetrics retention/scrape not covering the experiment label. The analyzer noted this makes the comparison inconclusive.","status":"open","priority":2,"issue_type":"bug","created_at":"2026-02-13T18:57:42.13657481-05:00","created_by":"illm","updated_at":"2026-02-13T18:57:42.13657481-05:00","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-0vl","title":"Crossplane providers overwhelming N100 - reduce provider count","description":"ROOT CAUSE FOUND: Providers OOM killed at 256Mi limit (using 258MB). Fixed by increasing limits to 512Mi in commit caee555. Still need MRAPs for long-term CRD reduction.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-17T16:27:00.59944283-05:00","created_by":"illm","updated_at":"2026-01-24T12:35:27.996341824-05:00","closed_at":"2026-01-24T12:35:27.996341824-05:00","close_reason":"Fixed: Reduced to 2 providers (helm, kubernetes), memory usage now ~340Mi total","labels":["crossplane","resources","talos"]}
{"id":"illm-k8s-ai-lab-18z","title":"Create Grafana Agent config for target clusters","description":"Create Helm values for Grafana Agent on target clusters with remote_write to hub Mimir. Lightweight (~100MB) observability.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T12:46:21.842940748-05:00","created_by":"illm","updated_at":"2026-02-02T18:41:42.478841853-05:00","closed_at":"2026-02-02T18:41:42.478841853-05:00","close_reason":"Closed: Requires target clusters which do not exist yet (single-node hub only). Reopen when target cluster provisioning is implemented."}
{"id":"illm-k8s-ai-lab-1cr","title":"Target cluster missing external-secrets CRDs","description":"ExternalSecrets fail on target cluster because external-secrets-operator isn't installed. Need to either install ESO on target or use different secret approach for tutorial.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-15T13:33:05.897483823-05:00","created_by":"illm","updated_at":"2026-01-15T13:37:44.917640719-05:00","closed_at":"2026-01-15T13:37:44.917640719-05:00","close_reason":"Fixed by reverting ExternalSecrets to regular Secrets"}
{"id":"illm-k8s-ai-lab-1e1","title":"Experiment namespaces need pod-security labels for privileged workloads","description":"ArgoCD creates the experiment namespace via CreateNamespace=true syncOption, but the namespace gets default PodSecurity 'baseline' enforcement. Promtail (and other DaemonSets like node-exporter) need hostPath volumes which require 'privileged' enforcement. The operator or ArgoCD app should set pod-security.kubernetes.io/enforce=privileged on the namespace. Options: (1) operator creates namespace pre-labeled before ArgoCD sync, (2) add Kyverno ClusterPolicy to auto-label experiment namespaces, (3) add namespace manifest as one of the ArgoCD sources.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-02-06T13:57:40.197878839-05:00","created_by":"illm","updated_at":"2026-02-06T14:09:20.736711479-05:00","closed_at":"2026-02-06T14:09:20.736711479-05:00","close_reason":"Fixed: operator pre-creates namespace with pod-security.kubernetes.io/enforce=privileged label","labels":["toil"]}
{"id":"illm-k8s-ai-lab-1es","title":"Analyzer failed: Claude credentials expired in OpenBao","description":"experiment-analyzer-gw-comp-b5jbs failed because Claude CLI auth failed. The credentials.json stored in OpenBao needs manual refresh. This is a recurring toil issue — credentials expire and need re-uploading. Error: 'Claude CLI auth failed — credentials may need manual refresh in OpenBao'.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-02-13T18:52:12.887961053-05:00","created_by":"illm","updated_at":"2026-02-13T19:02:10.788619212-05:00","closed_at":"2026-02-13T19:02:10.788619212-05:00","close_reason":"Fixed by refreshing Claude credentials in OpenBao and manually re-creating analyzer job. Analysis completed successfully (5 passes, 29KB analysis).","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-1ex","title":"Hub app deletion stuck on finalizer during reset","description":"When deleting the hub app-of-apps, the resources-finalizer.argocd.argoproj.io finalizer prevents deletion even after 10+ minutes. Child apps don't have owner references so cascade delete doesn't work as expected. Manual finalizer removal required.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-24T15:01:47.451643224-05:00","created_by":"illm","updated_at":"2026-01-24T17:28:34.160147662-05:00","closed_at":"2026-01-24T17:28:34.160147662-05:00","close_reason":"Addressed by talos-hub Taskfile automation (task talos-hub:reset handles webhook cleanup, finalizer removal, and OpenBao re-init)","labels":["argocd","toil"]}
{"id":"illm-k8s-ai-lab-1mw","title":"Analyzer Claude OAuth token expired — all passes fail with 401","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-02-12T13:57:30.696674864-05:00","created_by":"illm","updated_at":"2026-02-12T15:40:42.734778467-05:00","closed_at":"2026-02-12T15:40:42.734778467-05:00","close_reason":"Fixed: refreshed Claude OAuth credentials in OpenBao, reanalysis jobs succeeded"}
{"id":"illm-k8s-ai-lab-26g","title":"Gateway GKE node pool stuck in AsyncCreateFailure — Crossplane race condition","status":"open","priority":2,"issue_type":"bug","created_at":"2026-02-12T14:30:55.410190703-05:00","created_by":"illm","updated_at":"2026-02-12T14:30:55.410190703-05:00","labels":["gateway-comparison"]}
{"id":"illm-k8s-ai-lab-26k","title":"OpenBao needs re-initialization after Talos namespace reset","description":"On Talos hub, OpenBao uses local-path PVC storage. When the namespace is deleted during reset, all vault data is lost and OpenBao needs re-initialization. Keys from previous installs (in ~/.illmlab/openbao-keys.json) won't work. Consider using persistent hostPath mount like Kind does, or implement backup/restore process.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-24T15:16:51.150222183-05:00","created_by":"illm","updated_at":"2026-01-24T17:28:34.16831777-05:00","closed_at":"2026-01-24T17:28:34.16831777-05:00","close_reason":"Addressed by talos-hub Taskfile automation (task talos-hub:reset handles webhook cleanup, finalizer removal, and OpenBao re-init)","labels":["openbao","talos","toil"]}
{"id":"illm-k8s-ai-lab-2bj","title":"GKE bootstrap RBAC fails from operator pod — no GCP ADC, Crossplane SA key fallback doesn't work","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-02-06T20:45:14.475922306-05:00","created_by":"illm","updated_at":"2026-02-07T09:16:39.507213914-05:00","closed_at":"2026-02-07T09:16:39.507213914-05:00","close_reason":"Custom http.Transport on rest.Config bypassed bearer token injection. Removed it — client-go handles TLS natively.","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-30m","title":"Kyverno webhook blocks Helm install after teardown/deploy recovery","description":"After recovering from a failed cycle, Kyverno's mutating webhook is registered but pods aren't ready. Helm pre-upgrade hooks fail with 'connection refused'. Deploy needs to clean stale webhooks before the Helm step.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-28T17:38:34.297855963-05:00","created_by":"illm","updated_at":"2026-01-28T17:43:01.393741544-05:00","closed_at":"2026-01-28T17:43:01.393741544-05:00","close_reason":"Added webhook cleanup to deploy Step 4 before Helm install","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-351","title":"tsdb-comparison: Create experiment.yaml and component CRs","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-06T18:28:24.723988607-05:00","created_by":"illm","updated_at":"2026-02-06T18:31:10.761654518-05:00","closed_at":"2026-02-06T18:31:10.761654518-05:00","close_reason":"Created victoria-metrics, mimir, minio component CRs and experiment.yaml deploying TSDB comparison to GKE"}
{"id":"illm-k8s-ai-lab-39i","title":"ArgoCD OCI helm charts not working","description":"ArgoCD helm pull fails for OCI registries. May need ArgoCD config update or use YAML-based install instead.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-15T13:26:36.683603747-05:00","created_by":"illm","updated_at":"2026-01-15T17:49:50.778618288-05:00","closed_at":"2026-01-15T17:49:50.778618288-05:00","close_reason":"Fixed: Added GatewayClass to manifests, added grpcurl test pod, documented OCI workaround"}
{"id":"illm-k8s-ai-lab-3o4","title":"logging-comparison log-generator uses local image","description":"log-generator deployment uses 'log-generator:latest' which is a local image. Needs to be pushed to ttl.sh registry for Talos deployment.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-12T21:01:40.886143803-05:00","created_by":"illm","updated_at":"2026-01-12T21:02:03.002954628-05:00","closed_at":"2026-01-12T21:02:03.002954628-05:00","close_reason":"Closed","labels":["logging-comparison","toil"]}
{"id":"illm-k8s-ai-lab-4dw","title":"Test seaweedfs-tutorial on Talos","description":"Test seaweedfs-tutorial experiment on Talos platform. Goal: 'task talos:up -- seaweedfs-tutorial' works with zero manual intervention. Cycle the experiment, identify and fix any toil. Create child beads for each issue discovered during testing.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T08:03:03.00657484-05:00","created_by":"illm","updated_at":"2026-01-13T08:48:18.823497158-05:00","closed_at":"2026-01-13T08:48:18.823497158-05:00","close_reason":"SeaweedFS deploys successfully on Talos with PVC storage","labels":["talos-testing"]}
{"id":"illm-k8s-ai-lab-4dw.1","title":"SeaweedFS uses hostPath which fails on Talos read-only FS","description":"SeaweedFS helm chart default uses hostPath volumes at /ssd/ which fails on Talos (read-only filesystem). Need to configure persistence with PVC using local-path StorageClass instead.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-13T08:34:07.004796123-05:00","created_by":"illm","updated_at":"2026-01-13T08:48:18.801513116-05:00","closed_at":"2026-01-13T08:48:18.801513116-05:00","close_reason":"SeaweedFS deploys successfully on Talos with PVC storage","labels":["talos-testing"],"dependencies":[{"issue_id":"illm-k8s-ai-lab-4dw.1","depends_on_id":"illm-k8s-ai-lab-4dw","type":"parent-child","created_at":"2026-01-13T08:34:07.005996283-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-4dw.2","title":"SeaweedFS dashboard assumes monitoring namespace exists","description":"dashboard.yaml hardcoded to monitoring namespace. Either exclude dashboard/alerts from standalone deployment or add monitoring stack. For now, excluding with directory.exclude.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-13T08:41:04.851752414-05:00","created_by":"illm","updated_at":"2026-01-13T08:48:18.819881892-05:00","closed_at":"2026-01-13T08:48:18.819881892-05:00","close_reason":"SeaweedFS deploys successfully on Talos with PVC storage","labels":["talos-testing"],"dependencies":[{"issue_id":"illm-k8s-ai-lab-4dw.2","depends_on_id":"illm-k8s-ai-lab-4dw","type":"parent-child","created_at":"2026-01-13T08:41:04.861102428-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-4i9","title":"OOM during Crossplane v2 upgrade - reduce provider count","description":"Installing 25+ Crossplane providers simultaneously caused WSL to run out of memory (7.6GB total). Solution: In v2 family provider architecture, individual providers (like provider-aws-s3) only install CRDs - they don't run pods. We should install only family providers and let CRDs be installed on-demand.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-17T14:08:04.546335651-05:00","created_by":"illm","updated_at":"2026-01-17T14:11:09.497258534-05:00","closed_at":"2026-01-17T14:11:09.497258534-05:00","close_reason":"Fixed by reducing providers from 26 to 12"}
{"id":"illm-k8s-ai-lab-5cs","title":"Jaeger values.yaml has duplicate COLLECTOR_OTLP_ENABLED","description":"The allInOne.extraEnv adds COLLECTOR_OTLP_ENABLED but it already exists in the chart defaults, causing duplicate key error.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-12T21:46:17.63634854-05:00","created_by":"illm","updated_at":"2026-01-12T21:46:32.223955212-05:00","closed_at":"2026-01-12T21:46:32.223955212-05:00","close_reason":"Closed","labels":["toil","tracing-comparison"]}
{"id":"illm-k8s-ai-lab-5h2","title":"K8s gRPC health probes don't support TLS","description":"Kubernetes gRPC health probes don't support TLS. The grpc-healthcheck-demo was using port 9001 (TLS) which caused continuous TLS handshake failures. Fixed by using port 9000 (insecure) for health probes.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-15T13:49:32.060489683-05:00","created_by":"illm","updated_at":"2026-01-15T13:49:46.30314114-05:00","closed_at":"2026-01-15T13:49:46.30314114-05:00","close_reason":"Fixed by using insecure port 9000 for health probes","labels":["gateway-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-5yd","title":"ESO webhooks block namespace deletion during reset","description":"During talos-hub:reset, external-secrets webhooks remain after ESO is deleted, blocking tailscale namespace deletion. Need to delete external-secrets webhooks in Step 0.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-25T20:38:50.729243009-05:00","created_by":"illm","updated_at":"2026-01-26T08:32:15.892716377-05:00","closed_at":"2026-01-26T08:32:15.892716377-05:00","close_reason":"Fixed by adding external-secrets to webhook cleanup in Taskfile Step 0","labels":["external-secrets","networking","toil"]}
{"id":"illm-k8s-ai-lab-618","title":"ArgoCD terminatingReplicas comparison error on Talos","description":"Kubernetes 1.32+ adds terminatingReplicas to ReplicaSet/Deployment status. ArgoCD schema doesn't recognize it, causing ComparisonError.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-12T21:40:47.81587937-05:00","created_by":"illm","updated_at":"2026-01-12T21:41:26.548860557-05:00","closed_at":"2026-01-12T21:41:26.548860557-05:00","close_reason":"Closed","labels":["toil","tracing-comparison"]}
{"id":"illm-k8s-ai-lab-666","title":"Add local-path-provisioner to Talos bootstrap","description":"## Summary\nAdded local-path-provisioner to Talos platform bootstrap to support PVC-based workloads.\n\n## Changes\n- `platforms/talos/manifests/local-path-provisioner.yaml` - New manifest with:\n  - PodSecurity labels (privileged) on namespace\n  - Default StorageClass annotation\n- `platforms/talos/Taskfile.yaml`:\n  - Updated bootstrap task to install both kube-vip and local-path-provisioner\n  - Added local-path-storage to preserved namespaces\n\n## Context\nDiscovered during loki-tutorial deployment that Talos had no StorageClass, causing Loki PVC to stay Pending.\n\n## Commit\n`65ebb02` - feat: Add local-path-provisioner to Talos bootstrap\n","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-12T07:35:33.567206Z","updated_at":"2026-01-12T07:35:33.567206Z","closed_at":"2026-01-12T07:35:33.567206Z"}
{"id":"illm-k8s-ai-lab-68h","title":"Stale ArgoCD apps block new lab deployments","description":"When clusters are deleted via 'kind delete cluster', the ArgoCD apps remain in hub cluster and claim shared resources (ClusterRoles, ClusterRoleBindings, Namespaces). This causes new lab deployments to fail with 'is part of applications X and Y' errors. \n\nStale apps found: loki-tutorial-target, slo-tutorial-target, observability-cost-tutorial-target\n\nWorkaround: Manually delete stale ArgoCD apps before running new labs.\n\nRoot cause: kind:down task doesn't clean up ArgoCD apps when deleting clusters.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T13:13:39.350247155-05:00","created_by":"illm","updated_at":"2026-01-10T13:24:33.757053858-05:00","closed_at":"2026-01-10T13:24:33.757053858-05:00","close_reason":"Closed","labels":["config","prometheus-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-6hc","title":"Loki tutorial references wrong app label 'logging-app' instead of 'log-generator'","description":"The tutorial.yaml references {app=\"logging-app\"} in all LogQL examples, but the actual log-generator deployment has label app=log-generator. All LogQL queries in the tutorial need to use {app=\"log-generator\"} or the log-generator component needs a label override. Also references APP_IP endpoints (/health, /config) that may not exist in the log-generator deployment, and references a 'Log Analysis - Loki Tutorial' dashboard that doesn't exist. The Grafana instance also has no pre-configured Loki datasource.","status":"open","priority":2,"issue_type":"bug","created_at":"2026-02-06T14:03:47.777808209-05:00","created_by":"illm","updated_at":"2026-02-06T14:03:47.777808209-05:00","labels":["loki-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-6kp","title":"Test http-baseline on Talos","description":"Test http-baseline experiment on Talos platform. Goal: 'task talos:up -- http-baseline' works with zero manual intervention. Cycle the experiment, identify and fix any toil. Create child beads for each issue discovered during testing.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T08:02:57.890385649-05:00","created_by":"illm","updated_at":"2026-01-13T08:18:24.112057794-05:00","closed_at":"2026-01-13T08:18:24.112057794-05:00","close_reason":"http-baseline deploys successfully on Talos","labels":["talos-testing"]}
{"id":"illm-k8s-ai-lab-6kp.1","title":"Taskfile sed pattern doesn't handle comments in destination","description":"The talos:up sed pattern uses name: target$ which requires target at end of line. But http-baseline and other experiments have comments after target. Fix sed pattern to handle this.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-13T08:17:18.333322693-05:00","created_by":"illm","updated_at":"2026-01-13T08:18:24.10634152-05:00","closed_at":"2026-01-13T08:18:24.10634152-05:00","close_reason":"http-baseline deploys successfully on Talos","labels":["talos-testing"],"dependencies":[{"issue_id":"illm-k8s-ai-lab-6kp.1","depends_on_id":"illm-k8s-ai-lab-6kp","type":"parent-child","created_at":"2026-01-13T08:17:18.335151488-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-6qx","title":"Implement or remove unimplemented depends field on Target","description":"The Target type in experiment_types.go has a Depends []string field that is in the CRD schema but the operator ignores it — all targets are created in parallel. Either implement proper dependency graph traversal in reconcileProvisioning or remove the field from the CRD until it's needed. See experiment_controller.go:222 TODO comment.","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-06T12:15:53.237375542-05:00","created_by":"illm","updated_at":"2026-02-06T12:15:53.237375542-05:00"}
{"id":"illm-k8s-ai-lab-6t6","title":"grpcbin uses port 9000 for plaintext gRPC, not 9001","description":"All grpc-tutorial services were targeting port 9001 but grpcbin uses port 9000 for plaintext gRPC. This caused all gRPC tests to timeout. Fixed by changing targetPort to 9000.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-15T18:33:38.102675599-05:00","created_by":"illm","updated_at":"2026-01-15T18:33:44.193501852-05:00","closed_at":"2026-01-15T18:33:44.193501852-05:00","close_reason":"Fixed: Changed all service targetPort from 9001 to 9000","labels":["config","gateway-tutorial"]}
{"id":"illm-k8s-ai-lab-7eb","title":"Multi-namespace experiments need all namespaces PSS labeled","description":"Experiments like logging-comparison create multiple namespaces (loki, elasticsearch, monitoring, logging-comparison). The talos:up task only labels the main destination namespace, but DaemonSets like promtail/fluent-bit in sub-namespaces are blocked by PSS.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-12T21:03:00.810780882-05:00","created_by":"illm","updated_at":"2026-01-12T21:05:09.280020901-05:00","closed_at":"2026-01-12T21:05:09.280020901-05:00","close_reason":"Closed","labels":["logging-comparison","toil"]}
{"id":"illm-k8s-ai-lab-7lc","title":"gateway-tutorial: Create experiment.yaml and component CRs","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-06T18:28:25.396916821-05:00","created_by":"illm","updated_at":"2026-02-06T18:31:11.872588427-05:00","closed_at":"2026-02-06T18:31:11.872588427-05:00","close_reason":"Created nginx-ingress, envoy-gateway component CRs and experiment.yaml deploying gateway tutorial to GKE"}
{"id":"illm-k8s-ai-lab-7p5","title":"Tier 3: Prune dead code, update docs, align roadmap with reality","description":"Epic: Remove unused code and bring documentation in line with actual state.\n\nSubtasks:\n1. Remove or archive disabled Istio manifests (platform/hub/apps/disabled/) — decision made in ADR-014 to defer to Phase 7.\n2. Remove Vault component if replaced by OpenBao.\n3. Clean up Kind-specific hub config if only Talos is used (platform/hub/ vs platform/hub-talos/).\n4. Assess GitLab CI — is it active or dead? Remove if not used.\n5. Update ADR-013 status from \"Proposed\" to \"Accepted\" (Crossplane v2 is running).\n6. Update roadmap.md to reflect actual Phase 3 completion state.\n7. Audit experiment scenarios — which reference components that are not deployed?\n8. Consolidate scripts/ into Taskfile if appropriate.\n\nRelated existing beads:\n- aql (P2): kube-vip not respecting loadBalancerIP — config cleanup candidate\n- r8c (P1): Control-plane taint returns after reboot — config cleanup candidate\n\nNo blockers — this is independent cleanup work.\n\nSuccess criteria: No dead/unreachable code in tree. Docs accurately describe what exists. ADR statuses current.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-02T17:31:17.796774899-05:00","created_by":"illm","updated_at":"2026-02-02T18:24:33.799620678-05:00","closed_at":"2026-02-02T18:24:33.799620678-05:00","close_reason":"Completed: ADR-013 status updated, roadmap corrected (Istio deferred, Crossplane v2 added), Vault deprecated, dead script removed. Remaining items (GitLab CI, Kind vs Talos docs) are low-priority and not causing issues.","labels":["cleanup docs"]}
{"id":"illm-k8s-ai-lab-85t","title":"Crossplane orphan cleanup: 3 GCP networks leaked from gw-comp-6mcfj and gw-comp-8kb6g","description":"Post-mortem: experiments gw-comp-6mcfj and gw-comp-8kb6g were deleted but their GCP VPC networks + subnets were not cleaned up by Crossplane. This caused NETWORKS quota (5) exhaustion blocking gw-comp-b5jbs loadgen provisioning for 50+ min. Root cause likely: Crossplane managed resources were deleted (via experiment cleanup) before GCP finished deleting the actual resources, or the deletion failed silently. Consider: (1) adding a GCP orphan sweep CronJob, (2) requesting higher NETWORKS quota, (3) improving operator cleanup to verify GCP deletion.","status":"open","priority":2,"issue_type":"bug","created_at":"2026-02-13T17:45:35.683165315-05:00","created_by":"illm","updated_at":"2026-02-13T17:45:35.683165315-05:00","labels":["resources","toil"]}
{"id":"illm-k8s-ai-lab-86u","title":"cloud-provider-kind needs restart after new cluster creation","description":"When a new Kind cluster is created, cloud-provider-kind container cannot connect to it due to TLS certificate issues. LoadBalancer services remain in 'pending' state.\n\nError: 'tls: failed to verify certificate: x509: certificate signed by unknown authority'\n\nWorkaround: Restart cloud-provider-kind container after cluster creation.\n\nRoot cause: cloud-provider-kind doesn't automatically pick up credentials for new clusters.\n\nPossible fixes:\n1. Restart cloud-provider-kind after cluster creation in kind:up task\n2. Configure cloud-provider-kind to auto-reload when new clusters appear","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T14:04:56.331007316-05:00","created_by":"illm","updated_at":"2026-01-10T14:05:33.809441658-05:00","closed_at":"2026-01-10T14:05:33.809441658-05:00","close_reason":"Closed","labels":["networking","timing","toil"]}
{"id":"illm-k8s-ai-lab-87v","title":"GKE nodepool creation races with cluster creation","description":"Crossplane nodepool AsyncCreateFailure: 'Cluster is currently being created, deleted, updated or repaired and cannot be updated.' Seen during gateway-comparison-zzdxl provisioning. Nodepool created ~47min after cluster claim but GKE cluster wasn't ready. Crossplane retries, so it self-heals, but adds ~10-15min to provisioning time. Consider adding a dependency/ordering between cluster and nodepool in the Crossplane composition.","status":"open","priority":2,"issue_type":"bug","created_at":"2026-02-13T08:57:06.334553635-05:00","created_by":"illm","updated_at":"2026-02-13T08:57:06.334553635-05:00","labels":["gateway-comparison","resources","toil"]}
{"id":"illm-k8s-ai-lab-89a","title":"OpenBao secrets lost after reinit - ESO syncs fail","description":"After OpenBao is reinitialized during cluster reset, all stored secrets are lost. ExternalSecrets fail with SecretSyncedError. Affected: argocd-server-tls, argocd-webhook-secret, cluster-talos-target, cloudflare-api-token, crossplane credentials, tailscale-operator-oauth. Need to either: 1) Backup OpenBao before reset, 2) Recreate secrets after reset, 3) Use persistent storage that survives reset","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-26T08:31:33.76776628-05:00","created_by":"illm","updated_at":"2026-01-26T08:41:13.104480244-05:00","closed_at":"2026-01-26T08:41:13.104480244-05:00","close_reason":"Fixed by using local-path-retain StorageClass with Retain reclaim policy. PV survives PVC deletion so OpenBao secrets persist across cluster resets.","labels":["eso","openbao","toil"]}
{"id":"illm-k8s-ai-lab-8ax","title":"talos:up PSS labels applied after DaemonSet pods rejected","description":"Step 4 labels namespaces AFTER Step 3 waits for sync, but DaemonSet pods are created during sync. Need to restart DaemonSets after labeling or label namespaces earlier.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-12T21:12:21.093166496-05:00","created_by":"illm","updated_at":"2026-01-12T21:12:52.438533524-05:00","closed_at":"2026-01-12T21:12:52.438533524-05:00","close_reason":"Closed","labels":["toil"]}
{"id":"illm-k8s-ai-lab-8ms","title":"Test prometheus-tutorial experiment and tutorial","description":"Deploy prometheus-tutorial experiment CR, verify vcluster + components (kube-prometheus-stack, metrics-app, station-monitor), run through tutorial.yaml content, validate checkpoints.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-06T13:28:29.898093099-05:00","created_by":"illm","updated_at":"2026-02-06T14:09:31.454755577-05:00","closed_at":"2026-02-06T14:09:31.454755577-05:00","close_reason":"Tested: prometheus-tutorial experiment deploys successfully. Grafana, kube-state-metrics running. metrics-app and station-monitor deploy to hardcoded namespaces (see illm-k8s-ai-lab-jto). Missing Prometheus CRD prevents Prometheus server creation. Tutorial content is excellent.","labels":["testing"]}
{"id":"illm-k8s-ai-lab-8o9","title":"Hub cluster secret uses invalid config '# In-cluster' instead of JSON","description":"experiment_controller.go:335 sets kubeconfig=[]byte('# In-cluster') for hub clusters. This gets stored as ArgoCD cluster secret config, but ArgoCD expects valid JSON. Causes 'failed to unmarshal cluster config: invalid character # looking for beginning of value'. For hub clusters, operator should either skip cluster registration entirely (ArgoCD already has in-cluster access) or use valid JSON config like '{\"tlsClientConfig\":{\"insecure\":false}}'.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-02-06T13:32:39.786252795-05:00","created_by":"illm","updated_at":"2026-02-06T14:09:20.19560693-05:00","closed_at":"2026-02-06T14:09:20.19560693-05:00","close_reason":"Fixed: hub targets now skip cluster registration and create ArgoCD Application directly","labels":["config"]}
{"id":"illm-k8s-ai-lab-8xj","title":"Inline workflows missing serviceAccountName, use default SA with no RBAC","description":"When WorkflowTemplate not found, operator creates inline workflow without serviceAccountName. Uses 'default' SA which lacks 'workflowtaskresults' RBAC. Should set serviceAccountName: 'argo-workflow' (the SA used by hand-written workflows). Broader issue: operator doesn't use the Argo Workflows defined in experiments/*/workflow/experiment.yaml.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-02-06T13:41:05.381761921-05:00","created_by":"illm","updated_at":"2026-02-06T14:09:20.38581831-05:00","closed_at":"2026-02-06T14:09:20.38581831-05:00","close_reason":"Fixed: added serviceAccountName: argo-workflow to both inline and template workflows","labels":["config"]}
{"id":"illm-k8s-ai-lab-8xl","title":"cloud-gateway-comparison: AWS ALB IC vs Azure AGIC vs in-cluster","description":"Compare cloud-native gateways (AWS ALB IC, Azure AGIC) vs in-cluster (Envoy Gateway). Metrics: latency, throughput, cost, propagation time, features, observability, blast radius. Deploys to AWS/Azure via Crossplane + Talos baseline.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-15T09:20:19.498112321-05:00","created_by":"illm","updated_at":"2026-01-15T09:24:07.789812155-05:00","closed_at":"2026-01-15T09:24:07.789812155-05:00","close_reason":"Wrong tracking mechanism - these are roadmap items, not toil","dependencies":[{"issue_id":"illm-k8s-ai-lab-8xl","depends_on_id":"illm-k8s-ai-lab-age","type":"blocks","created_at":"2026-01-15T09:20:24.705355839-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-98l","title":"Implement budget tracking","description":"Create experiment-budget-limits ConfigMap and composition function for per-XRD + global budget enforcement.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T12:46:29.614265972-05:00","created_by":"illm","updated_at":"2026-02-02T18:30:16.311727569-05:00","closed_at":"2026-02-02T18:30:16.311727569-05:00","close_reason":"Removed per user request","dependencies":[{"issue_id":"illm-k8s-ai-lab-98l","depends_on_id":"illm-k8s-ai-lab-t4h","type":"blocks","created_at":"2026-01-17T12:46:41.398942013-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-999","title":"otel-demo images expire after 2h (ttl.sh)","description":"The otel-demo services use ttl.sh ephemeral images with 2h TTL. Images expire, causing ImagePullBackOff. Need to either: (1) use persistent registry like ghcr.io, or (2) build images as part of 'up' command.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-12T20:02:19.689543454-05:00","created_by":"illm","updated_at":"2026-01-12T20:26:35.171492802-05:00","closed_at":"2026-01-12T20:26:35.171492802-05:00","close_reason":"Fixed with 24h TTL and imagePullPolicy: Always","labels":["otel-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-9n0","title":"Kyverno webhooks block namespace deletion during reset","description":"During talos-hub:reset, Kyverno webhooks (validate.kyverno.svc-fail) remain after Kyverno is deleted, blocking resource deletion. Need to delete ALL Kyverno webhooks in Step 0, not just cert-manager and kyverno ones.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-25T20:38:48.358055295-05:00","created_by":"illm","updated_at":"2026-01-26T08:32:15.991137006-05:00","closed_at":"2026-01-26T08:32:15.991137006-05:00","close_reason":"Fixed by adding kyverno to webhook cleanup in Taskfile Step 0","labels":["kyverno","networking","toil"]}
{"id":"illm-k8s-ai-lab-9p4","title":"Crossplane v2 Upgrade","description":"Upgrade from Crossplane v1.18.1 to v2.x to enable namespace-scoped XRs, direct K8s resource composition, and MRDs. See ADR-013.","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-17T12:46:06.891449705-05:00","created_by":"illm","updated_at":"2026-01-17T14:31:35.908843763-05:00","closed_at":"2026-01-17T14:31:35.908843763-05:00","close_reason":"Implemented via phased ArgoCD apps (6 waves: core, base, azure, aws, gcp, configs)"}
{"id":"illm-k8s-ai-lab-9qw","title":"vcluster provisioning uses non-existent Cluster API CRD","description":"internal/crossplane/vcluster.go creates VCluster resources from infrastructure.cluster.x-k8s.io/v1alpha1, but no Cluster API or vcluster provider is installed. ADR-012 planned Crossplane compositions with provider-helm, but those were removed in v2 upgrade (ADR-013). ALL experiments with type:vcluster fail immediately. GKE path also uses wrong GVK (container.gcp.upbound.io vs gkeclusters.illm.io XRD). Only type:hub works.","status":"closed","priority":0,"issue_type":"bug","created_at":"2026-02-06T13:31:11.89997691-05:00","created_by":"illm","updated_at":"2026-02-08T18:00:21.002481005-05:00","closed_at":"2026-02-08T18:00:21.002481005-05:00","close_reason":"Cleaned dead vcluster refs from samples. All experiments use gke/hub.","labels":["config"]}
{"id":"illm-k8s-ai-lab-9s1","title":"loki-tutorial promtail blocked by PodSecurity baseline","description":"Two issues: (1) PodSecurity baseline blocks hostPath - needs namespace label, (2) Promtail can't find logs - Talos uses different log paths than standard Linux. Core loki infra works, promtail needs Talos-specific config.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-12T20:29:42.928600681-05:00","created_by":"illm","updated_at":"2026-01-12T20:36:20.525164441-05:00","closed_at":"2026-01-12T20:36:20.525164441-05:00","close_reason":"Superseded by general Talos PSS issue","labels":["loki-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-a63","title":"GCP NETWORKS quota (5) exceeded blocking loadgen provisioning","description":"gw-comp-b5jbs loadgen target stuck in Provisioning for 50+ min. Root cause: GCP quota NETWORKS (limit 5 per project) exhausted. Only 2 Crossplane-managed networks visible — likely 3 orphaned networks from previous experiments not cleaned up. Cascading failure: network→subnet→cluster→nodepool all Synced=False. Fix: delete orphaned GCP networks or request quota increase. Workaround: clean up completed experiment Crossplane resources faster.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-02-13T17:41:57.449652289-05:00","created_by":"illm","updated_at":"2026-02-13T17:45:29.675176475-05:00","closed_at":"2026-02-13T17:45:29.675176475-05:00","close_reason":"Deleted 3 orphaned GCP VPC networks (gw-comp-6mcfj-loadgen, gw-comp-8kb6g-app, gw-comp-8kb6g-loadgen) and their subnets. Freed quota from 5/5 to 2/5. Crossplane auto-retried and created loadgen network.","labels":["networking","resources","toil"]}
{"id":"illm-k8s-ai-lab-a7k","title":"Envoy Gateway v1.2.4 crashes on GRPCRoute RequestMirror","description":"Envoy Gateway v1.2.4 panics when processing GRPCRoute with RequestMirror filter. Error: interface conversion: interface {} is []v1.HTTPRouteFilter, not []v1.GRPCRouteFilter. Workaround: remove RequestMirror from GRPCRoutes.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-15T13:41:00.941997688-05:00","created_by":"illm","updated_at":"2026-01-15T17:48:26.400396086-05:00","closed_at":"2026-01-15T17:48:26.400396086-05:00","close_reason":"Workaround applied: GRPCRoute RequestMirror commented out in part5-grpc-gateway-api.yaml with reference to upstream bug","labels":["gateway-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-a7q","title":"Tailscale operator leaves stale StatefulSets after cluster reset","description":"After cluster reset/redeploy, Tailscale operator creates StatefulSets but the corresponding secrets don't exist. Operator logs: 'Tailscale proxy secret doesn't exist, but the corresponding StatefulSet already does. Something is wrong, please delete the StatefulSet.' Requires manual cleanup of all ts-* StatefulSets and secrets in tailscale namespace before operator can reconcile properly.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-26T09:57:42.492771261-05:00","created_by":"illm","updated_at":"2026-01-26T11:58:13.337439852-05:00","closed_at":"2026-01-26T11:58:13.337439852-05:00","close_reason":"Fixed in commit 847bfc6","labels":["tailscale","timing","toil"]}
{"id":"illm-k8s-ai-lab-age","title":"gateway-comparison: nginx vs Traefik vs Envoy Gateway","description":"Side-by-side comparison of gateway implementations with same demo app and routes. Compare config complexity, resource usage, features. Deploys to Talos.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-15T09:20:19.082344247-05:00","created_by":"illm","updated_at":"2026-01-15T09:24:07.785749757-05:00","closed_at":"2026-01-15T09:24:07.785749757-05:00","close_reason":"Wrong tracking mechanism - these are roadmap items, not toil","dependencies":[{"issue_id":"illm-k8s-ai-lab-age","depends_on_id":"illm-k8s-ai-lab-iku","type":"blocks","created_at":"2026-01-15T09:20:24.574174163-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-ahu","title":"hello-app app.yaml uses direct server URL instead of name: target","description":"hello-app's app.yaml uses server: https://kubernetes.default.svc instead of name: target. The talos:up task expects name: target to substitute.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-13T08:09:11.881063331-05:00","created_by":"illm","updated_at":"2026-01-13T08:12:57.836698276-05:00","closed_at":"2026-01-13T08:12:57.836698276-05:00","close_reason":"Fixed app.yaml to use name: target instead of server URL","labels":["hello-app","toil"]}
{"id":"illm-k8s-ai-lab-ajb","title":"ArgoCD CreateNamespace fails after namespace deletion","description":"After deleting a namespace and resetting, ArgoCD's CreateNamespace=true sync option sometimes fails with 'namespace not found' error even though the option should create the namespace. Manual namespace creation required as workaround.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-24T16:20:29.006758493-05:00","created_by":"illm","updated_at":"2026-02-02T18:38:29.349885603-05:00","closed_at":"2026-02-02T18:38:29.349885603-05:00","close_reason":"Known ArgoCD behavior: CreateNamespace fails when namespace is in Terminating state. Workaround: wait for namespace termination to complete or manually create namespace. Rare edge case during teardown/rebuild cycles.","labels":["argocd","toil"]}
{"id":"illm-k8s-ai-lab-akm","title":"Operator reconciles provisioning clusters every 10s (noisy)","description":"During gateway-comparison-zzdxl provisioning, the operator logs 'Cluster not ready yet' 6 times per minute (~every 10s). GKE provisioning takes 10-30min. Consider backing off to 30-60s requeue during Provisioning phase to reduce log noise and API load.","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-13T08:59:46.887004625-05:00","created_by":"illm","updated_at":"2026-02-13T08:59:46.887004625-05:00","labels":["resources","toil"]}
{"id":"illm-k8s-ai-lab-aql","title":"kube-vip not respecting loadBalancerIP field","description":"During Talos hub migration, discovered that kube-vip-cloud-provider may not be respecting the loadBalancerIP spec field. OpenBao requested 192.168.1.242 but got 192.168.1.240 (same as DNS). Currently works because different ports (DNS:53, OpenBao:8200), but should investigate proper IP allocation.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-17T16:24:11.384627435-05:00","created_by":"illm","updated_at":"2026-02-02T18:18:23.157574707-05:00","closed_at":"2026-02-02T18:18:23.157574707-05:00","close_reason":"No LoadBalancer services exist on the cluster; all services are ClusterIP or NodePort. Tailscale handles ingress instead of kube-vip LoadBalancers.","labels":["config","networking","talos"]}
{"id":"illm-k8s-ai-lab-aua","title":"Deploy SeaweedFS on hub cluster","description":"Deploy SeaweedFS S3-compatible storage for experiment artifacts (k6 results, Grafana snapshots, logs, heap dumps).","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T12:46:14.384661933-05:00","created_by":"illm","updated_at":"2026-01-24T15:28:35.49619668-05:00","closed_at":"2026-01-24T15:28:35.49619668-05:00","close_reason":"SeaweedFS deployed and working with bucket creation automation","dependencies":[{"issue_id":"illm-k8s-ai-lab-aua","depends_on_id":"illm-k8s-ai-lab-9p4","type":"blocks","created_at":"2026-01-17T12:46:40.959386413-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-av5","title":"Create generic workflow templates","description":"Create Argo Workflow templates: tutorial-interactive, benchmark-k6, benchmark-comparison, soak-test.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T12:46:26.105493757-05:00","created_by":"illm","updated_at":"2026-02-02T18:41:39.951082855-05:00","closed_at":"2026-02-02T18:41:39.951082855-05:00","close_reason":"Closed: PostgreSQL dependency (t4h) removed per user request. Workflow templates can be recreated when experiment result storage approach is decided.","dependencies":[{"issue_id":"illm-k8s-ai-lab-av5","depends_on_id":"illm-k8s-ai-lab-t4h","type":"blocks","created_at":"2026-01-17T12:46:41.175653645-05:00","created_by":"illm"},{"issue_id":"illm-k8s-ai-lab-av5","depends_on_id":"illm-k8s-ai-lab-aua","type":"blocks","created_at":"2026-01-17T12:46:41.286803027-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-b4d","title":"Test hello-app on Talos","description":"Test hello-app experiment on Talos platform. Goal: 'task talos:up -- hello-app' works with zero manual intervention. Cycle the experiment, identify and fix any toil. Create child beads for each issue discovered during testing.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T08:02:56.628679475-05:00","created_by":"illm","updated_at":"2026-01-13T08:12:58.140405786-05:00","closed_at":"2026-01-13T08:12:58.140405786-05:00","close_reason":"hello-app deploys successfully on Talos with task talos:up -- hello-app","labels":["talos-testing"]}
{"id":"illm-k8s-ai-lab-b7z","title":"Create experiment scaffolding skill","description":"Create a Claude Code skill (/create-experiment) that scaffolds a new experiment directory from the _template. Should prompt for experiment name, target count/types (hub vs GKE), components to include, metrics, workflow template, and whether to publish. Generates experiment.yaml with proper generateName, namespace, component refs, and cluster specs. Validates GKE name lengths before writing. Could also generate a matching component.yaml if custom components are needed.","status":"open","priority":4,"issue_type":"feature","created_at":"2026-02-13T17:19:38.400618846-05:00","created_by":"illm","updated_at":"2026-02-13T17:19:44.591572728-05:00"}
{"id":"illm-k8s-ai-lab-bji","title":"Test multi-cloud-demo on Talos","description":"Test multi-cloud-demo experiment on Talos platform. Goal: 'task talos:up -- multi-cloud-demo' works with zero manual intervention. Cycle the experiment, identify and fix any toil. Create child beads for each issue discovered during testing.","notes":"Blocked by illm-k8s-ai-lab-bji.1 - needs structural redesign","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T08:02:59.196152983-05:00","created_by":"illm","updated_at":"2026-01-13T20:44:52.468141217-05:00","closed_at":"2026-01-13T20:44:52.468141217-05:00","close_reason":"Deprioritized - multi-cloud-demo not on current roadmap.","labels":["talos-testing"]}
{"id":"illm-k8s-ai-lab-bji.1","title":"multi-cloud-demo needs structural redesign for Talos","description":"Experiment has: 1) server URL instead of name: target, 2) wrong paths (crossplane, k6 don't exist), 3) depends on Crossplane on hub cluster. May need separate target/loadgen apps like other experiments.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T08:22:39.345450998-05:00","created_by":"illm","updated_at":"2026-01-13T20:44:48.160479121-05:00","closed_at":"2026-01-13T20:44:48.160479121-05:00","close_reason":"Deprioritized - not on current roadmap. Experiment needs redesign but not urgent.","labels":["talos-testing"],"dependencies":[{"issue_id":"illm-k8s-ai-lab-bji.1","depends_on_id":"illm-k8s-ai-lab-bji","type":"parent-child","created_at":"2026-01-13T08:22:39.347823399-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-bus","title":"Operator deploys ArgoCD apps to 'default' namespace instead of experiment-scoped namespace","description":"The ApplicationManager.CreateApplication always sets destination.namespace to 'default'. This causes: (1) cluster-scoped resource conflicts with existing ArgoCD apps (SharedResourceWarning), (2) tutorial workloads mixed with other workloads in default namespace, (3) no isolation between experiments. Should deploy to experiment-specific namespace like '{experimentName}' or '{experimentName}-{targetName}'. The experiment.yaml tutorial files reference namespaces like 'loki-tutorial' and 'prometheus-tutorial' for their services.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-02-06T13:54:44.340977398-05:00","created_by":"illm","updated_at":"2026-02-06T14:09:20.825225782-05:00","closed_at":"2026-02-06T14:09:20.825225782-05:00","close_reason":"Fixed: ArgoCD app destination namespace set to experimentName instead of default","labels":["toil"]}
{"id":"illm-k8s-ai-lab-bvh","title":"OpenBao PVC stuck terminating blocks ESO sync","description":"During cluster reset, OpenBao PVC gets stuck in Terminating state. OpenBao regenerates with new root token but ESO still has old token, causing all ExternalSecrets to fail with 403. Need to: 1) Force delete stuck PVC 2) Reinitialize OpenBao 3) Update ESO token","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-26T07:42:48.885097197-05:00","created_by":"illm","updated_at":"2026-01-26T08:32:15.778278372-05:00","closed_at":"2026-01-26T08:32:15.778278372-05:00","close_reason":"Fixed by patching PVC finalizers during cleanup and adding PVC cleanup to Taskfile Step 5","labels":["eso","openbao","toil"]}
{"id":"illm-k8s-ai-lab-by6","title":"Envoy Gateway helm chart repo 404 error","description":"The https://gateway.envoyproxy.io/charts URL returns 404. Need to find correct helm repo URL or use OCI format properly.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-15T13:23:05.8751578-05:00","created_by":"illm","updated_at":"2026-01-15T13:24:09.672280533-05:00","closed_at":"2026-01-15T13:24:09.672280533-05:00","close_reason":"Split Envoy Gateway into separate single-source ArgoCD Application"}
{"id":"illm-k8s-ai-lab-chc","title":"STATE wipe puts Talos into maintenance mode, needs apply-config before bootstrap","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-28T14:28:23.613360442-05:00","created_by":"illm","updated_at":"2026-01-28T15:01:06.573748045-05:00","closed_at":"2026-01-28T15:01:06.573748045-05:00","close_reason":"Fixed in deploy task: Step 1 applies machine config if node is in maintenance mode; Helm guard clears stale pending-install releases before upgrade","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-cl1","title":"SeaweedFS S3 endpoint stuck in notReadyAddresses after reboot","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-26T20:50:19.185044466-05:00","created_by":"illm","updated_at":"2026-02-02T18:38:24.674947916-05:00","closed_at":"2026-02-02T18:38:24.674947916-05:00","close_reason":"Fixed: Root cause was stale PVCs with node affinity for old node talos-jid-knd. Deleted stale PVCs, new PVCs created on current node. This is a one-time node migration issue, not a recurring reboot problem.","labels":["networking","timing","toil"]}
{"id":"illm-k8s-ai-lab-cqc","title":"Namespace termination blocked by pods with finalizers","description":"During talos-hub:reset, namespaces get stuck in Terminating state because pods still exist. The script removes resource finalizers but doesn't force delete pods. Need to add force pod deletion or wait longer for pods to terminate gracefully.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-24T16:12:43.838641883-05:00","created_by":"illm","updated_at":"2026-01-24T17:28:34.131248005-05:00","closed_at":"2026-01-24T17:28:34.131248005-05:00","close_reason":"Addressed by talos-hub Taskfile automation (task talos-hub:reset handles webhook cleanup, finalizer removal, and OpenBao re-init)","labels":["k8s","toil"]}
{"id":"illm-k8s-ai-lab-d0y","title":"Crossplane nodepool stuck in async create after failure","description":"Nodepool gateway-comparison-zzdxl-app-95n6s-nodes: AsyncCreateFailure at 13:39 left Synced:True but Ready:False. atProvider empty (never created on GCP). Provider in-memory async tracker thinks create is still running, blocking both retry and delete. Required provider-gcp-container pod restart to clear. Root cause: Crossplane async operation tracking doesn't handle GKE 400 failedPrecondition properly — leaves stale 'operation running' state in memory. This is the second issue in the provisioning chain: first the nodepool raced with cluster creation, then the failed async create got stuck.","status":"open","priority":1,"issue_type":"bug","created_at":"2026-02-13T09:04:52.410282314-05:00","created_by":"illm","updated_at":"2026-02-13T09:04:52.410282314-05:00","labels":["gateway-comparison","resources","toil"]}
{"id":"illm-k8s-ai-lab-d8f","title":"kong/httpbin image cannot run as non-root on port 80","description":"kong/httpbin:0.1.0 binds to port 80 which requires root. With runAsNonRoot: true, the container fails to start. Fixed by switching to mccutchen/go-httpbin which supports PORT env var.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-15T13:44:49.887606948-05:00","created_by":"illm","updated_at":"2026-01-15T13:45:03.22224403-05:00","closed_at":"2026-01-15T13:45:03.22224403-05:00","close_reason":"Fixed by switching to mccutchen/go-httpbin","labels":["gateway-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-d8v","title":"Kibana OOMKilled - needs more memory","description":"Kibana container is OOMKilled with 512Mi limit. Needs 1Gi for stable operation.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-12T21:04:53.501749078-05:00","created_by":"illm","updated_at":"2026-01-12T21:05:09.270943794-05:00","closed_at":"2026-01-12T21:05:09.270943794-05:00","close_reason":"Closed","labels":["logging-comparison","toil"]}
{"id":"illm-k8s-ai-lab-dcm","title":"SeaweedFS PostSync hook doesn't trigger on redeploy","description":"The PostSync hook for bucket creation didn't trigger during the redeploy cycle. The job had to be manually applied. The hook worked on the first deploy but not on subsequent syncs. May need to investigate ArgoCD hook behavior or change to a different approach (initContainer, CronJob).","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-24T16:39:21.228125268-05:00","created_by":"illm","updated_at":"2026-02-02T18:38:26.974467501-05:00","closed_at":"2026-02-02T18:38:26.974467501-05:00","close_reason":"Worked around: seaweedfs-config uses sync-wave Jobs with retry logic and a placeholder ConfigMap instead of PostSync hooks. seaweedfs-config is Synced+Healthy.","labels":["argocd","seaweedfs","toil"]}
{"id":"illm-k8s-ai-lab-dgz","title":"otel-demo pods missing securityContext for restricted PSS","description":"Pods warn about PodSecurity restricted mode: need securityContext.allowPrivilegeEscalation=false, capabilities.drop=[ALL], runAsNonRoot=true, seccompProfile","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-12T20:26:42.277331295-05:00","created_by":"illm","updated_at":"2026-01-13T16:34:49.844647847-05:00","closed_at":"2026-01-13T16:34:49.844647847-05:00","close_reason":"Added securityContext to all otel-demo pods for restricted PSS compliance","labels":["config","otel-tutorial"]}
{"id":"illm-k8s-ai-lab-dj1","title":"Envoy Gateway OCI chart URL incorrect in ArgoCD app","description":"ArgoCD app.yaml uses wrong OCI format: oci://docker.io/envoyproxy gateway-helm. Should be oci://docker.io/envoyproxy/gateway-helm or use the standard helm repo https://gateway.envoyproxy.io/charts","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-15T13:20:05.182003208-05:00","created_by":"illm","updated_at":"2026-01-15T13:20:30.566665018-05:00","closed_at":"2026-01-15T13:20:30.566665018-05:00","close_reason":"Fixed by using HTTPS helm repo URL instead of OCI format"}
{"id":"illm-k8s-ai-lab-dq9","title":"Control plane crashes during Crossplane CRD installation","description":"During provider installation, the ~800 CRDs overwhelm the API server, causing kube-scheduler and kube-controller-manager to lose their leader leases. Control plane crashes in CrashLoopBackOff while CRDs are being registered. Related to ewz but distinct - this is CRD installation load, not post-reboot.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-27T06:38:50.705022539-05:00","created_by":"illm","updated_at":"2026-02-02T18:39:00.359772303-05:00","closed_at":"2026-02-02T18:39:00.359772303-05:00","close_reason":"Mitigated: Longer leader-elect lease durations prevent scheduler/controller-manager crash during CRD blast. Full fix is Crossplane v2 MRDs (selective CRD installation, ~800 → ~20 CRDs) documented in ADR-013. Runtime configs already limit provider memory to 512Mi.","labels":["crossplane","timing","toil"]}
{"id":"illm-k8s-ai-lab-dwk","title":"ArgoCD Application builder uses 'path' instead of 'chart' for Helm repos","description":"When Component CRs define Helm chart sources (e.g., repoURL: https://grafana.github.io/helm-charts with path: loki), the operator's ArgoCD Application builder sets 'path' in the ArgoCD source spec. For Helm repositories, ArgoCD requires 'chart' field instead of 'path'. The 'path' field is only for git repos. This causes 'repository not found' errors for all Helm-based components (loki, promtail, kube-prometheus-stack). Fix: in application.go CreateApplication, detect when source.Helm is set and use 'chart' instead of 'path'. Also need to update Component CRD types to support both chart and path fields, or auto-detect based on repoURL.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-02-06T13:46:05.648613582-05:00","created_by":"illm","updated_at":"2026-02-06T14:09:20.560216655-05:00","closed_at":"2026-02-06T14:09:20.560216655-05:00","close_reason":"Fixed: added chart field to ComponentSource, updated application.go to use chart for Helm repos, updated 3 component.yaml files","labels":["toil"]}
{"id":"illm-k8s-ai-lab-ei7","title":"Talos API server becomes unresponsive during hub migration","description":"During hub migration to Talos N100, the Kubernetes API server became unresponsive. Node is pingable (192.168.1.178) but kubectl and talosctl cannot connect to port 6443. Possible causes: resource exhaustion from deploying many apps simultaneously, kube-vip conflict, or N100 resource limits.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-17T16:07:51.568853175-05:00","created_by":"illm","updated_at":"2026-01-17T16:24:18.412741687-05:00","closed_at":"2026-01-17T16:24:18.412741687-05:00","close_reason":"API server recovered after ~3 minutes. Possible temporary overload during heavy app deployment.","labels":["resources","talos","timing"]}
{"id":"illm-k8s-ai-lab-ewz","title":"Scheduler stuck after reboot - not acquiring lease","status":"closed","priority":0,"issue_type":"bug","created_at":"2026-01-26T21:12:06.49079538-05:00","created_by":"illm","updated_at":"2026-02-02T18:38:57.945160355-05:00","closed_at":"2026-02-02T18:38:57.945160355-05:00","close_reason":"Fixed: Added longer leader-elect lease durations (30s lease, 20s renew, 5s retry) to scheduler and controller-manager in Talos config template. Default 15s/10s/2s was too aggressive for single-node cluster where etcd restarts alongside the control plane. Apply with talosctl on next reboot.","labels":["timing","toil"]}
{"id":"illm-k8s-ai-lab-fek","title":"Cycle wait: talosctl version --insecure exits 1 in maintenance mode","description":"talosctl version --insecure returns exit code 1 in maintenance mode (API is not implemented in maintenance mode) even though the node IS reachable. The cycle wait loop uses exit code to detect availability, so it times out after 10 minutes. Fix: check for output content (e.g. grep for Tag:) instead of exit code.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-28T17:57:28.530974146-05:00","created_by":"illm","updated_at":"2026-01-28T18:00:27.105564596-05:00","closed_at":"2026-01-28T18:00:27.105564596-05:00","close_reason":"Fixed: check for Tag: in output instead of exit code","labels":["toil"]}
{"id":"illm-k8s-ai-lab-fh1","title":"Test cicd-fundamentals on Talos","description":"Test cicd-fundamentals experiment on Talos platform. Goal: 'task talos:up -- cicd-fundamentals' works with zero manual intervention. Cycle the experiment, identify and fix any toil. Create child beads for each issue discovered during testing.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T08:03:01.728938433-05:00","created_by":"illm","updated_at":"2026-01-13T08:26:37.234423197-05:00","closed_at":"2026-01-13T08:26:37.234423197-05:00","close_reason":"cicd-fundamentals is documentation-only, no deployable app for Talos","labels":["talos-testing"]}
{"id":"illm-k8s-ai-lab-fh1.1","title":"cicd-fundamentals is documentation-only, no deployable app","description":"This experiment describes a CI/CD pipeline concept using GitHub Actions and ArgoCD Image Updater. There's no target/argocd/app.yaml to deploy. Either convert to deployable experiment or mark as N/A for Talos.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-13T08:26:36.912659227-05:00","created_by":"illm","updated_at":"2026-01-13T20:39:07.790244819-05:00","closed_at":"2026-01-13T20:39:07.790244819-05:00","close_reason":"Documentation-only experiment describing CI/CD architecture. No deployable workload - references hello-app which is a separate experiment. N/A for Talos testing.","labels":["talos-testing"],"dependencies":[{"issue_id":"illm-k8s-ai-lab-fh1.1","depends_on_id":"illm-k8s-ai-lab-fh1","type":"parent-child","created_at":"2026-01-13T08:26:36.913930107-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-g0i","title":"Deploy Experiment Operator to hub cluster","description":"CRDs exist in operators/experiment-operator/config/crd/bases/ but are not applied to talos-hub. Operator pod is not running. This blocks ALL experiment testing.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-06T13:28:16.505101779-05:00","created_by":"illm","updated_at":"2026-02-06T20:45:08.989439264-05:00","closed_at":"2026-02-06T20:45:08.989439264-05:00","close_reason":"Operator deployed via ArgoCD, hello-app experiment completed full lifecycle (Pending→Provisioning→Ready→Running→Complete)","labels":["config","timing"]}
{"id":"illm-k8s-ai-lab-gak","title":"Envoy Gateway Helm chart doesn't create GatewayClass","description":"The Envoy Gateway Helm chart (v1.2.4 via GitHub source) doesn't create the 'eg' GatewayClass. This prevents Gateway resources from being programmed. Had to create it manually. Should add GatewayClass to manifests or values.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-15T17:33:57.761702306-05:00","created_by":"illm","updated_at":"2026-01-15T17:49:50.764853001-05:00","closed_at":"2026-01-15T17:49:50.764853001-05:00","close_reason":"Fixed: Added GatewayClass to manifests, added grpcurl test pod, documented OCI workaround","labels":["config","gateway-tutorial"]}
{"id":"illm-k8s-ai-lab-h2o","title":"Tier 1: Stop the bleeding — ArgoCD green, remove broken compositions","description":"Epic: Get ArgoCD to a clean state and triage existing beads.\n\nSubtasks:\n1. Fix crossplane-xrds OutOfSync — remove or convert non-functional legacy compositions (caches, databases, experiment-clusters, queues) that fail Pipeline mode validation. Keep XRD definitions.\n2. Investigate and fix other OutOfSync apps: cert-manager, hub, seaweedfs.\n3. Triage all 13 open beads against current 5-day-old cluster — close anything already resolved.\n4. Decide on AWS/Azure provider installation vs removing untestable compositions.\n\nRelated existing beads:\n- dq9 (P1): Control plane crashes during Crossplane CRD install — may be resolved or needs revalidation\n- jne (P2): StatefulSet spec changes require deletion — affects Mimir/Loki redeploy\n- dcm (P2): SeaweedFS PostSync hook doesn't trigger — affects seaweedfs OutOfSync\n- ajb (P2): ArgoCD CreateNamespace fails after deletion\n\nSuccess criteria: All ArgoCD apps either Synced+Healthy or intentionally excluded. No spurious sync errors.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-02T17:30:44.063285184-05:00","created_by":"illm","updated_at":"2026-02-02T18:18:28.377964911-05:00","closed_at":"2026-02-02T18:18:28.377964911-05:00","close_reason":"32/33 ArgoCD apps Synced+Healthy. Remaining OutOfSync is seaweedfs (StatefulSet immutable fields - tracked by jne). Removed 11 broken legacy compositions, fixed cert-manager webhooks, fixed hub ignoreDifferences, fixed stale PVCs across seaweedfs and observability namespaces.","labels":["cleanup"]}
{"id":"illm-k8s-ai-lab-hi0","title":"slo-tutorial: Create experiment.yaml and pyrra component CR","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-06T18:28:22.807027348-05:00","created_by":"illm","updated_at":"2026-02-06T18:31:08.823769755-05:00","closed_at":"2026-02-06T18:31:08.823769755-05:00","close_reason":"Created pyrra component CR and experiment.yaml deploying Pyrra+Prometheus to GKE"}
{"id":"illm-k8s-ai-lab-ibc","title":"SeaweedFS idx volume must be persistent","description":"SeaweedFS volume server crashed with 'idx file does not exist' after pod restarts. Root cause: Helm chart defaults idx volume to emptyDir, losing index files on restart while data files persist. Fixed by configuring idx as PVC in seaweedfs.yaml.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-25T20:04:40.618887707-05:00","created_by":"illm","updated_at":"2026-01-25T20:04:46.78196279-05:00","closed_at":"2026-01-25T20:04:46.78196279-05:00","close_reason":"Fixed by changing idx volume from emptyDir to PVC in platform/hub/values/seaweedfs.yaml","labels":["config","seaweedfs","toil"]}
{"id":"illm-k8s-ai-lab-ibx","title":"Namespaces need explicit PSS labels for operators requiring elevated permissions","description":"Several operators (Tailscale, potentially others) require privileged PodSecurityStandards. Without explicit namespace labels, pods fail with PodSecurity violations. Fixed for tailscale namespace in platform/hub/manifests/tailscale-config/namespace.yaml. May need to audit other namespaces: kyverno, external-secrets, cert-manager, etc.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-26T09:57:56.419250386-05:00","created_by":"illm","updated_at":"2026-01-26T12:00:03.603314045-05:00","closed_at":"2026-01-26T12:00:03.603314045-05:00","close_reason":"Audited all namespaces. Tailscale fixed in commit c734e9b. Other namespaces (cert-manager, crossplane-system, external-secrets) have no PSS violations - they work with restricted profile.","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-iku","title":"gateway-tutorial: Ingress → Gateway API evolution","description":"Deep tutorial covering Ingress basics, hitting limitations, migrating to Gateway API, and Gateway API deep dive. Deploys to Kind/Talos.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-15T09:20:18.673748521-05:00","created_by":"illm","updated_at":"2026-01-15T09:24:07.777746595-05:00","closed_at":"2026-01-15T09:24:07.777746595-05:00","close_reason":"Wrong tracking mechanism - these are roadmap items, not toil"}
{"id":"illm-k8s-ai-lab-ikx","title":"Operator deletion handler crashes on not-found ArgoCD apps","description":"handleDeletion errors on 'applications.argoproj.io hello-app-app not found' when experiment is deleted before ArgoCD apps were created. Should ignore not-found errors during cleanup. Also stuck in retry loop: Provisioning phase retries endlessly with 'Target has no cluster name' after initial creation failure.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-02-06T13:31:31.712640227-05:00","created_by":"illm","updated_at":"2026-02-06T14:09:20.479364025-05:00","closed_at":"2026-02-06T14:09:20.479364025-05:00","close_reason":"Fixed: added errors.IsNotFound checks in DeleteApplication and UnregisterCluster","labels":["config"]}
{"id":"illm-k8s-ai-lab-irz","title":"Taskfile cluster discovery broken after migration","description":"The hub Taskfile discovers clusters by searching EXP_PATH/*/cluster.yaml (target/cluster.yaml, loadgen/cluster.yaml). After migration, these files are now at experiments/{name}/legacy/target/cluster.yaml. The pattern still matches since legacy/target/ is a subdirectory, BUT the Taskfile also uses basename of the cluster_dir to build cluster names (e.g. 'target', 'loadgen') which would now be 'legacy'. Need to update glob pattern to search in legacy/ subdirs. Workaround: update Taskfile path to experiments/{name}/legacy/*/cluster.yaml or just experiments/{name}.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-02-05T22:12:51.093694469-05:00","created_by":"illm","updated_at":"2026-02-06T06:42:06.907905221-05:00","closed_at":"2026-02-06T06:42:06.907905221-05:00","close_reason":"Fixed during migration - Taskfile paths updated to use experiments/{name}/legacy/*/ pattern","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-j4c","title":"PVC finalizers block namespace deletion during reset","description":"PVCs with kubernetes.io/pvc-protection finalizer block namespace deletion. Need to patch PVC finalizers explicitly in cleanup step.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-25T20:38:52.657686193-05:00","created_by":"illm","updated_at":"2026-01-26T08:32:16.08575496-05:00","closed_at":"2026-01-26T08:32:16.08575496-05:00","close_reason":"Fixed by adding explicit PVC finalizer cleanup in Taskfile Step 5","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-jne","title":"StatefulSet spec changes require deletion before redeploy","description":"When Helm chart values change StatefulSet spec fields beyond allowed (replicas, template, etc), ArgoCD sync fails with 'spec: Forbidden: updates to statefulset spec for fields other than...' error. Workaround: delete the StatefulSet with --cascade=orphan, clear ArgoCD operation state, then sync again. Affects Mimir and Loki on redeployment.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-24T17:28:20.886348008-05:00","created_by":"illm","updated_at":"2026-02-02T18:40:12.400567656-05:00","closed_at":"2026-02-02T18:40:12.400567656-05:00","close_reason":"Fixed: Added jqPathExpressions to ignoreDifferences for StatefulSet volumeClaimTemplate fields (volumeMode, apiVersion, kind, status) that Kubernetes adds but Helm doesn't render. Added RespectIgnoreDifferences=true. seaweedfs now Synced+Healthy.","labels":["argocd","helm","toil"]}
{"id":"illm-k8s-ai-lab-jto","title":"Component manifests hardcode namespace (demo, station) breaking experiment namespace isolation","description":"metrics-app kustomization.yaml sets namespace: demo, station-monitor sets namespace: station. This overrides the ArgoCD Application destination namespace. When the operator creates an ArgoCD app targeting namespace 'prometheus-tutorial-test', the components deploy to 'demo' and 'station' instead. Options: (1) remove namespace from kustomization.yaml and rely on ArgoCD destination, (2) accept component-defined namespaces and update tutorial service refs, (3) use ArgoCD 'namespace' override per source.","status":"open","priority":2,"issue_type":"bug","created_at":"2026-02-06T14:08:04.022473477-05:00","created_by":"illm","updated_at":"2026-02-06T14:08:04.022473477-05:00","labels":["toil"]}
{"id":"illm-k8s-ai-lab-jze","title":"Deploy Mimir on hub cluster","description":"Deploy Mimir for metrics aggregation from all experiment clusters via Grafana Agent remote_write.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T12:46:17.932558007-05:00","created_by":"illm","updated_at":"2026-01-24T16:48:18.655994425-05:00","closed_at":"2026-01-24T16:48:18.655994425-05:00","close_reason":"Mimir deployed as part of LGTM observability stack with SeaweedFS backend","dependencies":[{"issue_id":"illm-k8s-ai-lab-jze","depends_on_id":"illm-k8s-ai-lab-9p4","type":"blocks","created_at":"2026-01-17T12:46:41.070334998-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-kso","title":"Jaeger image tag 1.62 doesn't exist","description":"jaegertracing/all-in-one:1.62 not found on Docker Hub. Need to update to a valid version.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-12T21:50:35.346086307-05:00","created_by":"illm","updated_at":"2026-01-12T21:50:49.937757209-05:00","closed_at":"2026-01-12T21:50:49.937757209-05:00","close_reason":"Closed","labels":["toil","tracing-comparison"]}
{"id":"illm-k8s-ai-lab-kxd","title":"otel-tutorial: Create experiment.yaml and component CRs","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-06T18:28:20.962032105-05:00","created_by":"illm","updated_at":"2026-02-06T18:31:06.968252946-05:00","closed_at":"2026-02-06T18:31:06.968252946-05:00","close_reason":"Created otel-collector, tempo, otel-demo component CRs and experiment.yaml deploying OTel stack to GKE"}
{"id":"illm-k8s-ai-lab-l9w","title":"Gateway comparison has no load generation — k6 manifests exist but aren't wired in","description":"experiments/gateway-comparison/manifests/k6-load-test.yaml contains a complete k6 Job (init container for endpoint discovery, 16-min staged load test, per-gateway latency/error/throughput metrics) but it's never deployed. The experiment has only one target (app) and the workflow phased-observation step just sleeps. All gateway-comparison runs to date have been idle-only — no load generation, no latency benchmarks. Need to decide: same-cluster k6 Job deployed as a component, or separate loadgen cluster target.","status":"open","priority":1,"issue_type":"feature","created_at":"2026-02-13T10:24:48.571031024-05:00","created_by":"illm","updated_at":"2026-02-13T10:24:48.571031024-05:00","labels":["gateway-comparison","networking"]}
{"id":"illm-k8s-ai-lab-lp8","title":"Stale Helm pending-install blocks ArgoCD upgrade on deploy","description":"After a failed or interrupted deploy, Helm can leave ArgoCD in pending-install state. helm upgrade --install then fails. Deploy task needs to detect and clear stale releases.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-28T14:30:54.913596063-05:00","created_by":"illm","updated_at":"2026-01-28T15:01:06.587256308-05:00","closed_at":"2026-01-28T15:01:06.587256308-05:00","close_reason":"Fixed in deploy task: Step 1 applies machine config if node is in maintenance mode; Helm guard clears stale pending-install releases before upgrade","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-m8g","title":"Loki component missing required Helm values for single-binary mode","description":"Loki Helm chart 6.28.0 requires explicit storage configuration. Without it, helm template fails with 'nil pointer evaluating interface {}.chunks'. The Component CR needs default Helm parameters for single-binary/filesystem mode: loki.commonConfig.replication_factor=1, loki.storage.type=filesystem, etc.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-02-06T13:50:37.427864059-05:00","created_by":"illm","updated_at":"2026-02-06T14:09:20.649265361-05:00","closed_at":"2026-02-06T14:09:20.649265361-05:00","close_reason":"Fixed: added required Helm parameters for Loki single-binary mode","labels":["loki-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-moe","title":"Kyverno webhooks block pod creation when admission controller is down","description":"When Kyverno admission controller is unhealthy (CrashLoopBackOff), its mutating/validating webhooks remain registered and block all pod creation with 'failed calling webhook: dial tcp: connection refused'. Workaround: manually delete kyverno-*-webhook-cfg resources. Consider setting failurePolicy: Ignore for non-critical environments.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-26T09:57:49.182199998-05:00","created_by":"illm","updated_at":"2026-01-26T11:58:13.326389159-05:00","closed_at":"2026-01-26T11:58:13.326389159-05:00","close_reason":"Fixed in commit 847bfc6","labels":["kyverno","networking","toil"]}
{"id":"illm-k8s-ai-lab-n0e","title":"tracing-comparison uses non-existent ghcr.io images","description":"Same as otel-tutorial - otel-demo services use ghcr.io images that don't exist.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-12T21:31:20.370836536-05:00","created_by":"illm","updated_at":"2026-01-12T21:32:06.06718126-05:00","closed_at":"2026-01-12T21:32:06.06718126-05:00","close_reason":"Closed","labels":["toil","tracing-comparison"]}
{"id":"illm-k8s-ai-lab-nd0","title":"otel-tutorial missing demo app images","description":"## Issue\notel-tutorial deployment fails due to missing container images.\n\n## Failing Images\n- `ghcr.io/illmadecoder/otel-demo-user-service:latest` - 403 Forbidden\n- `ghcr.io/illmadecoder/otel-demo-order-service:latest` - 403 Forbidden  \n- `ghcr.io/illmadecoder/otel-demo-payment-service:latest` - 403 Forbidden\n\n## Working Components\n- Grafana: Running\n- Prometheus: Running\n- OTEL Collector: Running\n- Tempo: Running\n\n## Action Required\nBuild and push demo app images, or replace with existing public OTEL demo images.\n\n## File\n`experiments/scenarios/otel-tutorial/manifests/otel-demo-app.yaml`\n","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-12T07:52:09.999549Z","updated_at":"2026-01-12T20:26:35.325050174-05:00","closed_at":"2026-01-12T20:26:35.325050174-05:00","close_reason":"Superseded by illm-k8s-ai-lab-999 fix"}
{"id":"illm-k8s-ai-lab-ong","title":"Tier 2: Stabilize observability stack — all Progressing apps to Healthy","description":"Epic: Get grafana, loki, mimir, tempo, seaweedfs from Progressing to Healthy.\n\nSubtasks:\n1. Diagnose why each is stuck in Progressing (PVC issues? resource limits? missing config? readiness probes?).\n2. Fix SeaweedFS — both the app and its config are Progressing + OutOfSync.\n3. Fix Loki, Mimir, Tempo — the metrics/logs/traces pipeline.\n4. Fix Grafana — visualization depends on the above backends.\n5. Validate end-to-end: can a pod's metrics/logs/traces flow through to Grafana?\n\nRelated existing beads:\n- cl1 (P1): SeaweedFS S3 endpoint stuck in notReadyAddresses — directly related\n- dcm (P2): SeaweedFS PostSync hook doesn't trigger — bucket creation issue\n- jne (P2): StatefulSet spec changes require deletion — affects Mimir/Loki\n- s9m (P1): Kyverno CrashLoopBackOff — may cause admission webhook interference\n- ewz (P0): Scheduler stuck after reboot — could be root cause of Progressing state\n\nSuccess criteria: All 5 observability apps show Synced+Healthy. Grafana can display metrics from Mimir, logs from Loki, traces from Tempo.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-02T17:30:58.283482322-05:00","created_by":"illm","updated_at":"2026-02-02T18:18:25.855952804-05:00","closed_at":"2026-02-02T18:18:25.855952804-05:00","close_reason":"All 5 observability apps (grafana, loki, mimir, tempo, seaweedfs) now Synced+Healthy. Fixed stale PVC node affinity issue (PVs bound to old node talos-jid-knd). Deleted stale PVCs and pods, new PVCs created on current node talos-23n-3ay.","labels":["cleanup observability"],"dependencies":[{"issue_id":"illm-k8s-ai-lab-ong","depends_on_id":"illm-k8s-ai-lab-h2o","type":"blocks","created_at":"2026-02-02T17:31:26.776435972-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-pcj","title":"Talos enforces Pod Security Standards - DaemonSets blocked","description":"Talos enforces PSS baseline by default. DaemonSets like promtail, node-exporter that need hostPath/hostNetwork/hostPID are blocked. Fix: label namespaces with pod-security.kubernetes.io/enforce=privileged. Consider adding this to talos:up task or ArgoCD app.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-12T20:36:20.437720096-05:00","created_by":"illm","updated_at":"2026-01-12T20:37:33.480990498-05:00","closed_at":"2026-01-12T20:37:33.480990498-05:00","close_reason":"Fixed by auto-labeling namespace in talos:up task","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-png","title":"Scheduler and controller-manager failing startup probes","status":"closed","priority":0,"issue_type":"bug","created_at":"2026-01-26T22:02:54.161135379-05:00","created_by":"illm","updated_at":"2026-01-26T22:11:21.739456076-05:00","closed_at":"2026-01-26T22:11:21.739456076-05:00","close_reason":"Duplicate of illm-k8s-ai-lab-ewz - scheduler issue","labels":["timing","toil"]}
{"id":"illm-k8s-ai-lab-qn5","title":"Test loki-tutorial experiment and tutorial","description":"Deploy loki-tutorial experiment CR, verify vcluster + components (loki, promtail, kube-prometheus-stack, log-generator), run through tutorial.yaml content, validate checkpoints.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-06T13:28:28.791995159-05:00","created_by":"illm","updated_at":"2026-02-06T14:09:31.335784132-05:00","closed_at":"2026-02-06T14:09:31.335784132-05:00","close_reason":"Tested: loki-tutorial experiment deploys successfully. Loki, Promtail, Grafana, log-generator all running. Tutorial content validates well but has wrong app label references (see illm-k8s-ai-lab-6hc). Node-exporter port conflict expected on hub.","labels":["testing"]}
{"id":"illm-k8s-ai-lab-qqs","title":"seaweedfs-tutorial: Create experiment.yaml and component CR","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-06T18:28:19.021091223-05:00","created_by":"illm","updated_at":"2026-02-06T18:31:03.288842056-05:00","closed_at":"2026-02-06T18:31:03.288842056-05:00","close_reason":"Created seaweedfs component CR and experiment.yaml deploying SeaweedFS + Prometheus/Grafana to GKE"}
{"id":"illm-k8s-ai-lab-r5e","title":"OpenBao PVC→hostPath migration requires manual StatefulSet delete","description":"One-time issue: switching OpenBao from PVC (local-path-retain) to hostPath requires deleting the StatefulSet (volumeClaimTemplates are immutable) and PVC/PV. ArgoCD then recreates with hostPath. Not a recurring issue — only happens during the migration.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-28T15:18:13.94031291-05:00","created_by":"illm","updated_at":"2026-01-28T15:18:24.7114629-05:00","closed_at":"2026-01-28T15:18:24.7114629-05:00","close_reason":"One-time migration completed. StatefulSet, PVC, and PVs deleted manually. ArgoCD recreated with hostPath. Won't recur.","labels":["config"]}
{"id":"illm-k8s-ai-lab-r8c","title":"Control-plane taint returns after reboot","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-26T21:47:11.646643734-05:00","created_by":"illm","updated_at":"2026-02-02T18:38:22.232918614-05:00","closed_at":"2026-02-02T18:38:22.232918614-05:00","close_reason":"Mitigated: allowSchedulingOnControlPlanes: true is correctly set. The transient NoSchedule taint during boot is normal Kubernetes behavior — running pods are not evicted. Talos removes it during reconciliation. Critical workloads (Crossplane providers) already have tolerations.","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-rle","title":"Duplicate cluster secrets cause ArgoCD sync failures","description":"Two different secret naming patterns are used:\n- kind:up creates 'argocd-cluster-{cluster}' (line ~504)\n- kind:conduct creates 'cluster-{cluster}' (line ~794)\n- kind:down only deletes 'cluster-{cluster}'\n\nThis leaves orphaned 'argocd-cluster-*' secrets that cause 'there are 2 clusters with the same name' errors.\n\nFix: Standardize on one naming pattern and clean up both in down task.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T13:41:50.937981592-05:00","created_by":"illm","updated_at":"2026-01-10T13:45:51.370671733-05:00","closed_at":"2026-01-10T13:45:51.370671733-05:00","close_reason":"Closed","labels":["config","toil"]}
{"id":"illm-k8s-ai-lab-s59","title":"observability-cost-tutorial: Create experiment.yaml and component CRs","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-06T18:28:23.77717304-05:00","created_by":"illm","updated_at":"2026-02-06T18:31:09.737609553-05:00","closed_at":"2026-02-06T18:31:09.737609553-05:00","close_reason":"Created cardinality-generator component CR and experiment.yaml deploying cost analysis stack to GKE"}
{"id":"illm-k8s-ai-lab-s9m","title":"Kyverno admission controller CrashLoopBackOff on etcd timeouts","description":"Kyverno admission controller enters CrashLoopBackOff due to etcd/API server timeouts. Logs show 'etcdserver: request timed out' and 'failed to renew lease'.\n\nInvestigation findings:\n- Resource usage is low (7m CPU, 40Mi memory) - not resource exhaustion\n- No Istio sidecar (istio-injection=disabled on namespace)\n- Likely intermittent API server load during cluster startup\n- Currently stable with 36 restarts but running\n\nMitigations applied:\n- failurePolicy: Ignore prevents cluster-wide pod blocking (commit 847bfc6)\n\nRemaining:\n- Monitor for recurrence\n- Consider increasing liveness probe timeouts if issue persists","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-26T09:57:35.316491598-05:00","created_by":"illm","updated_at":"2026-02-02T18:38:48.738909374-05:00","closed_at":"2026-02-02T18:38:48.738909374-05:00","close_reason":"Mitigated: failurePolicy: Ignore prevents cluster-wide pod blocking. Added startup probe (30x10s=5min window) for graceful startup on slow API server. Kyverno currently stable at 0 restarts on latest boot.","labels":["kyverno","timing","toil"]}
{"id":"illm-k8s-ai-lab-t47","title":"GKE nodepool async create hangs after provider restart","description":"After Crossplane provider-gcp-container restart, nodepool creation fires 'CreatedExternalResource' but async callback never returns. atProvider stays empty indefinitely. This happened 4 times in sequence for gateway-comparison-zzdxl-app-95n6s-nodes. Root cause likely: GKE has pending operations from previous failed creates, blocking new creates. Provider has no timeout for async creates — it just hangs forever. Related to illm-k8s-ai-lab-d0y. Workaround: delete experiment and re-run. Fix: either add async operation timeout or use observe-before-create pattern.","status":"open","priority":1,"issue_type":"bug","created_at":"2026-02-13T09:37:18.486714601-05:00","created_by":"illm","updated_at":"2026-02-13T09:37:18.486714601-05:00","labels":["gateway-comparison","resources","toil"]}
{"id":"illm-k8s-ai-lab-t4h","title":"Deploy PostgreSQL on hub cluster","description":"Deploy PostgreSQL for experiment results storage. Schema includes: run metadata, metrics summaries, budget tracking, JSONB columns.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T12:46:10.871937093-05:00","created_by":"illm","updated_at":"2026-02-02T18:30:16.46568289-05:00","closed_at":"2026-02-02T18:30:16.46568289-05:00","close_reason":"Removed per user request","dependencies":[{"issue_id":"illm-k8s-ai-lab-t4h","depends_on_id":"illm-k8s-ai-lab-9p4","type":"blocks","created_at":"2026-01-17T12:46:40.857343699-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-tel","title":"gateway-tutorial requires grpcurl for Part 5","description":"Part 5 gRPC tests require grpcurl to be installed. The tutorial assumes grpcurl is available but doesn't include installation instructions. Should either add grpcurl install steps or use a grpcurl pod for testing.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-15T17:39:29.586369657-05:00","created_by":"illm","updated_at":"2026-01-15T17:49:50.772898456-05:00","closed_at":"2026-01-15T17:49:50.772898456-05:00","close_reason":"Fixed: Added GatewayClass to manifests, added grpcurl test pod, documented OCI workaround","labels":["gateway-tutorial"]}
{"id":"illm-k8s-ai-lab-u10","title":"Kyverno webhooks block namespace deletion after uninstall","description":"When deleting namespaces after removing Kyverno, the validating webhooks (validate.kyverno.svc-fail) block resource deletion because the webhook service no longer exists. Need to clean up MutatingWebhookConfiguration and ValidatingWebhookConfiguration resources.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-24T15:07:33.074785248-05:00","created_by":"illm","updated_at":"2026-01-24T17:28:34.15253213-05:00","closed_at":"2026-01-24T17:28:34.15253213-05:00","close_reason":"Addressed by talos-hub Taskfile automation (task talos-hub:reset handles webhook cleanup, finalizer removal, and OpenBao re-init)","labels":["kyverno","toil"]}
{"id":"illm-k8s-ai-lab-uxp","title":"Workflow manager uses namespace 'argo' instead of 'argo-workflows'","description":"internal/workflow/manager.go:19 has DefaultNamespace = 'argo' but argo-workflows is deployed in 'argo-workflows' namespace on the hub. Causes 'namespaces argo not found' when submitting workflows.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-02-06T13:37:03.168080019-05:00","created_by":"illm","updated_at":"2026-02-06T14:09:20.286497919-05:00","closed_at":"2026-02-06T14:09:20.286497919-05:00","close_reason":"Fixed: DefaultNamespace changed to argo-workflows","labels":["config"]}
{"id":"illm-k8s-ai-lab-vek","title":"ArgoCD deployments missing app.kubernetes.io/instance label","description":"After cluster reset and ArgoCD redeploy, deployment templates are missing app.kubernetes.io/instance label required by service selectors. This causes pods to not be registered as endpoints. Workaround: manually patch deployments to add the label.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-26T08:30:15.293829645-05:00","created_by":"illm","updated_at":"2026-01-26T12:01:38.153694672-05:00","closed_at":"2026-01-26T12:01:38.153694672-05:00","close_reason":"Fixed in commit 797208a - added podLabels with instance label to all ArgoCD components","labels":["argocd","toil"]}
{"id":"illm-k8s-ai-lab-viw","title":"ArgoCD pods missing container port names after Istio injection","description":"When ArgoCD pods are injected with Istio sidecars, the container port names are lost. Services using targetPort by name (repo-server, http, grpc) can't find endpoints. Workaround: patch services to use numeric targetPort instead of named ports.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-26T08:30:08.73530669-05:00","created_by":"illm","updated_at":"2026-01-26T12:01:38.313519971-05:00","closed_at":"2026-01-26T12:01:38.313519971-05:00","close_reason":"Transient issue - pods have proper port names, endpoints are working. Issue was during initial startup race condition.","labels":["argocd","istio","toil"]}
{"id":"illm-k8s-ai-lab-vq6","title":"Test hello-app experiment deployment","description":"Deploy hello-app experiment CR via kubectl, verify vcluster target provisions, hello-app deploys, workflow validates.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-06T13:28:27.775674222-05:00","created_by":"illm","updated_at":"2026-02-06T14:09:31.250802427-05:00","closed_at":"2026-02-06T14:09:31.250802427-05:00","close_reason":"Tested: hello-app experiment lifecycle works end-to-end with hub cluster type. Full lifecycle: Pending→Provisioning→Ready→Running→Complete, clean deletion. ArgoCD app healthy, workflow succeeds.","labels":["testing"]}
{"id":"illm-k8s-ai-lab-vxu","title":"Crossplane cloud provider credentials lost - need to restore","description":"Crossplane cloud provider credentials were lost when OpenBao was reinitialized.\n\nSTATUS: Azure credentials restored to secret/cloud/azure\n- ExternalSecret now syncs successfully\n\nREMAINING ISSUE: Providers have package dependency conflict:\n'cannot resolve package dependencies: node provider-family-azure already exists'\nThis is a separate Crossplane packaging issue, not credentials.\n\nAWS and GCP credentials still need to be created and stored.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-26T10:02:43.185017453-05:00","created_by":"illm","updated_at":"2026-01-26T14:27:31.340068822-05:00","closed_at":"2026-01-26T14:27:31.340068822-05:00","close_reason":"All cloud credentials restored to OpenBao:\n- AWS: secret/cloud/aws (accessKeyId, secretAccessKey)\n- Azure: secret/cloud/azure (clientId, clientSecret, tenantId, subscriptionId)\n- GCP: secret/cloud/gcp (credentials JSON)\n\nNote: Crossplane providers still have package conflict issue (separate from credentials).","labels":["config","crossplane"]}
{"id":"illm-k8s-ai-lab-wod","title":"Talos prometheus-tutorial deployment validation - 2026-01-12","description":"## Deployment Test Results\n\nFull tear-down and redeploy of prometheus-tutorial on Talos cluster.\n\n### Timing Summary\n| Phase | Duration |\n|-------|----------|\n| Tear down | 0s |\n| Reset cluster | 21s |\n| Create ArgoCD app | 0s |\n| Trigger sync | 0s |\n| Wait for sync | 24s |\n| Wait for pods ready | 6s |\n| **Total deploy** | **30s** |\n| URL validation | 5s |\n\n### URL Validation Results\n| Service | URL | HTTP Code | Response Time |\n|---------|-----|-----------|---------------|\n| Grafana | 192.168.1.241:80 | 302 | 0.018s |\n| Prometheus | 192.168.1.242:9090 | 200 | 0.017s |\n| metrics-app | 192.168.1.240:80 | 200 | 0.017s |\n| station-monitor | 192.168.1.243:80 | 200 | 0.078s |\n\n### Issues Fixed During Session\n1. **RBAC permissions for kube-vip** - Cloud provider needed `configmaps` (create/update) and `events` (create/patch) permissions\n2. **ArgoCD ComparisonError** - Added `ignoreDifferences` for ReplicaSet/Deployment status fields\n\n### Commits\n- `92ce6ee` - fix: Add ignoreDifferences for ReplicaSet/Deployment status fields\n- `496c699` - fix: Add missing RBAC permissions for kube-vip cloud provider\n\n### Performance Notes\n- Sync phase (24s) is the bottleneck - waiting for ArgoCD to reconcile\n- All URL response times excellent (\u003c0.1s)\n- station-monitor slightly slower (0.078s vs ~0.017s for others)\n","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-12T07:11:21.367117Z","updated_at":"2026-01-12T07:11:21.367117Z","closed_at":"2026-01-12T07:11:21.367117Z"}
{"id":"illm-k8s-ai-lab-wra","title":"logging-comparison: Create experiment.yaml","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-06T18:28:20.2176094-05:00","created_by":"illm","updated_at":"2026-02-06T18:31:05.748954773-05:00","closed_at":"2026-02-06T18:31:05.748954773-05:00","close_reason":"Created experiment.yaml deploying Loki+ELK side-by-side to GKE for comparison"}
{"id":"illm-k8s-ai-lab-wup","title":"Envoy Gateway v1.2.4 CRD schema changes","description":"Envoy Gateway v1.2.4 changed CRD schemas: retryOn must use triggers array inside object, allowOrigins must be plain strings not type/value objects. Fixed manifests to match new schema.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-15T13:52:23.4655385-05:00","created_by":"illm","updated_at":"2026-01-15T13:52:38.867035014-05:00","closed_at":"2026-01-15T13:52:38.867035014-05:00","close_reason":"Fixed by updating to v1.2.4 schema","labels":["gateway-tutorial","toil"]}
{"id":"illm-k8s-ai-lab-x1z","title":"ArgoCD app stuck in terminating state after cluster deletion","description":"When tearing down experiments, we delete the ArgoCD app with --wait=false then immediately delete the Kind cluster. But the app has a resources-finalizer that requires ArgoCD to clean up resources on the target cluster. Since the cluster is already deleted, the finalizer can never complete and the app is stuck forever in Terminating state.\n\nObserved: deletionTimestamp set but app not deleted due to finalizer.\n\nImpact: Subsequent deploys see 'resource is currently being deleted' warning and may not work correctly.\n\nFix options:\n1. Wait for app deletion before cluster deletion (slow)\n2. Remove finalizer before deleting app when cluster is being destroyed\n3. Use cascade=orphan to skip resource cleanup","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T13:34:12.184556089-05:00","created_by":"illm","updated_at":"2026-01-10T13:45:51.154939305-05:00","closed_at":"2026-01-10T13:45:51.154939305-05:00","close_reason":"Closed","labels":["config","timing","toil"]}
{"id":"illm-k8s-ai-lab-y7o","title":"Rebuild otel-demo images with non-root user","description":"otel-demo container images run as root. Need to rebuild with USER directive in Dockerfile to support runAsNonRoot for full restricted PSS compliance.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-13T16:44:26.352319843-05:00","created_by":"illm","updated_at":"2026-01-13T20:32:27.159340479-05:00","closed_at":"2026-01-13T20:32:27.159340479-05:00","close_reason":"Rebuilt images with numeric UID (USER 1000) to satisfy Kubernetes runAsNonRoot verification. All pods now running with restricted PSS on Talos.","labels":["config","otel-tutorial"]}
{"id":"illm-k8s-ai-lab-y7w","title":"nginx-ingress snippets require annotations-risk-level","description":"nginx-ingress v1.9+ silently ignores snippet annotations without annotations-risk-level: Critical. This caused header-based-routing and header-manipulation ingresses to not get addresses. Added config fix.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-15T16:44:08.561735017-05:00","created_by":"illm","updated_at":"2026-01-15T17:41:20.037028683-05:00","closed_at":"2026-01-15T17:41:20.037028683-05:00","close_reason":"Fixed during gateway-tutorial validation - added annotations-risk-level to nginx-ingress config and changed grpcbin from gRPC to TCP probes","labels":["config","gateway-tutorial"]}
{"id":"illm-k8s-ai-lab-z07","title":"Test tsdb-comparison on Talos","description":"Test tsdb-comparison experiment on Talos platform. Goal: 'task talos:up -- tsdb-comparison' works with zero manual intervention. Cycle the experiment, identify and fix any toil. Create child beads for each issue discovered during testing.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T08:03:04.322198693-05:00","created_by":"illm","updated_at":"2026-01-13T09:21:19.089449253-05:00","closed_at":"2026-01-13T09:21:19.089449253-05:00","close_reason":"tsdb-comparison deploys successfully on Talos with all apps Healthy","labels":["talos-testing"]}
{"id":"illm-k8s-ai-lab-z07.1","title":"tsdb-comparison apps need ignoreDifferences for terminatingReplicas","description":"Kubernetes 1.32+ adds terminatingReplicas field. Need to add ignoreDifferences to prometheus, mimir, and workload apps.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-13T08:53:04.594401497-05:00","created_by":"illm","updated_at":"2026-01-13T09:21:19.083036006-05:00","closed_at":"2026-01-13T09:21:19.083036006-05:00","close_reason":"tsdb-comparison deploys successfully on Talos with all apps Healthy","labels":["talos-testing"],"dependencies":[{"issue_id":"illm-k8s-ai-lab-z07.1","depends_on_id":"illm-k8s-ai-lab-z07","type":"parent-child","created_at":"2026-01-13T08:53:04.595745045-05:00","created_by":"illm"}]}
{"id":"illm-k8s-ai-lab-z5z","title":"grpcbin doesn't support gRPC health protocol","description":"moul/grpcbin image doesn't implement grpc.health.v1.Health. K8s native gRPC health probes fail. Currently using TCP probes as workaround. Should create custom gRPC demo image that implements health protocol for proper tutorial.","status":"closed","priority":3,"issue_type":"bug","created_at":"2026-01-15T16:25:54.711056073-05:00","created_by":"illm","updated_at":"2026-01-15T17:41:20.051443669-05:00","closed_at":"2026-01-15T17:41:20.051443669-05:00","close_reason":"Fixed during gateway-tutorial validation - added annotations-risk-level to nginx-ingress config and changed grpcbin from gRPC to TCP probes","labels":["gateway-tutorial"]}
