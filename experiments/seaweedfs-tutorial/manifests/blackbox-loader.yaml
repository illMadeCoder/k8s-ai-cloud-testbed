# Blackbox Loader - Generates and loads sensor readings into SeaweedFS
#
# Demonstrates O(1) lookup by storing 100,000 small files and timing retrieval
apiVersion: v1
kind: ConfigMap
metadata:
  name: blackbox-loader-script
  namespace: seaweedfs
data:
  loader.sh: |
    #!/bin/bash
    set -e

    SEAWEED_FILER="${SEAWEED_FILER:-seaweedfs-filer:8888}"
    SEAWEED_S3="${SEAWEED_S3:-seaweedfs-s3:8333}"
    SENSOR_COUNT="${SENSOR_COUNT:-100000}"

    echo "=========================================="
    echo "  USS KUBERNETES - BLACKBOX RECOVERY"
    echo "=========================================="
    echo ""
    echo "Decrypting sealed sensor archives..."
    echo "Classification: EYES ONLY"
    echo ""
    echo "Records to restore: $SENSOR_COUNT"
    echo ""

    # Wait for SeaweedFS to be ready
    echo "Connecting to archive storage..."
    until curl -s "http://$SEAWEED_FILER/" > /dev/null 2>&1; do
      sleep 2
    done
    echo "Connection established."
    echo ""

    # Create the blackbox directory
    echo "Initializing blackbox archive..."
    curl -s -X POST "http://$SEAWEED_FILER/blackbox/"

    # Generate sensor readings
    echo "Restoring $SENSOR_COUNT sensor readings..."
    echo "Date range: Stardates 41000 - 51000 (10 year span)"
    echo ""

    START_TIME=$(date +%s%N)

    for i in $(seq 1 $SENSOR_COUNT); do
      # Generate a stardate (like 47634.44)
      YEAR=$((41000 + RANDOM % 10000))
      DECIMAL=$((RANDOM % 100))
      STARDATE="${YEAR}.${DECIMAL}"

      # Generate sensor data (small JSON, ~200 bytes)
      SENSOR_DATA=$(cat <<EOF
    {
      "stardate": "$STARDATE",
      "sensor_id": "SEN-$(printf '%05d' $i)",
      "sector": $((RANDOM % 20 + 1)),
      "readings": {
        "temperature": $((RANDOM % 100 + 200)),
        "radiation": $((RANDOM % 1000)),
        "hull_integrity": $((RANDOM % 10 + 90)),
        "warp_field": $(echo "scale=2; $RANDOM/32767" | bc)
      },
      "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
    }
    EOF
    )

      # Upload to SeaweedFS (using form upload)
      echo "$SENSOR_DATA" > /tmp/sensor.json
      curl -s -F "file=@/tmp/sensor.json" \
        "http://$SEAWEED_FILER/blackbox/sensors/${STARDATE}.json" > /dev/null

      # Progress indicator
      if [ $((i % 1000)) -eq 0 ]; then
        echo "  Uploaded $i / $SENSOR_COUNT readings..."
      fi
    done

    END_TIME=$(date +%s%N)
    DURATION=$(( (END_TIME - START_TIME) / 1000000 ))

    echo ""
    echo "=========================================="
    echo "  ARCHIVE RESTORATION COMPLETE"
    echo "=========================================="
    echo "  Records restored: $SENSOR_COUNT"
    echo "  Time elapsed: ${DURATION}ms"
    echo ""

    # Store the needle - the classified Sector 12 incident
    NEEDLE_STARDATE="47634.44"
    echo "[CLASSIFIED] Injecting Sector 12 incident record..."
    printf '{"stardate":"%s","sensor_id":"OUTPOST-12-FINAL","sector":12,"classification":"CLASSIFIED","alert":"DISTRESS SIGNAL DETECTED","readings":{"temperature":9999,"radiation":50000,"hull_integrity":45,"warp_field":0.00,"life_signs":0,"beacon":"AUTOMATED DISTRESS - NO SURVIVORS"},"note":"Official report redacted. This reading was sealed by Starfleet Command.","timestamp":"2367-05-23T14:32:00Z"}' "$NEEDLE_STARDATE" > /tmp/needle.json
    curl -s -F "file=@/tmp/needle.json" "http://$SEAWEED_FILER/blackbox/sensors/${NEEDLE_STARDATE}.json"

    echo ""
    echo "=========================================="
    echo "  READY FOR INVESTIGATION"
    echo "=========================================="
    echo ""
    echo "  The Sector 12 incident occurred somewhere around"
    echo "  stardate 47634.XX - but which exact reading?"
    echo ""
    echo "  In a traditional filesystem, searching 10,000 files"
    echo "  would take seconds. With SeaweedFS: milliseconds."
    echo ""
    echo "  Find the truth, Engineer."
    echo ""

    # Keep container running for exploration
    echo "[Archive terminal ready]"
    tail -f /dev/null

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blackbox-loader
  namespace: seaweedfs
spec:
  replicas: 1
  selector:
    matchLabels:
      app: blackbox-loader
  template:
    metadata:
      labels:
        app: blackbox-loader
    spec:
      containers:
        - name: loader
          image: alpine:3.19
          command: ["/bin/sh", "-c"]
          args:
            - |
              apk add --no-cache bash curl bc
              bash /scripts/loader.sh
          env:
            - name: SEAWEED_FILER
              value: "seaweedfs-filer:8888"
            - name: SENSOR_COUNT
              value: "10000"
          volumeMounts:
            - name: scripts
              mountPath: /scripts/loader.sh
              subPath: loader.sh
            - name: benchmark
              mountPath: /scripts/benchmark.sh
              subPath: benchmark.sh
          resources:
            requests:
              memory: 64Mi
              cpu: 100m
            limits:
              memory: 128Mi
              cpu: 500m
      volumes:
        - name: scripts
          configMap:
            name: blackbox-loader-script
            defaultMode: 0755
        - name: benchmark
          configMap:
            name: benchmark-script
            defaultMode: 0755

---
# Benchmark: SeaweedFS O(1) vs Filesystem O(n)
apiVersion: v1
kind: ConfigMap
metadata:
  name: benchmark-script
  namespace: seaweedfs
data:
  benchmark.sh: |
    #!/bin/bash
    echo "=========================================="
    echo "  THE LOSF PROBLEM: Why Object Storage?"
    echo "=========================================="
    echo ""
    echo "LOSF = Lots of Small Files"
    echo "Traditional filesystems degrade as file count grows."
    echo "Let's prove it."
    echo ""

    SEAWEED_FILER="${SEAWEED_FILER:-seaweedfs-filer:8888}"
    FILE_COUNT=10000
    TEST_DIR="/tmp/losf-test"

    # --- Phase 1: Create files on local filesystem ---
    echo "=========================================="
    echo "  PHASE 1: Creating $FILE_COUNT files on disk"
    echo "=========================================="
    rm -rf $TEST_DIR
    mkdir -p $TEST_DIR

    START=$(date +%s%N)
    for i in $(seq 1 $FILE_COUNT); do
      echo "{\"id\":$i,\"data\":\"sensor-reading\"}" > "$TEST_DIR/file_$i.json"
      if [ $((i % 2000)) -eq 0 ]; then
        echo "  Created $i files..."
      fi
    done
    END=$(date +%s%N)
    CREATE_TIME=$(( (END - START) / 1000000 ))
    echo "  Done. Time: ${CREATE_TIME}ms"
    echo ""

    # --- Phase 2: Filesystem lookup test ---
    echo "=========================================="
    echo "  PHASE 2: Filesystem Lookup (find + cat)"
    echo "=========================================="
    echo "  Finding file_5000.json among $FILE_COUNT files..."
    echo ""

    TOTAL_FS=0
    for run in 1 2 3 4 5; do
      START=$(date +%s%N)
      # This is what apps do: scan directory, find file, read it
      cat "$TEST_DIR/file_5000.json" > /dev/null
      END=$(date +%s%N)
      DURATION=$(( (END - START) / 1000000 ))
      TOTAL_FS=$((TOTAL_FS + DURATION))
      echo "  Run $run: ${DURATION}ms"
    done
    AVG_FS=$(echo "scale=2; $TOTAL_FS / 5" | bc)
    echo ""
    echo "  Average: ${AVG_FS}ms (filesystem cached - real perf is worse)"
    echo ""

    # Show what 'ls' looks like with many files
    echo "  Try 'ls' on $FILE_COUNT files:"
    START=$(date +%s%N)
    ls $TEST_DIR | wc -l > /dev/null
    END=$(date +%s%N)
    LS_TIME=$(( (END - START) / 1000000 ))
    echo "  ls | wc -l: ${LS_TIME}ms"
    echo ""

    # --- Phase 3: SeaweedFS lookup test ---
    echo "=========================================="
    echo "  PHASE 3: SeaweedFS Lookup (O(1))"
    echo "=========================================="
    echo "  Finding stardate 47634.44 among $FILE_COUNT files..."
    echo ""

    TOTAL_SW=0
    for run in 1 2 3 4 5; do
      START=$(date +%s%N)
      curl -s "http://$SEAWEED_FILER/blackbox/sensors/47634.44.json" > /dev/null
      END=$(date +%s%N)
      DURATION=$(( (END - START) / 1000000 ))
      TOTAL_SW=$((TOTAL_SW + DURATION))
      echo "  Run $run: ${DURATION}ms"
    done
    AVG_SW=$(echo "scale=2; $TOTAL_SW / 5" | bc)
    echo ""
    echo "  Average: ${AVG_SW}ms"
    echo ""

    # --- Results ---
    echo "=========================================="
    echo "  RESULTS"
    echo "=========================================="
    echo ""
    echo "  Filesystem (cached):  ${AVG_FS}ms per lookup"
    echo "  SeaweedFS:            ${AVG_SW}ms per lookup"
    echo ""
    echo "  At 10k files, filesystem is cached and fast."
    echo "  At 1M+ files, filesystem degrades to 50-200ms."
    echo "  SeaweedFS stays constant: O(1)."
    echo ""
    echo "  Why? Haystack architecture:"
    echo "    file_id -> volume_id -> offset -> ONE disk read"
    echo ""
    echo "  This is why Loki, Tempo, and Thanos use object storage."
    echo ""

    # Cleanup
    rm -rf $TEST_DIR

---
apiVersion: v1
kind: Service
metadata:
  name: seaweedfs-filer-external
  namespace: seaweedfs
spec:
  type: LoadBalancer
  selector:
    app.kubernetes.io/name: seaweedfs
    app.kubernetes.io/component: filer
  ports:
    - name: http
      port: 8888
      targetPort: 8888

---
apiVersion: v1
kind: Service
metadata:
  name: seaweedfs-s3-external
  namespace: seaweedfs
spec:
  type: LoadBalancer
  selector:
    app.kubernetes.io/name: seaweedfs
    app.kubernetes.io/component: filer
  ports:
    - name: s3
      port: 8333
      targetPort: 8333

---
apiVersion: v1
kind: Service
metadata:
  name: seaweedfs-master-external
  namespace: seaweedfs
spec:
  type: LoadBalancer
  selector:
    app.kubernetes.io/name: seaweedfs
    app.kubernetes.io/component: master
  ports:
    - name: http
      port: 9333
      targetPort: 9333
