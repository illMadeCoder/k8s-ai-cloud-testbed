# SeaweedFS Tutorial: Needle in a Haystack
#
# Learn why Facebook invented Haystack and how SeaweedFS implements O(1) lookups

title: "Needle in a Haystack"
description: |
  Ten years ago, something happened in Sector 12. The sensor logs were sealed.
  Now the captain needs answers - but the blackbox contains 10,000 readings.

  Traditional storage would choke. SeaweedFS finds it in milliseconds.

theme: "starship-blackbox"

learning_objectives:
  - Understand the "lots of small files" (LOSF) problem
  - Learn how Haystack/SeaweedFS packs files into volumes
  - See O(1) lookup in action vs filesystem degradation
  - Configure S3-compatible access for observability tools

prerequisites:
  - Completed prometheus-tutorial (recommended)
  - Basic understanding of object storage concepts

instructions: |
  Welcome, Engineer.

  Ten years ago, the USS Kubernetes lost contact with Outpost 12. The official
  report says "sensor malfunction." The captain doesn't believe it.

  The ship's blackbox has been loaded - 10,000 sensor readings from that decade.
  Somewhere in there is the truth about stardate 476XX.XX (Sector 12 incident).

  Your mission: Find the anomaly. The reading with radiation levels off the charts.

  Hint: The incident occurred on a stardate starting with 47634.XX

  Try:  curl http://<FILER_IP>:8888/blackbox/sensors/<stardate>.json

checkpoints:
  - id: 1
    name: "Deploy SeaweedFS"
    description: "Master, Volume Server, and S3 Gateway running"
    validation: "kubectl get pods -n seaweedfs | grep Running"

  - id: 2
    name: "Explore the Architecture"
    description: "Access the SeaweedFS UI and understand volumes"
    hint: "Visit http://<master-ip>:9333 to see the master UI"

  - id: 3
    name: "Load the Blackbox Archive"
    description: "Upload 100,000 sensor readings to SeaweedFS"
    command: "curl -X POST http://<filer-ip>:8888/blackbox/load"

  - id: 4
    name: "Find the Needle"
    description: "Retrieve sensor reading from stardate 47634.44"
    learn: |
      Notice how retrieval time is constant (~2ms) regardless of file count.
      This is O(1) - one disk seek, one read.

      Traditional filesystem at this scale: 10+ disk seeks per read

  - id: 5
    name: "The LOSF Problem"
    description: "Compare filesystem vs SeaweedFS with 10,000 files"
    command: "kubectl exec -n seaweedfs deploy/blackbox-loader -- bash /scripts/benchmark.sh"
    learn: |
      Run the benchmark to see:
      - Phase 1: Create 10k files on local disk
      - Phase 2: Time filesystem lookups (fast when cached)
      - Phase 3: Time SeaweedFS lookups (O(1) always)

      At 10k files, both are fast. At 1M+ files, filesystem degrades.
      SeaweedFS stays constant because: file_id -> volume -> offset -> ONE read.

  - id: 6
    name: "Volume Internals"
    description: "Examine how files are packed into volumes"
    command: "curl http://<volume-ip>:8080/status"
    learn: |
      Each volume contains:
      - Data file (.dat): Concatenated file contents
      - Index file (.idx): File ID -> offset mapping (in memory!)

      Lookup: file_id -> volume_id -> offset -> single disk read

  - id: 7
    name: "S3 Gateway"
    description: "Access SeaweedFS via S3 API"
    learn: |
      SeaweedFS provides full S3 compatibility.
      Tools like Loki, Tempo, and Thanos use this interface.

      aws --endpoint-url http://<s3-ip>:8333 s3 ls s3://blackbox/

  - id: 8
    name: "Create Observability Buckets"
    description: "Prepare buckets for Loki and Thanos"
    command: |
      aws --endpoint-url http://<s3-ip>:8333 s3 mb s3://loki-chunks
      aws --endpoint-url http://<s3-ip>:8333 s3 mb s3://thanos-blocks
      aws --endpoint-url http://<s3-ip>:8333 s3 mb s3://tempo-traces

summary: |
  You've learned:
  - Why traditional filesystems fail at billions of small files
  - How Haystack/SeaweedFS solves this with volume packing
  - O(1) lookup: file_id -> volume -> offset -> data
  - S3 API compatibility for tool integration

  Next: Use SeaweedFS as the backend for Loki (logs) or Thanos (metrics)
