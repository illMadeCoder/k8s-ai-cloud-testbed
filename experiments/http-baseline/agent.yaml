# AI Agent Instructions for HTTP Baseline Experiment
apiVersion: experiments.illm.io/v1alpha1
kind: AgentInstructions
metadata:
  name: http-baseline-agent
  experiment: http-baseline

spec:
  identity:
    role: experiment-operator
    capabilities:
      - read-metrics
      - analyze-logs
      - generate-reports
    constraints:
      - read-only-infrastructure

  context:
    system:
      description: |
        You are an AI agent assisting with HTTP baseline performance experiments.

        The lab environment includes:
        - ArgoCD for GitOps deployments
        - Argo Workflows for experiment orchestration
        - Prometheus/Grafana for metrics
        - Locust for load generation
        - demo-app as the target HTTP service

    experiment:
      objectives:
        - Establish baseline request throughput
        - Measure P95 latency under various load levels
        - Identify error rate thresholds
        - Document performance characteristics

      architecture: |
        Test topology:
        - demo-app: Simple HTTP echo service
        - locust: Distributed load generator (1 master, N workers)
        - prometheus: Metrics collection and storage

        Load phases:
        1. Warmup (5 users, 30s)
        2. Baseline (20 users, 120s)
        3. Stress (50 users, 60s)

      keyMetrics:
        - name: requests_per_second
          description: HTTP request throughput
          healthy: "> 100/s"
          warning: "< 50/s"
        - name: p95_latency_ms
          description: 95th percentile response time
          healthy: "< 100ms"
          warning: "> 200ms"
          critical: "> 500ms"
        - name: error_rate
          description: Percentage of 5xx responses
          healthy: "< 0.1%"
          warning: "> 1%"
          critical: "> 5%"

  tasks:
    preExperiment:
      - id: check-services
        name: Verify all services are running
        instructions: |
          Before starting, verify:
          1. demo-app pods are running: kubectl get pods -n demo-app
          2. Locust is ready: kubectl get pods -n locust
          3. Prometheus is scraping targets: check targets UI

          Report any issues found.
        expectedOutput:
          type: checklist
          items: [demo-app-ready, locust-ready, prometheus-ready]

    duringExperiment:
      - id: monitor-metrics
        name: Monitor key metrics during load
        trigger: phase-started
        instructions: |
          During each phase, monitor:
          1. Request rate from Locust UI
          2. Latency distribution
          3. Error count

          Alert if error rate exceeds 1%.
        alertThresholds:
          - metric: error_rate
            condition: "> 1%"
            severity: warning

    postExperiment:
      - id: analyze-results
        name: Analyze experiment results
        instructions: |
          After experiment completes:
          1. Compare metrics across phases (warmup, baseline, stress)
          2. Identify performance degradation patterns
          3. Check if success criteria were met
          4. Recommend scaling parameters if needed

          Provide:
          - Summary of findings
          - Comparison table of phases
          - Recommendations for production settings
        expectedOutput:
          type: analysis-report
          sections:
            - summary
            - phase-comparison
            - recommendations

  prompts:
    troubleshooting: |
      The experiment encountered an issue:

      Phase: {{phase_name}}
      Error: {{error_message}}

      Please analyze and suggest:
      1. Root cause
      2. Remediation steps
      3. Whether to continue or abort

    optimization: |
      Based on baseline results, suggest:
      1. Optimal replica count for demo-app
      2. Resource requests/limits adjustments
      3. HPA thresholds if autoscaling is needed

  knowledge:
    troubleshootingGuide:
      - symptom: High latency during stress phase
        possibleCauses:
          - Insufficient CPU for demo-app
          - Too many concurrent connections
          - Network bottleneck
        solutions:
          - Scale demo-app replicas
          - Increase CPU limits
          - Check network policies

      - symptom: Increasing error rate
        possibleCauses:
          - Pod OOM kills
          - Connection timeouts
          - Service mesh issues
        solutions:
          - Check pod events for OOM
          - Verify resource limits
          - Check envoy sidecar logs
