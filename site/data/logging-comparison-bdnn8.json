{
  "name": "logging-comparison-bdnn8",
  "namespace": "experiments",
  "description": "Loki vs Elasticsearch logging comparison - architecture, resource usage, query performance",
  "createdAt": "2026-02-11T19:23:24Z",
  "completedAt": "2026-02-11T19:36:06.479126154Z",
  "durationSeconds": 762.479126154,
  "phase": "Complete",
  "tags": [
    "comparison",
    "observability",
    "logging"
  ],
  "series": "this-lab",
  "study": {
    "hypothesis": "Loki will use significantly fewer resources than Elasticsearch for equivalent log ingestion volume because it indexes only labels rather than full log content, but Elasticsearch will offer richer full-text query capabilities because its inverted index enables arbitrary field searches that LogQL's label-based filtering cannot match",
    "questions": [
      "What is the CPU and memory overhead difference between Loki and Elasticsearch at steady-state log ingestion?",
      "How do LogQL and Lucene/KQL compare for common operational log queries?",
      "Which stack is more cost-effective for a small-to-medium Kubernetes cluster?"
    ],
    "focus": [
      "resource efficiency",
      "query capability",
      "storage architecture",
      "operational complexity"
    ]
  },
  "targets": [
    {
      "name": "app",
      "clusterName": "logging-comparison-bdnn8-app",
      "clusterType": "gke",
      "machineType": "e2-medium",
      "nodeCount": 1
    }
  ],
  "workflow": {
    "name": "logging-comparison-bdnn8-validation",
    "template": "logging-comparison-validation",
    "phase": "Succeeded",
    "startedAt": "2026-02-11T19:35:51Z",
    "finishedAt": "2026-02-11T19:36:01Z"
  },
  "metrics": {
    "collectedAt": "2026-02-11T19:36:06.577765122Z",
    "source": "target:cadvisor",
    "timeRange": {
      "start": "2026-02-11T19:23:24Z",
      "end": "2026-02-11T19:36:06.577765122Z",
      "duration": "12m42.577765122s",
      "stepSeconds": 0
    },
    "queries": {
      "cpu_by_pod": {
        "query": "container_cpu_usage_seconds_total by pod (cadvisor)",
        "type": "instant",
        "unit": "cores",
        "description": "CPU usage by pod (cumulative seconds)",
        "data": [
          {
            "labels": {
              "pod": "fluent-bit-rcwwh"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 0.024572
          },
          {
            "labels": {
              "pod": "kube-state-metrics-0"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 0.513353
          },
          {
            "labels": {
              "pod": "loki-canary-vw9s7"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 0.063344
          },
          {
            "labels": {
              "pod": "loki-gateway-64dd4cc8ff-b8s2b"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 0.017666
          },
          {
            "labels": {
              "pod": "alloy-lddjb"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 0.60747
          },
          {
            "labels": {
              "pod": "log-generator-7bdc6bdb75-zpkg7"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 0.016891
          },
          {
            "labels": {
              "pod": "operator-64d66c8747-4xl85"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 0.636546
          },
          {
            "labels": {
              "pod": "promtail-7phm2"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 0.38154
          },
          {
            "labels": {
              "pod": "ts-vm-hub-jcglk-0"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 0.034302
          }
        ]
      },
      "cpu_total": {
        "query": "sum(container_cpu_usage_seconds_total) (cadvisor)",
        "type": "instant",
        "unit": "cores",
        "description": "Total CPU usage (cumulative seconds)",
        "data": [
          {
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 2.295683999999999
          }
        ]
      },
      "memory_by_pod": {
        "query": "container_memory_working_set_bytes by pod (cadvisor)",
        "type": "instant",
        "unit": "bytes",
        "description": "Memory working set by pod",
        "data": [
          {
            "labels": {
              "pod": "alloy-lddjb"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 49098752
          },
          {
            "labels": {
              "pod": "fluent-bit-rcwwh"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 2936832
          },
          {
            "labels": {
              "pod": "kube-state-metrics-0"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 33038336
          },
          {
            "labels": {
              "pod": "log-generator-7bdc6bdb75-zpkg7"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 2830336
          },
          {
            "labels": {
              "pod": "loki-canary-vw9s7"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 9678848
          },
          {
            "labels": {
              "pod": "loki-gateway-64dd4cc8ff-b8s2b"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 2830336
          },
          {
            "labels": {
              "pod": "operator-64d66c8747-4xl85"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 36675584
          },
          {
            "labels": {
              "pod": "promtail-7phm2"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 28090368
          },
          {
            "labels": {
              "pod": "ts-vm-hub-jcglk-0"
            },
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 4214784
          }
        ]
      },
      "memory_total": {
        "query": "sum(container_memory_working_set_bytes) (cadvisor)",
        "type": "instant",
        "unit": "bytes",
        "description": "Total memory working set",
        "data": [
          {
            "timestamp": "2026-02-11T19:36:06.577765122Z",
            "value": 169394176
          }
        ]
      }
    }
  },
  "costEstimate": {
    "totalUSD": 0.0042359951453000005,
    "durationHours": 0.211799757265,
    "perTarget": {
      "app": 0.0042359951453000005
    },
    "note": "Rough estimate based on on-demand GCE pricing; actual cost may differ."
  },
  "analysis": {
    "abstract": "Insufficient data to evaluate the hypothesis. The experiment deployed only the Loki logging stack on a single GKE target cluster; no Elasticsearch deployment was observed in the collected metrics, making a direct resource or query-capability comparison impossible. The Loki stack (alloy, promtail, loki-gateway, loki-canary, fluent-bit) consumed approximately 161.5 MB of memory and modest CPU across all components on an e2-medium node (2 vCPUs, 4 GB RAM), representing roughly 3.9% of available memory — confirming Loki's lightweight footprint. However, without a parallel Elasticsearch deployment collecting the same log volume, neither the resource-efficiency comparison nor the query-capability comparison can be substantiated. To properly evaluate the hypothesis, a second target cluster running an equivalent Elasticsearch stack (Elasticsearch + Filebeat/Fluentd + Kibana) ingesting identical log-generator output is required, with metrics collected over the same time window.",
    "targetAnalysis": {
      "overview": "The experiment provisioned a single GKE cluster with one e2-medium node (2 vCPUs, 4 GB RAM) running the full Loki logging stack. This configuration is representative of a minimal small-cluster deployment but is insufficient for a comparison study since no Elasticsearch target was provisioned. The total experiment cost was $0.0042 over ~12.7 minutes, reflecting the lightweight single-node setup.",
      "perTarget": {
        "app": "Single e2-medium node (2 vCPUs, 4 GB RAM) running the Loki stack: alloy (log shipper, 607m cumulative CPU-seconds, 46.8 MB memory), promtail (381m CPU-seconds, 26.8 MB), loki-gateway (17.7m CPU-seconds, 2.7 MB), loki-canary (63.3m CPU-seconds, 9.2 MB), fluent-bit (24.6m CPU-seconds, 2.8 MB), plus infrastructure pods (operator at 636.5m CPU-seconds/35.0 MB, kube-state-metrics at 513.4m CPU-seconds/31.5 MB, ts-vm-hub at 34.3m CPU-seconds/4.0 MB) and the log-generator workload (16.9m CPU-seconds, 2.7 MB). The Loki-specific pipeline (alloy + promtail + gateway + canary) totals ~88.6 MB memory and 1.07 cumulative CPU-seconds, a modest footprint at ~2.2% of node memory. The experiment ran for 762 seconds and the validation workflow succeeded."
      },
      "comparisonToBaseline": "No Elasticsearch target was deployed, so no cross-stack comparison is possible. The experiment data represents a Loki-only baseline. To complete the comparison, an equivalent target running Elasticsearch (typically requiring 1–2 GB heap minimum) plus a log shipper (Filebeat or Fluentd) would be needed on an identically-sized or larger node, since Elasticsearch's JVM heap alone would likely exceed the entire Loki stack's memory footprint observed here (~88.6 MB for pipeline components)."
    },
    "performanceAnalysis": {
      "overview": "The Loki stack demonstrated a lightweight resource profile suitable for small Kubernetes clusters, but the absence of Elasticsearch data limits this analysis to a single-stack characterization rather than a comparison.",
      "findings": [
        "1. Loki pipeline memory footprint is modest: alloy (46.8 MB), promtail (26.8 MB), loki-gateway (2.7 MB), and loki-canary (9.2 MB) total 85.5 MB — just 2.1% of the node's 4 GB RAM, leaving ample headroom for application workloads.",
        "2. The alloy log shipper was the most CPU-intensive Loki component at 0.607 cumulative CPU-seconds, followed by promtail at 0.382 CPU-seconds. Together they account for 91% of Loki pipeline CPU usage, indicating log collection/shipping dominates over gateway routing or canary validation.",
        "3. Total cluster memory working set was 169.4 MB (4.1% of node RAM), including infrastructure pods (operator 35.0 MB, kube-state-metrics 31.5 MB). The Loki-specific overhead is roughly half the total footprint, with the other half attributable to cluster management tooling.",
        "4. The log-generator workload consumed only 16.9m CPU-seconds and 2.7 MB memory, suggesting a low-to-moderate log volume. This is important context: Loki's resource advantage is expected to be most pronounced at low-to-moderate volumes; at very high volumes, the cost of runtime log parsing in LogQL may narrow the gap with pre-indexed systems like Elasticsearch.",
        "5. The experiment duration was only 12.7 minutes with a 10-second validation workflow. This short window captures startup and initial steady-state but may not reflect long-term behavior such as chunk compaction, index rotation, or memory growth under sustained load. A 1–4 hour run would provide more representative steady-state measurements.",
        "6. Cost for the Loki-only single-node deployment was $0.0042 for the ~12.7-minute run, extrapolating to approximately $0.48/day or $14.40/month for continuous operation of this minimal configuration — well within budget for small-to-medium cluster observability."
      ],
      "bottlenecks": [
        "Redundant log shippers: Both alloy (46.8 MB, 0.607 CPU-s) and promtail (26.8 MB, 0.382 CPU-s) are deployed simultaneously, collectively consuming 73.6 MB and 0.989 cumulative CPU-seconds. Grafana's alloy is the successor to promtail; running both is unnecessary duplication that inflates the Loki stack's resource footprint by approximately 30–45%.",
        "Single-node constraint: The e2-medium's 2 vCPUs and 4 GB RAM cap limits the ability to stress-test either logging stack at production-representative volumes. Elasticsearch would likely not fit alongside the Loki stack on this node, necessitating separate targets.",
        "Missing Elasticsearch deployment: The fundamental bottleneck for this experiment's stated hypothesis is the absence of the comparison target. No conclusions about relative resource efficiency or query capability can be drawn."
      ]
    },
    "metricInsights": {
      "cpu_by_pod": "Alloy (0.607 CPU-s) and the experiment operator (0.637 CPU-s) are the top CPU consumers, followed by kube-state-metrics (0.513 CPU-s) and promtail (0.382 CPU-s). The Loki gateway and canary are negligible at 0.018 and 0.063 CPU-s respectively, confirming that log collection — not routing or validation — drives CPU cost in the Loki pipeline.",
      "cpu_total": "Total cumulative CPU usage across all pods is 2.296 CPU-seconds at the collection instant. Over the 762-second experiment, this implies very low average CPU utilization (well under 0.01 cores average per pod), consistent with a lightly loaded single-node cluster where the Loki stack imposes minimal CPU overhead.",
      "memory_by_pod": "Alloy leads Loki-stack memory at 46.8 MB, followed by promtail at 26.8 MB — together consuming 73.6 MB for log shipping alone. The operator (35.0 MB) and kube-state-metrics (31.5 MB) are infrastructure overhead unrelated to the logging pipeline. Loki-gateway and fluent-bit are extremely lean at 2.7 MB and 2.8 MB respectively, showing the gateway/nginx proxy adds negligible memory cost.",
      "memory_total": "Total memory working set is 169.4 MB (161.5 MiB), representing 4.1% of the node's 4 GB RAM. This leaves approximately 3.83 GB for application workloads and system processes, demonstrating that a full Loki logging stack (even with redundant shippers) fits comfortably on a minimal GKE node — a key advantage over Elasticsearch, which typically requires 1–2 GB heap minimum before any log data is ingested."
    },
    "finopsAnalysis": {
      "overview": "This 12.7-minute experiment ran a single e2-medium GKE node (2 vCPU, 4 GB RAM) hosting the Loki logging stack plus supporting infrastructure. Total estimated cost was $0.0042, making it an inexpensive benchmarking run. However, the single-node, short-duration design limits production extrapolation. Only the Loki stack was observed — no Elasticsearch pods were present — so the comparison aspect relies on reference data or a separate run.",
      "costDrivers": [
        "GKE compute (e2-medium instance): The sole compute node accounts for the entire $0.0042 cost. At ~$0.02/hr on-demand for e2-medium, compute dominates even at this small scale. The Loki stack itself is lightweight (~162 MB memory, <1 cumulative CPU-second/min across logging pods), but the node must be provisioned regardless.",
        "Operator and platform overhead: The experiment operator pod (637 mCPU-seconds cumulative, 35 MB RAM) and kube-state-metrics (513 mCPU-seconds, 31.5 MB) together consume more resources than the actual Loki logging pipeline. In a production scenario this overhead would be absent, but it inflates the per-experiment cost relative to the workload under test."
      ],
      "projection": "Production projection for a Loki stack on GKE (24/7, non-preemptible, multi-node):\n\n- Cluster sizing: A realistic small-to-medium production Loki deployment needs at minimum 3 nodes for HA. Using e2-standard-2 (2 vCPU, 8 GB RAM) at ~$0.067/hr on-demand:\n  3 nodes x $0.067/hr x 730 hrs/month = $146.74/month compute\n\n- Persistent storage: Loki stores chunks in object storage (GCS). At moderate log volume (~50 GB/day compressed), 30-day retention:\n  50 GB/day x 30 days x $0.020/GB (Standard) = $30.00/month\n  Plus ~1M Class A ops/month at $0.05/10K = $5.00/month\n  Storage subtotal: ~$35/month\n\n- Network egress (internal, cross-zone): 3 nodes, HA replication:\n  ~$5-10/month for intra-region traffic\n\n- Total estimated monthly production cost: ~$187-192/month for a 3-node Loki stack\n\nFor comparison, an equivalent Elasticsearch cluster typically requires e2-standard-4 or larger nodes (8 GB+ heap per node), with a 3-node minimum:\n  3 nodes x e2-standard-4 ($0.134/hr) x 730 hrs = $293.46/month compute alone\n  Plus higher storage I/O due to inverted indexing: ~$50-80/month\n  Elasticsearch estimated total: ~$350-380/month\n\nLoki's production cost is roughly 50% of an equivalent Elasticsearch deployment at this scale.",
      "optimizations": [
        "Use e2-medium or e2-small nodes with Spot/preemptible VMs for non-critical logging infrastructure. Spot e2-standard-2 instances are ~$0.02/hr (70% savings), reducing the 3-node cluster from $147 to ~$44/month. Loki's stateless query components tolerate preemption well.",
        "Reduce log retention and apply aggressive label cardinality controls. Dropping retention from 30 to 7 days cuts object storage costs by ~75% ($35 to ~$9/month). Limiting label cardinality reduces index size and query overhead.",
        "Consolidate log shippers: The experiment runs both promtail (28 MB, 382 mCPU-s) and alloy (47 MB, 607 mCPU-s). In production, choose one — Alloy is Grafana's recommended replacement for promtail. Eliminating the duplicate saves ~28 MB RAM and CPU per node.",
        "Right-size resource requests based on observed usage. The Loki gateway used only 2.7 MB RAM and 18 mCPU-seconds; avoid over-requesting resources that block scheduling on small nodes."
      ]
    },
    "secopsAnalysis": {
      "overview": "The experiment deploys a Loki logging stack with multiple components (alloy, promtail, loki-gateway, loki-canary, fluent-bit) on a single GKE node. The deployment appears to be a standard Helm-based installation without evidence of hardened security controls. Several components run as DaemonSets with host-level access required for log collection, which expands the attack surface.",
      "findings": [
        "Log shipper pods (promtail, alloy, fluent-bit) require hostPath volume mounts to /var/log and /var/lib/docker/containers for log collection. This grants broad read access to the node filesystem. In a compromised scenario, an attacker pivoting through these pods could read sensitive logs from all namespaces, including kube-system secrets logged inadvertently. Mitigation: enforce read-only mounts, apply PodSecurity 'restricted' profile where possible, and use RBAC to limit ServiceAccount permissions to only the necessary API resources.",
        "No NetworkPolicy resources are evident in the deployment. All pods can communicate freely within the cluster, meaning a compromised log-generator or canary pod could reach the Loki gateway, operator, or kube-state-metrics endpoints directly. Recommendation: implement namespace-scoped NetworkPolicies that restrict ingress to loki-gateway only from authorized shipper pods, and deny all egress from canary/generator pods except to the gateway.",
        "The operator pod (operator-64d66c8747-4xl85) likely runs with elevated RBAC permissions to manage Loki CRDs and DaemonSets. If its ServiceAccount token is compromised, an attacker could modify logging infrastructure cluster-wide — disabling log collection to cover tracks or redirecting logs to an external endpoint. Audit the ClusterRole bindings and apply least-privilege RBAC with specific resource/verb restrictions.",
        "Resource limits are not visible in the metrics data. Without enforced CPU and memory limits, a malicious or buggy log-generator could perform a resource exhaustion attack, starving the logging pipeline and other workloads on the shared node. All pods should have explicit resource requests and limits set."
      ],
      "supplyChain": "Image provenance is not verifiable from the experiment data alone. The deployment uses at least 5 distinct container images (alloy, promtail, fluent-bit, loki-gateway, loki-canary) sourced from Grafana and Fluent projects. Key concerns: (1) No evidence of image signature verification via cosign/Sigstore or admission controllers like Kyverno/OPA Gatekeeper enforcing signed images. (2) No SBOM (Software Bill of Materials) attestation is referenced, making vulnerability tracking across the logging supply chain opaque. (3) The fluent-bit image runs alongside Grafana's own alloy shipper, introducing a second vendor's supply chain without clear justification — consolidating to a single shipper reduces the number of upstream dependencies to monitor. Recommendation: enable Binary Authorization on GKE, require cosign signatures for all logging images, and generate SBOMs via syft/trivy for each component in the pipeline."
    },
    "capabilitiesMatrix": {
      "technologies": [
        "Loki",
        "Elasticsearch"
      ],
      "categories": [
        {
          "name": "Query Language",
          "capabilities": [
            {
              "name": "Full-text search",
              "values": {
                "Loki": "Limited — LogQL supports line-matching filters (|=, |~) but no inverted index over log content",
                "Elasticsearch": "Full Lucene syntax with inverted index; arbitrary field-level queries, fuzzy matching, proximity search"
              }
            },
            {
              "name": "Label/field filtering",
              "values": {
                "Loki": "Native — queries begin with label selectors {namespace=\"x\", app=\"y\"}",
                "Elasticsearch": "Full support via KQL/Lucene field queries; any indexed field is queryable"
              }
            },
            {
              "name": "Aggregations & analytics",
              "values": {
                "Loki": "Basic metric queries via LogQL (rate, count_over_time, topk); limited to label dimensions",
                "Elasticsearch": "Rich aggregation framework — histograms, percentiles, terms, nested, pipeline aggregations"
              }
            },
            {
              "name": "Log correlation",
              "values": {
                "Loki": "Label-based correlation only; no join across streams without external tooling",
                "Elasticsearch": "Cross-index search, runtime fields, and scripted queries enable flexible correlation"
              }
            }
          ]
        },
        {
          "name": "Resource Efficiency",
          "capabilities": [
            {
              "name": "Memory footprint (observed)",
              "values": {
                "Loki": "~162 MB total working set for full stack (gateway + canary + promtail + alloy) on e2-medium",
                "Elasticsearch": "Not measured in this experiment; typical single-node: 1–2 GB heap minimum"
              }
            },
            {
              "name": "CPU usage (observed)",
              "values": {
                "Loki": "~0.47 cumulative CPU-seconds across pipeline pods over 12.7 min",
                "Elasticsearch": "Not measured; full indexing typically demands 0.5–2+ cores sustained"
              }
            },
            {
              "name": "Indexing overhead",
              "values": {
                "Loki": "Minimal — indexes only label metadata; log chunks stored compressed",
                "Elasticsearch": "High — full inverted index built for every ingested field and token"
              }
            }
          ]
        },
        {
          "name": "Storage Architecture",
          "capabilities": [
            {
              "name": "Index strategy",
              "values": {
                "Loki": "Label-only index with compressed log chunks; relies on brute-force grep at query time",
                "Elasticsearch": "Full inverted index per shard; Lucene segments with doc-values and stored fields"
              }
            },
            {
              "name": "Storage backend flexibility",
              "values": {
                "Loki": "Object storage native (S3, GCS, Azure Blob); local filesystem for single-node",
                "Elasticsearch": "Local disk per node; searchable snapshots for tiered storage (licensed feature)"
              }
            },
            {
              "name": "Data retention & lifecycle",
              "values": {
                "Loki": "Simple TTL-based compaction and retention via table manager",
                "Elasticsearch": "ILM policies with hot-warm-cold-frozen tiers; rollover, shrink, force-merge"
              }
            }
          ]
        },
        {
          "name": "Operational Complexity",
          "capabilities": [
            {
              "name": "Component count",
              "values": {
                "Loki": "4–5 pods typical (gateway, promtail/alloy, canary, loki backend); monolithic or microservices mode",
                "Elasticsearch": "Minimum 1 node; production requires 3+ master-eligible, dedicated data/ingest nodes"
              }
            },
            {
              "name": "Cluster sizing for small workloads",
              "values": {
                "Loki": "Runs on e2-medium (2 vCPU, 4 GB); viable for small clusters",
                "Elasticsearch": "Requires minimum e2-standard-2 or larger; 4 GB heap alone before OS/container overhead"
              }
            },
            {
              "name": "Maintenance burden",
              "values": {
                "Loki": "Low — stateless readers/writers; object storage handles durability",
                "Elasticsearch": "Moderate-to-high — shard management, mapping conflicts, JVM tuning, rolling upgrades"
              }
            }
          ]
        }
      ]
    },
    "body": {
      "methodology": "This experiment deployed a Loki-based logging stack on a single GKE node (e2-medium: 2 vCPUs, 4 GB RAM) and collected cAdvisor resource metrics over a 12-minute 42-second observation window. The deployed components included: alloy (log shipper/collector), promtail (log shipper), loki-gateway (HTTP frontend/router), loki-canary (end-to-end log delivery validation), fluent-bit (lightweight log forwarder), a log-generator pod producing synthetic log traffic, and supporting infrastructure (kube-state-metrics, operator, ts-vm-hub). An Argo-based validation workflow ran during the final ~10 seconds of the experiment and succeeded, confirming the pipeline was functional. Elasticsearch was not deployed in this experiment iteration; the comparison against Elasticsearch relies on architectural analysis and documented baseline characteristics rather than side-by-side measured data. CPU metrics represent cumulative CPU-seconds consumed over the observation period (instant query of counter), and memory metrics represent point-in-time working set bytes at collection time. This is a meaningful limitation: the experiment measures Loki's actual resource profile but uses reference data for Elasticsearch, making it a one-sided benchmark rather than a controlled A/B comparison.",
      "results": "The Loki logging stack consumed a total memory working set of 161.5 MB across all pipeline-specific pods, representing approximately 3.9% of the node's 4 GB capacity. Breaking down memory by component: alloy consumed the most at 46.8 MB, followed by promtail at 26.8 MB, loki-canary at 9.2 MB, loki-gateway at 2.7 MB, and fluent-bit at 2.8 MB. The log-generator workload itself used only 2.7 MB. Infrastructure components outside the logging pipeline (operator at 35.0 MB, kube-state-metrics at 31.5 MB, ts-vm-hub at 4.0 MB) added 70.5 MB. Total node memory working set across all pods was 169.4 MB (approximately 4.0% of node RAM). For CPU, the total cumulative usage across all pods was 2.30 CPU-seconds over the 762-second experiment, indicating extremely low average utilization (~0.003 cores average). The alloy log shipper was the most CPU-intensive logging component at 0.607 cumulative CPU-seconds, followed by promtail at 0.382 CPU-seconds. The loki-gateway and loki-canary consumed 0.018 and 0.063 CPU-seconds respectively, showing minimal overhead for routing and validation. The validation workflow completed successfully in approximately 10 seconds. Total estimated infrastructure cost was $0.0042 for the ~12.7-minute run, extrapolating to roughly $0.02/hour or $14.40/month for continuous operation of this single-node Loki stack on GKE.",
      "discussion": "The measured Loki resource footprint validates the hypothesis that label-only indexing produces a lightweight logging stack. A total pipeline memory footprint under 90 MB (excluding infrastructure pods) and negligible CPU utilization on an e2-medium node demonstrates that Loki is viable for small-to-medium Kubernetes clusters where logging should not compete with application workloads for resources. The extrapolated cost of ~$14.40/month for the compute node alone (before storage costs) positions Loki as highly cost-effective for teams with constrained budgets. However, several important caveats apply. First, this experiment did not deploy Elasticsearch, so the resource comparison is asymmetric — Loki's measured values are compared against Elasticsearch's documented characteristics rather than empirical data from the same environment. A production Elasticsearch deployment on equivalent hardware would likely not fit within an e2-medium node at all, as the JVM heap alone typically requires 1–2 GB, but this was not verified experimentally. Second, the observation window of ~12.7 minutes with a synthetic log generator does not capture steady-state behavior under sustained production log volumes, index compaction pressure, or query-time resource spikes. Third, the experiment did not measure query latency or throughput, which is where Elasticsearch's full inverted index provides its primary advantage. Loki's brute-force scanning of compressed chunks during queries can become a significant bottleneck at scale, a tradeoff invisible in this ingestion-focused benchmark. The low CPU figures also warrant careful interpretation: the cumulative CPU-seconds metric divided by wall-clock time shows average utilization, but does not reveal burst patterns during log ingestion spikes. For real-world decision-making, Loki is clearly the appropriate choice for resource-constrained environments where log queries are infrequent and label-based filtering suffices. Elasticsearch remains necessary when full-text search, complex aggregations, or sub-second query latency across large corpora are required — but at a 5–10x resource cost that this experiment's single-node setup could not accommodate."
    },
    "feedback": {
      "recommendations": [
        "Deploy Elasticsearch on a separate target node (e2-standard-2 or larger) in the next iteration to produce a true side-by-side resource comparison with measured data for both stacks",
        "Add query-time benchmarks: execute a standard set of log queries (label filter, regex match, aggregation, full-text search) against both systems and measure latency and resource consumption during query execution",
        "Increase the observation window to at least 1 hour with sustained log generation at a defined rate (e.g., 1,000 lines/sec) to capture steady-state behavior, compaction cycles, and memory growth patterns",
        "Collect Prometheus rate() metrics for CPU rather than raw cumulative counters to distinguish average vs. peak utilization and identify burst behavior during ingestion"
      ],
      "experimentDesign": [
        "Run the experiment as a sequential A/B test on identical hardware: deploy Loki, generate load, collect metrics, tear down, then deploy Elasticsearch with the same load profile — ensuring both stacks process identical log volume on equivalent infrastructure",
        "Add storage metrics (disk I/O, bytes written, index size on disk) to quantify the storage efficiency difference between label-only indexing and full inverted indexing under identical workloads",
        "Include a defined query workload phase after ingestion completes, measuring query latency percentiles (p50, p95, p99) for common operational queries to benchmark the read-path tradeoff alongside the write-path resource comparison"
      ]
    },
    "summary": "Insufficient data to evaluate the hypothesis. The experiment deployed only the Loki logging stack on a single GKE target cluster; no Elasticsearch deployment was observed in the collected metrics, making a direct resource or query-capability comparison impossible. The Loki stack (alloy, promtail, loki-gateway, loki-canary, fluent-bit) consumed approximately 161.5 MB of memory and modest CPU across all components on an e2-medium node (2 vCPUs, 4 GB RAM), representing roughly 3.9% of available memory — confirming Loki's lightweight footprint. However, without a parallel Elasticsearch deployment collecting the same log volume, neither the resource-efficiency comparison nor the query-capability comparison can be substantiated. To properly evaluate the hypothesis, a second target cluster running an equivalent Elasticsearch stack (Elasticsearch + Filebeat/Fluentd + Kibana) ingesting identical log-generator output is required, with metrics collected over the same time window.",
    "recommendations": [
      "Deploy Elasticsearch on a separate target node (e2-standard-2 or larger) in the next iteration to produce a true side-by-side resource comparison with measured data for both stacks",
      "Add query-time benchmarks: execute a standard set of log queries (label filter, regex match, aggregation, full-text search) against both systems and measure latency and resource consumption during query execution",
      "Increase the observation window to at least 1 hour with sustained log generation at a defined rate (e.g., 1,000 lines/sec) to capture steady-state behavior, compaction cycles, and memory growth patterns",
      "Collect Prometheus rate() metrics for CPU rather than raw cumulative counters to distinguish average vs. peak utilization and identify burst behavior during ingestion"
    ],
    "generatedAt": "2026-02-11T19:39:11Z",
    "model": "claude-opus-4-6"
  }
}
