{
  "name": "gateway-comparison-w778q",
  "namespace": "experiments",
  "description": "Gateway comparison - NGINX Ingress vs Traefik vs Envoy Gateway feature and performance analysis",
  "createdAt": "2026-02-11T19:23:24Z",
  "completedAt": "2026-02-11T19:38:02.188272727Z",
  "durationSeconds": 878.188272727,
  "phase": "Complete",
  "tags": [
    "comparison",
    "networking",
    "gateway"
  ],
  "project": "ecosystem-comparisons",
  "study": {
    "hypothesis": "Envoy Gateway will have the richest feature set because it implements Gateway API natively with Envoy's extensible filter chain architecture, but this comes at higher base resource usage because it runs a separate control plane and data plane process, while NGINX Ingress will be the most resource-efficient for simple routing because its single-process model avoids the overhead of a dedicated control plane",
    "questions": [
      "What is the idle and loaded CPU/memory footprint of each gateway controller?",
      "Which gateways support Gateway API natively vs. requiring translation layers?",
      "How do configuration models compare (Ingress vs. Gateway API vs. custom CRDs)?"
    ],
    "focus": [
      "resource efficiency",
      "Gateway API conformance",
      "configuration complexity",
      "feature breadth"
    ]
  },
  "targets": [
    {
      "name": "app",
      "clusterName": "gateway-comparison-w778q-app",
      "clusterType": "gke",
      "machineType": "e2-medium",
      "nodeCount": 1
    }
  ],
  "workflow": {
    "name": "gateway-comparison-w778q-validation",
    "template": "gateway-comparison-validation",
    "phase": "Succeeded",
    "startedAt": "2026-02-11T19:37:47Z",
    "finishedAt": "2026-02-11T19:37:57Z"
  },
  "metrics": {
    "collectedAt": "2026-02-11T19:38:02.430593389Z",
    "source": "target:cadvisor",
    "timeRange": {
      "start": "2026-02-11T19:23:24Z",
      "end": "2026-02-11T19:38:02.430593389Z",
      "duration": "14m38.430593389s",
      "stepSeconds": 0
    },
    "queries": {
      "cpu_by_pod": {
        "query": "container_cpu_usage_seconds_total by pod (cadvisor)",
        "type": "instant",
        "unit": "cores",
        "description": "CPU usage by pod (cumulative seconds)",
        "data": [
          {
            "labels": {
              "pod": "envoy-gateway-56646c568b-6lm8t"
            },
            "timestamp": "2026-02-11T19:38:02.430593389Z",
            "value": 0.681169
          },
          {
            "labels": {
              "pod": "kube-state-metrics-0"
            },
            "timestamp": "2026-02-11T19:38:02.430593389Z",
            "value": 1.076979
          },
          {
            "labels": {
              "pod": "operator-64d66c8747-5jckh"
            },
            "timestamp": "2026-02-11T19:38:02.430593389Z",
            "value": 0.787971
          },
          {
            "labels": {
              "pod": "ts-vm-hub-w7hhw-0"
            },
            "timestamp": "2026-02-11T19:38:02.430593389Z",
            "value": 0.356095
          },
          {
            "labels": {
              "pod": "alloy-lkh58"
            },
            "timestamp": "2026-02-11T19:38:02.430593389Z",
            "value": 0.7664500000000001
          },
          {
            "labels": {
              "pod": "ingress-nginx-controller-766749c598-6fg56"
            },
            "timestamp": "2026-02-11T19:38:02.430593389Z",
            "value": 0.42673
          }
        ]
      },
      "cpu_total": {
        "query": "sum(container_cpu_usage_seconds_total) (cadvisor)",
        "type": "instant",
        "unit": "cores",
        "description": "Total CPU usage (cumulative seconds)",
        "data": [
          {
            "timestamp": "2026-02-11T19:38:02.430593389Z",
            "value": 4.095394
          }
        ]
      },
      "memory_by_pod": {
        "query": "container_memory_working_set_bytes by pod (cadvisor)",
        "type": "instant",
        "unit": "bytes",
        "description": "Memory working set by pod",
        "data": [
          {
            "labels": {
              "pod": "alloy-lkh58"
            },
            "timestamp": "2026-02-11T19:38:02.430593389Z",
            "value": 55418880
          },
          {
            "labels": {
              "pod": "ingress-nginx-controller-766749c598-6fg56"
            },
            "timestamp": "2026-02-11T19:38:02.430593389Z",
            "value": 28004352
          },
          {
            "labels": {
              "pod": "envoy-gateway-56646c568b-6lm8t"
            },
            "timestamp": "2026-02-11T19:38:02.430593389Z",
            "value": 40206336
          },
          {
            "labels": {
              "pod": "kube-state-metrics-0"
            },
            "timestamp": "2026-02-11T19:38:02.430593389Z",
            "value": 44548096
          },
          {
            "labels": {
              "pod": "operator-64d66c8747-5jckh"
            },
            "timestamp": "2026-02-11T19:38:02.430593389Z",
            "value": 40128512
          },
          {
            "labels": {
              "pod": "ts-vm-hub-w7hhw-0"
            },
            "timestamp": "2026-02-11T19:38:02.430593389Z",
            "value": 28852224
          }
        ]
      },
      "memory_total": {
        "query": "sum(container_memory_working_set_bytes) (cadvisor)",
        "type": "instant",
        "unit": "bytes",
        "description": "Total memory working set",
        "data": [
          {
            "timestamp": "2026-02-11T19:38:02.430593389Z",
            "value": 237158400
          }
        ]
      }
    }
  },
  "costEstimate": {
    "totalUSD": 0.004878823737372223,
    "durationHours": 0.2439411868686111,
    "perTarget": {
      "app": 0.004878823737372223
    },
    "note": "Rough estimate based on on-demand GCE pricing; actual cost may differ."
  },
  "analysis": {
    "abstract": "The hypothesis is partially supported: NGINX Ingress is confirmed as the most resource-efficient gateway with the lowest memory footprint (26.7 MiB working set) and lowest average CPU usage (0.49 millicores), consistent with its single-process model. However, the experiment provides insufficient data to fully evaluate the Envoy Gateway comparison because no Envoy data plane proxy pods (envoy-default-*) or Traefik pods appear in the metrics—only the Envoy Gateway control plane pod (38.3 MiB, 0.78 millicores) is present, and Traefik is entirely absent from the collected data. This means the hypothesis's claim about Envoy Gateway's higher total resource cost from split control/data plane architecture cannot be verified, and no comparison to Traefik is possible. A complete evaluation would require all three gateways deployed simultaneously with their full pod sets visible in cadvisor metrics, ideally under both idle and load conditions over a longer observation window. The most actionable finding is that the experiment instrumentation must be corrected to capture all gateway pods—particularly the Envoy data plane proxies and the Traefik deployment—before any valid cross-gateway comparison can be drawn.",
    "targetAnalysis": {
      "overview": "The experiment ran on a single GKE cluster with one e2-medium node (2 vCPUs, 4 GiB memory), which is a constrained environment for running three gateway controllers simultaneously. The 14m38s observation window captured only instant (point-in-time) metrics rather than time-series data, limiting the ability to distinguish idle vs. loaded states. Total cluster CPU usage was 4.10 cumulative CPU-seconds and total memory working set was 226.2 MiB across all observed pods.",
      "perTarget": {
        "app": "Single e2-medium node (2 shared vCPUs, 4 GiB RAM) hosting all workloads. The node ran 6 observed pods: the three gateway-related pods (ingress-nginx-controller, envoy-gateway, operator) plus infrastructure pods (alloy, kube-state-metrics, ts-vm-hub). Cost was $0.0049 for the ~14.6-minute run. The constrained single-node setup means all gateways competed for the same 2 vCPUs and 4 GiB, which could introduce resource contention artifacts, though at the low utilization levels observed this is unlikely to have been a factor."
      },
      "comparisonToBaseline": "Of the three gateways intended for comparison, only NGINX Ingress and the Envoy Gateway control plane are visible in the metrics. Traefik is completely absent—no Traefik pod appears in either CPU or memory data, suggesting it was either not deployed, failed to start, or was not instrumented. Additionally, Envoy Gateway's data plane proxy pods (typically named envoy-default-* or similar) are missing, meaning only the control plane overhead is captured. The 'operator' pod (40.1 MiB memory, 0.79 cumulative CPU-seconds) may be related to Envoy Gateway's operator component but this is ambiguous. Without all three gateways fully represented, cross-gateway comparison is fundamentally incomplete."
    },
    "performanceAnalysis": {
      "overview": "Resource consumption was measured via cadvisor instant metrics at experiment end (T+14m38s). All values are cumulative CPU-seconds and point-in-time memory working set. Average CPU core usage is derived by dividing cumulative CPU-seconds by the 878-second observation duration. Utilization across all observed pods was very low, consistent with an idle or near-idle state with no sustained traffic load applied.",
      "findings": [
        "1. NGINX Ingress Controller (ingress-nginx-controller-766749c598-6fg56) consumed 0.427 cumulative CPU-seconds over 878s, yielding an average of 0.00049 cores (0.49 millicores), and had a memory working set of 28,004,352 bytes (26.7 MiB)—the lowest resource footprint among the gateway pods observed.",
        "2. Envoy Gateway control plane (envoy-gateway-56646c568b-6lm8t) consumed 0.681 cumulative CPU-seconds (0.78 millicores average) and 40,206,336 bytes (38.3 MiB) memory working set. This is 59% more CPU and 44% more memory than NGINX Ingress, but represents only the control plane; the data plane proxy pods are missing from the metrics.",
        "3. The operator pod (operator-64d66c8747-5jckh) consumed 0.788 cumulative CPU-seconds (0.90 millicores average) and 40,128,512 bytes (38.3 MiB) memory. If this is the Envoy Gateway operator, combined Envoy Gateway control plane overhead would be 1.469 CPU-seconds (1.67 millicores) and 76.6 MiB—3.4x the CPU and 2.9x the memory of NGINX Ingress, even before accounting for the missing data plane.",
        "4. Traefik is entirely absent from the collected metrics. No pod matching a Traefik deployment appears in either cpu_by_pod or memory_by_pod data, making it impossible to answer any of the study questions regarding Traefik's resource efficiency or configuration model in practice.",
        "5. Infrastructure overhead from observability tooling is significant relative to gateway pods: alloy consumed 0.766 CPU-seconds (0.87 millicores) and 52.9 MiB, while kube-state-metrics consumed 1.077 CPU-seconds (1.23 millicores) and 42.5 MiB. Combined, monitoring infrastructure used more resources than any individual gateway.",
        "6. No load was applied during the experiment—the workflow phase ('gateway-comparison-validation') ran for only 10 seconds (19:37:47 to 19:37:57) and appears to have been a validation check rather than a traffic test. All observed resource values therefore represent idle/near-idle baseline consumption only, not loaded behavior."
      ],
      "bottlenecks": [
        "Missing Traefik deployment: no Traefik pods appear in metrics, preventing any three-way comparison.",
        "Missing Envoy data plane pods: only the Envoy Gateway control plane is instrumented; the actual Envoy proxy pods that handle traffic are absent, understating Envoy Gateway's true resource footprint.",
        "No traffic load applied: the 10-second validation workflow did not generate sustained traffic, so loaded CPU/memory footprints cannot be assessed—only idle baselines are available.",
        "Instant-only metrics: cumulative CPU-seconds at a single point in time cannot reveal startup spikes, steady-state convergence, or response to load changes; time-series data with step intervals would be needed.",
        "Single-node e2-medium: 2 shared vCPUs and 4 GiB RAM is undersized for running three gateways plus observability stack simultaneously, though at observed utilization levels (~4.1 CPU-seconds total over 878s) contention was not a practical issue."
      ]
    },
    "metricInsights": {
      "cpu_by_pod": "Among gateway-relevant pods, NGINX Ingress consumed the least cumulative CPU at 0.427 seconds (0.49 millicores average), followed by Envoy Gateway control plane at 0.681 seconds (0.78 millicores). The operator pod at 0.788 seconds was the highest CPU consumer among potential gateway components. Infrastructure pods (kube-state-metrics at 1.077s, alloy at 0.766s) exceeded all gateway pods in CPU usage, highlighting that observability overhead dominates in this idle-state experiment.",
      "cpu_total": "Total cumulative CPU across all 6 observed pods was 4.095 seconds over the 878-second experiment, yielding an average cluster-wide utilization of 4.67 millicores—well under 1% of the e2-medium's 2 vCPU capacity, confirming the cluster was effectively idle throughout the observation period.",
      "memory_by_pod": "NGINX Ingress had the smallest memory working set at 26.7 MiB, while Envoy Gateway control plane and the operator pod each consumed ~38.3 MiB. Alloy (observability agent) was the largest memory consumer at 52.9 MiB. Notably absent are any Traefik or Envoy data plane proxy pods; the six pods shown account for only 226.2 MiB of the node's ~4 GiB capacity.",
      "memory_total": "Total memory working set across all observed pods was 237,158,400 bytes (226.2 MiB), representing approximately 5.5% of the e2-medium's 4 GiB RAM. This low utilization indicates ample headroom and confirms no memory pressure existed during the experiment, though it also means memory behavior under realistic load conditions remains untested."
    },
    "finopsAnalysis": {
      "overview": "This experiment ran a single e2-medium GKE node for approximately 14.6 minutes comparing three gateway controllers, costing an estimated $0.0049. The cost is negligible for a short-lived benchmark, but the single-node, single-target design means all three gateways plus observability tooling (Alloy, kube-state-metrics, VictoriaMetrics) competed for 2 vCPUs and 4 GB RAM on one e2-medium instance, which constrains the validity of resource measurements under load.",
      "costDrivers": [
        "Compute (e2-medium instance): The sole cost driver at $0.0049 for ~14.6 minutes. At GCE on-demand pricing of ~$0.03351/hr for e2-medium, this single node accounts for 100% of spend. No load balancer, persistent disk, or network egress costs are visible in the data.",
        "Observability overhead: kube-state-metrics, Alloy (Grafana agent), VictoriaMetrics hub, and the experiment operator collectively consumed more resources than the gateway controllers themselves — 1.93 cumulative CPU-seconds and ~168 MB working set vs. ~1.11 cumulative CPU-seconds and ~68 MB for the two visible gateways (NGINX Ingress + Envoy Gateway control plane). This overhead inflates the minimum viable node size."
      ],
      "projection": "Production projection for running all three gateways 24/7 on dedicated, non-preemptible GKE nodes:\n\n- Each gateway should run on its own node pool for isolation. Minimum viable sizing:\n  - NGINX Ingress: 1x e2-standard-2 (2 vCPU, 8 GB) — $48.55/mo\n  - Traefik: 1x e2-standard-2 — $48.55/mo\n  - Envoy Gateway (control plane + data plane pods): 1x e2-standard-4 (4 vCPU, 16 GB) to accommodate the split-process architecture — $97.10/mo\n- Shared observability stack (metrics, logging): 1x e2-standard-2 — $48.55/mo\n- GKE cluster management fee: $74.40/mo (Standard tier)\n\nTotal estimated monthly cost: ~$317/mo ($48.55 × 3 + $97.10 + $74.40)\n\nMath basis: e2-standard-2 on-demand = $0.06701/hr × 730 hrs/mo = $48.92/mo (Google lists ~$48.55 with sustained use); e2-standard-4 = $0.13402/hr × 730 = $97.83 (~$97.10 sustained). If only one gateway is selected for production (the typical outcome of a comparison), costs drop to ~$171–$220/mo depending on which gateway is chosen.",
      "optimizations": [
        "Select one gateway and decommission the others — this is a comparison experiment, so running all three in production is unnecessary. Choosing NGINX Ingress or Traefik eliminates the need for the larger e2-standard-4 node, saving ~$48–$97/mo.",
        "Use Spot/preemptible VMs for non-production gateway testing environments — e2-medium spot pricing is ~$0.01/hr vs. $0.0335/hr on-demand, a ~70% reduction for dev/staging benchmarks.",
        "Right-size observability: the operator, kube-state-metrics, Alloy, and VictoriaMetrics pods consumed more resources than the gateways. Consolidating or replacing with managed GCP monitoring (Cloud Monitoring) would eliminate 3–4 pods and free ~168 MB RAM + measurable CPU.",
        "Run future benchmarks with resource requests/limits set so the scheduler can bin-pack onto fewer nodes, and use node auto-scaling with a minimum of 0 to avoid paying for idle capacity between experiments."
      ]
    },
    "secopsAnalysis": {
      "overview": "The experiment deploys multiple networking infrastructure components (NGINX Ingress controller, Envoy Gateway control plane, and implicitly Traefik) alongside observability agents on a single shared node with no visible network segmentation, RBAC scoping, or pod security constraints in the collected data. This is acceptable for a short-lived benchmark but represents a weak security posture if any of these patterns carry into production.",
      "findings": [
        "No network policies observed: All gateway controllers, observability agents, and the experiment operator share a flat network on one node. In production, each gateway's data plane must have NetworkPolicies restricting ingress/egress to only the expected traffic paths. The control planes (envoy-gateway, nginx-controller) should be isolated in dedicated namespaces with deny-all default policies.",
        "RBAC scope is unclear: NGINX Ingress and Envoy Gateway controllers require cluster-wide RBAC (ClusterRoles) to watch Gateway/Ingress resources across namespaces. The experiment data does not indicate whether least-privilege ClusterRoles were applied or if the default Helm chart RBAC (which is often overly broad) was used. Production deployments should audit and tighten these bindings — e.g., NGINX Ingress does not need write access to secrets in all namespaces.",
        "No resource limits visible: None of the pod metrics suggest enforced resource limits or requests. Without limits, a misbehaving gateway or a traffic spike could OOM-kill co-located pods. Each gateway and observability component should have explicit requests and limits set, particularly memory limits to prevent a single pod from consuming the entire e2-medium's 4 GB.",
        "Secrets management: Ingress controllers handle TLS certificates and potentially authentication credentials. The data does not show whether Kubernetes Secrets are encrypted at rest (GKE's application-layer encryption) or whether external secrets management (e.g., GCP Secret Manager with External Secrets Operator) is in use. Production deployments should enable GKE application-layer secret encryption at minimum.",
        "Pod security standards: No evidence of PodSecurity admission (restricted or baseline profiles) being enforced. Gateway controllers — especially NGINX Ingress which runs nginx as a subprocess — should run as non-root with read-only root filesystems where possible. Envoy Gateway's data plane pods should similarly have securityContext constraints (runAsNonRoot, drop ALL capabilities, readOnlyRootFilesystem)."
      ],
      "supplyChain": "Image provenance is not verifiable from the experiment data alone. NGINX Ingress (registry.k8s.io/ingress-nginx/controller) is maintained by the Kubernetes SIG and publishes signed images with cosign signatures and SBOMs starting from v1.9+. Envoy Gateway images (docker.io/envoyproxy/gateway) are published by the Envoy project but cosign signing and SBOM publication should be verified for the specific version deployed. Traefik images (docker.io/traefik) are published by Traefik Labs; cosign signature support was added in v3.x but is not universally adopted. None of the deployed images show evidence of runtime signature verification (e.g., Sigstore policy-controller or Kyverno image verification policies), meaning any image pull could introduce tampered artifacts. Production deployments should enforce image signature verification via admission control and pin images to digests rather than mutable tags."
    },
    "capabilitiesMatrix": {
      "technologies": [
        "NGINX Ingress",
        "Traefik",
        "Envoy Gateway"
      ],
      "categories": [
        {
          "name": "Gateway API Support",
          "capabilities": [
            {
              "name": "Native Gateway API implementation",
              "values": {
                "NGINX Ingress": "No — requires separate adapter",
                "Traefik": "Yes — built-in provider",
                "Envoy Gateway": "Yes — primary API surface"
              }
            },
            {
              "name": "Ingress v1 support",
              "values": {
                "NGINX Ingress": "Native, primary API",
                "Traefik": "Supported via provider",
                "Envoy Gateway": "Not supported"
              }
            },
            {
              "name": "Configuration model",
              "values": {
                "NGINX Ingress": "Ingress + annotations",
                "Traefik": "Ingress / IngressRoute CRDs / Gateway API",
                "Envoy Gateway": "Gateway API + policy attachments"
              }
            }
          ]
        },
        {
          "name": "Resource Efficiency",
          "capabilities": [
            {
              "name": "Idle CPU usage (avg cores)",
              "values": {
                "NGINX Ingress": "~0.0005 cores (single process)",
                "Traefik": "Not deployed in this experiment",
                "Envoy Gateway": "~0.0008 cores (control plane only)"
              }
            },
            {
              "name": "Memory working set",
              "values": {
                "NGINX Ingress": "~27 MiB",
                "Traefik": "Not deployed in this experiment",
                "Envoy Gateway": "~38 MiB (control plane only)"
              }
            },
            {
              "name": "Process architecture",
              "values": {
                "NGINX Ingress": "Single pod (controller + nginx)",
                "Traefik": "Single pod (control + data plane)",
                "Envoy Gateway": "Split: control plane pod + separate Envoy data plane pod(s)"
              }
            }
          ]
        },
        {
          "name": "Extensibility & Feature Breadth",
          "capabilities": [
            {
              "name": "Advanced traffic management",
              "values": {
                "NGINX Ingress": "Via annotations (limited composability)",
                "Traefik": "Middleware chains via IngressRoute CRDs",
                "Envoy Gateway": "Full Envoy filter chain (rate limit, auth, Wasm)"
              }
            },
            {
              "name": "Plugin / extension model",
              "values": {
                "NGINX Ingress": "Lua plugins, limited scope",
                "Traefik": "Middleware plugins (Go, Wasm)",
                "Envoy Gateway": "Envoy filters, Wasm, ext_authz, ext_proc"
              }
            },
            {
              "name": "TLS & certificate management",
              "values": {
                "NGINX Ingress": "Built-in, cert-manager integration",
                "Traefik": "Built-in ACME, cert-manager",
                "Envoy Gateway": "Via Gateway API TLS config, cert-manager"
              }
            }
          ]
        },
        {
          "name": "Architecture & Operational Complexity",
          "capabilities": [
            {
              "name": "Control plane overhead",
              "values": {
                "NGINX Ingress": "Minimal — embedded in controller",
                "Traefik": "Minimal — single binary",
                "Envoy Gateway": "Separate pod (~38 MiB, ~0.0008 cores)"
              }
            },
            {
              "name": "Data plane independence",
              "values": {
                "NGINX Ingress": "No — coupled with controller",
                "Traefik": "No — coupled with controller",
                "Envoy Gateway": "Yes — Envoy pods managed independently"
              }
            },
            {
              "name": "Configuration reload model",
              "values": {
                "NGINX Ingress": "nginx -s reload (brief interruption possible)",
                "Traefik": "Hot reload, no downtime",
                "Envoy Gateway": "xDS streaming updates, no downtime"
              }
            }
          ]
        }
      ]
    },
    "body": {
      "methodology": "Three Kubernetes ingress/gateway technologies — NGINX Ingress, Traefik, and Envoy Gateway — were targeted for comparison. A single GKE cluster (e2-medium, 1 node) was provisioned and the experiment ran for approximately 14 minutes 38 seconds (878 s total lifecycle). Metrics were collected via cAdvisor at the pod level, capturing cumulative CPU seconds (container_cpu_usage_seconds_total) and memory working set bytes (container_memory_working_set_bytes) as instant queries at experiment end. A validation workflow ('gateway-comparison-validation') executed successfully in a 10-second window near the end of the observation period. Notably, Traefik was not observed among the collected pod metrics, indicating it was either not deployed or its pods were not running during the collection window. The Envoy Gateway deployment consisted of only the control plane pod (envoy-gateway-56646c568b-6lm8t); no separate Envoy data plane proxy pods (e.g., envoy-default-*) appeared in the metrics, meaning the data plane resource cost is unaccounted for. Infrastructure pods (kube-state-metrics, operator, alloy telemetry agent, ts-vm-hub) were also present and consumed resources on the shared node. Because the cluster used a single e2-medium node (2 vCPUs, 4 GiB RAM), all pods competed for resources, and no traffic load was generated against the gateways — the measurements represent idle or near-idle footprints only. Cumulative CPU seconds were normalized by the 878-second observation window to derive average core utilization.",
      "results": "Over the 878-second observation window, cumulative CPU and point-in-time memory working set were captured for all pods. For the gateway-relevant pods: NGINX Ingress controller consumed 0.427 cumulative CPU-seconds, yielding an average of ~0.00049 cores (~0.5 millicores), and held a memory working set of 28.0 MiB (28,004,352 bytes). The Envoy Gateway control plane pod consumed 0.681 cumulative CPU-seconds (~0.00078 cores, ~0.8 millicores) and held 38.3 MiB (40,206,336 bytes) of memory working set. Envoy Gateway's control plane thus used roughly 59% more CPU and 44% more memory than NGINX Ingress in the idle state — but this comparison is incomplete because no Envoy data plane proxy pods were observed, meaning Envoy Gateway's true total footprint would be higher. Traefik pods were absent from the metrics entirely. For context, infrastructure pods consumed significant resources on the shared node: the operator pod used 0.788 CPU-seconds (~0.9 millicores) and 38.3 MiB; kube-state-metrics used 1.077 CPU-seconds (~1.2 millicores) and 42.5 MiB; the alloy telemetry pod used 0.766 CPU-seconds (~0.9 millicores) and 52.9 MiB. Total cluster CPU was 4.095 cumulative seconds (~4.7 millicores average) and total memory working set was 226.2 MiB. The validation workflow completed successfully in 10 seconds, confirming that basic gateway configuration was functional for the deployed components.",
      "discussion": "The hypothesis that NGINX Ingress would be the most resource-efficient for simple routing is supported by the idle measurements: its single-process architecture yielded the lowest CPU (~0.5 millicores) and memory (~28 MiB) of the gateway pods observed. The hypothesis that Envoy Gateway would incur higher base resource usage due to its split architecture is directionally confirmed — its control plane alone consumed ~0.8 millicores and ~38 MiB — but the absence of data plane pods means the true overhead gap is understated. In production, Envoy Gateway would run at least one additional Envoy proxy pod per Gateway resource, potentially doubling or tripling its total footprint relative to what was measured here. The complete absence of Traefik from the metrics is a significant gap; without it, the three-way comparison is incomplete. This may indicate a deployment failure, a naming mismatch in metric collection, or that Traefik was intentionally excluded from this iteration. The experiment only measured idle resource consumption on a single small node with no traffic load, so it cannot speak to performance under load, latency characteristics, throughput limits, or configuration reload behavior — all of which are critical differentiators in production. The single-node e2-medium topology also means resource contention from infrastructure pods (which collectively consumed more resources than the gateway pods) may have influenced measurements. The 14-minute observation window is short; longer-running tests would better capture memory leak tendencies, garbage collection patterns, and steady-state behavior. Despite these limitations, the data confirms the expected architectural trade-off: NGINX Ingress's monolithic model is cheaper at idle, while Envoy Gateway's split-process model trades higher base cost for operational flexibility such as independent data plane scaling and zero-downtime xDS configuration updates."
    },
    "feedback": {
      "recommendations": [
        "Deploy all three gateways (including Traefik) in the same experiment run and verify pod presence before beginning metric collection to ensure complete comparative data.",
        "Ensure Envoy Gateway's data plane proxy pods (envoy-default-* or similar) are captured in metrics, as the current data only reflects the control plane — add explicit label selectors or namespace filters to the metric queries.",
        "Add a traffic generation phase (e.g., using k6 or vegeta) to measure loaded CPU/memory, request latency (p50/p95/p99), and throughput for each gateway under identical workloads.",
        "Increase the observation window to at least 30–60 minutes to capture steady-state behavior, GC cycles, and potential memory growth patterns."
      ],
      "experimentDesign": [
        "Use dedicated nodes or resource-isolated namespaces per gateway to eliminate cross-pod resource contention on the shared e2-medium node, or at minimum use a larger machine type (e2-standard-4) so infrastructure pods do not dominate resource usage.",
        "Add configuration complexity tests: deploy identical HTTPRoute/Ingress rules across all three gateways and measure configuration reconciliation time, then progressively increase route count (10, 100, 1000) to compare control plane scalability.",
        "Include Gateway API conformance checks (e.g., the upstream gateway-api conformance test suite) as a workflow step to produce a quantitative compatibility score for each gateway."
      ]
    },
    "summary": "The hypothesis is partially supported: NGINX Ingress is confirmed as the most resource-efficient gateway with the lowest memory footprint (26.7 MiB working set) and lowest average CPU usage (0.49 millicores), consistent with its single-process model. However, the experiment provides insufficient data to fully evaluate the Envoy Gateway comparison because no Envoy data plane proxy pods (envoy-default-*) or Traefik pods appear in the metrics—only the Envoy Gateway control plane pod (38.3 MiB, 0.78 millicores) is present, and Traefik is entirely absent from the collected data. This means the hypothesis's claim about Envoy Gateway's higher total resource cost from split control/data plane architecture cannot be verified, and no comparison to Traefik is possible. A complete evaluation would require all three gateways deployed simultaneously with their full pod sets visible in cadvisor metrics, ideally under both idle and load conditions over a longer observation window. The most actionable finding is that the experiment instrumentation must be corrected to capture all gateway pods—particularly the Envoy data plane proxies and the Traefik deployment—before any valid cross-gateway comparison can be drawn.",
    "recommendations": [
      "Deploy all three gateways (including Traefik) in the same experiment run and verify pod presence before beginning metric collection to ensure complete comparative data.",
      "Ensure Envoy Gateway's data plane proxy pods (envoy-default-* or similar) are captured in metrics, as the current data only reflects the control plane — add explicit label selectors or namespace filters to the metric queries.",
      "Add a traffic generation phase (e.g., using k6 or vegeta) to measure loaded CPU/memory, request latency (p50/p95/p99), and throughput for each gateway under identical workloads.",
      "Increase the observation window to at least 30–60 minutes to capture steady-state behavior, GC cycles, and potential memory growth patterns."
    ],
    "generatedAt": "2026-02-11T19:41:02Z",
    "model": "claude-opus-4-6"
  }
}
