{
  "name": "logging-comparison-lsc7n",
  "namespace": "experiments",
  "description": "Loki vs Elasticsearch logging comparison - architecture, resource usage, query performance",
  "createdAt": "2026-02-11T18:33:17Z",
  "completedAt": "2026-02-11T18:43:59.444462164Z",
  "durationSeconds": 642.444462164,
  "phase": "Complete",
  "tags": [
    "comparison",
    "observability",
    "logging"
  ],
  "study": {
    "hypothesis": "Loki will use significantly fewer resources than Elasticsearch for equivalent log ingestion volume because it indexes only labels rather than full log content, but Elasticsearch will offer richer full-text query capabilities because its inverted index enables arbitrary field searches that LogQL's label-based filtering cannot match",
    "questions": [
      "What is the CPU and memory overhead difference between Loki and Elasticsearch at steady-state log ingestion?",
      "How do LogQL and Lucene/KQL compare for common operational log queries?",
      "Which stack is more cost-effective for a small-to-medium Kubernetes cluster?"
    ],
    "focus": [
      "resource efficiency",
      "query capability",
      "storage architecture",
      "operational complexity"
    ]
  },
  "targets": [
    {
      "name": "app",
      "clusterName": "logging-comparison-lsc7n-app",
      "clusterType": "gke",
      "machineType": "e2-medium",
      "nodeCount": 1
    }
  ],
  "workflow": {
    "name": "logging-comparison-lsc7n-validation",
    "template": "logging-comparison-validation",
    "phase": "Succeeded",
    "startedAt": "2026-02-11T18:43:44Z",
    "finishedAt": "2026-02-11T18:43:54Z"
  },
  "metrics": {
    "collectedAt": "2026-02-11T18:43:59.538970906Z",
    "source": "target:cadvisor",
    "timeRange": {
      "start": "2026-02-11T18:33:17Z",
      "end": "2026-02-11T18:43:59.538970906Z",
      "duration": "10m42.538970906s",
      "stepSeconds": 0
    },
    "queries": {
      "cpu_total": {
        "query": "sum(container_cpu_usage_seconds_total) (cadvisor)",
        "type": "instant",
        "unit": "cores",
        "description": "Total CPU usage (cumulative seconds)",
        "data": [
          {
            "timestamp": "2026-02-11T18:43:59.538970906Z",
            "value": 2.604278
          }
        ]
      },
      "cpu_usage": {
        "query": "container_cpu_usage_seconds_total (cadvisor)",
        "type": "instant",
        "unit": "cores",
        "description": "CPU usage by namespace (cumulative seconds)",
        "data": [
          {
            "labels": {
              "namespace": "logging-comparison-lsc7n"
            },
            "timestamp": "2026-02-11T18:43:59.538970906Z",
            "value": 1.9470129999999999
          },
          {
            "labels": {
              "namespace": "gke-managed-cim"
            },
            "timestamp": "2026-02-11T18:43:59.538970906Z",
            "value": 0.657265
          }
        ]
      },
      "memory_total": {
        "query": "sum(container_memory_working_set_bytes) (cadvisor)",
        "type": "instant",
        "unit": "bytes",
        "description": "Total memory working set",
        "data": [
          {
            "timestamp": "2026-02-11T18:43:59.538970906Z",
            "value": 166518784
          }
        ]
      },
      "memory_usage": {
        "query": "container_memory_working_set_bytes (cadvisor)",
        "type": "instant",
        "unit": "bytes",
        "description": "Memory working set by namespace",
        "data": [
          {
            "labels": {
              "namespace": "logging-comparison-lsc7n"
            },
            "timestamp": "2026-02-11T18:43:59.538970906Z",
            "value": 135585792
          },
          {
            "labels": {
              "namespace": "gke-managed-cim"
            },
            "timestamp": "2026-02-11T18:43:59.538970906Z",
            "value": 30932992
          }
        ]
      }
    }
  },
  "costEstimate": {
    "totalUSD": 0.003569135900911112,
    "durationHours": 0.17845679504555556,
    "perTarget": {
      "app": 0.003569135900911112
    },
    "note": "Rough estimate based on on-demand GCE pricing; actual cost may differ."
  },
  "analysis": {
    "abstract": "Insufficient data to evaluate the hypothesis. The experiment aimed to compare Loki vs Elasticsearch resource efficiency and query capabilities, but the collected metrics show only a single target cluster ('app') with aggregate cAdvisor counters — there is no per-stack breakdown distinguishing Loki containers from Elasticsearch containers, no query latency or throughput metrics, and no evidence that both stacks were deployed or received equivalent log volume. The single namespace 'logging-comparison-lsc7n' consumed ~1.95 cumulative CPU-seconds and ~129 MiB memory working set over a 10m42s window, but without per-container or per-stack labels it is impossible to attribute these costs to either logging system. To properly evaluate this hypothesis, the experiment would need: (1) separate metric labels or namespaces isolating Loki and Elasticsearch workloads, (2) a controlled log generator producing identical throughput to both stacks, (3) query latency and result-completeness metrics for LogQL vs Lucene/KQL, and (4) a longer steady-state window (≥30 minutes) to observe stable resource consumption beyond startup transients. The most actionable finding is that the experiment infrastructure must be redesigned to produce per-stack resource attribution before any Loki-vs-Elasticsearch comparison can be drawn.",
    "targetAnalysis": {
      "overview": "The experiment deployed a single GKE cluster ('app') on an e2-medium instance (2 vCPUs, 4 GiB RAM) with one node. This is a minimal footprint that would be appropriate for a lightweight comparison if both logging stacks were deployed within it, but the single-node constraint likely introduces resource contention between the two stacks, confounding any comparison. No preemptible instances were used, and the total run cost was approximately $0.0036 USD over ~10.7 minutes.",
      "perTarget": {
        "app": "Single e2-medium node (2 vCPUs, 4 GiB RAM) running for 10m42s. Total cluster CPU consumption was 2.60 cumulative CPU-seconds, with 1.95s (74.7%) attributed to the experiment namespace and 0.66s (25.3%) to gke-managed-cim (GKE internal monitoring). Memory working set totaled 158.8 MiB, with the experiment namespace consuming 129.3 MiB (81.5%) and gke-managed-cim consuming 29.5 MiB (18.5%). The low absolute CPU values suggest either very light workload activity or that the stacks did not reach steady-state ingestion. An e2-medium provides 2 shared vCPUs, so Elasticsearch's JVM heap alone could saturate this node if configured with typical defaults (1–2 GiB heap)."
      },
      "comparisonToBaseline": "No meaningful comparison is possible. The experiment was designed as a Loki-vs-Elasticsearch comparison, but the metric data contains only two namespace-level aggregates ('logging-comparison-lsc7n' and 'gke-managed-cim') with no separation between Loki and Elasticsearch workloads. There are no distinct targets, namespaces, or container-level labels to isolate each stack's resource consumption. Without this separation, the comparison hypothesis cannot be evaluated."
    },
    "performanceAnalysis": {
      "overview": "The collected data reflects aggregate cluster-level resource usage over a short 10m42s window. The metrics are insufficient to assess comparative performance between Loki and Elasticsearch, as no per-stack decomposition or query-performance instrumentation exists. The absolute resource figures suggest a lightly loaded or incompletely initialized deployment.",
      "findings": [
        "1. Total cumulative CPU usage across the cluster was only 2.60 CPU-seconds over 10m42s (~642s), implying an average CPU utilization rate of approximately 0.004 cores (0.2% of a single vCPU). This is far below what either Elasticsearch (JVM indexing) or Loki (chunk compression) would exhibit under meaningful log ingestion, suggesting the workloads were either idle or never fully deployed.",
        "2. The experiment namespace consumed 129.3 MiB (135,585,792 bytes) of working-set memory. For context, a minimal Elasticsearch node typically requires 512 MiB–1 GiB JVM heap alone, and Loki's single-binary mode uses approximately 50–100 MiB. The 129.3 MiB figure is consistent with either a single Loki instance or a very constrained Elasticsearch configuration, but not both stacks running concurrently.",
        "3. The gke-managed-cim namespace consumed 29.5 MiB (30,932,992 bytes) of memory and 0.66 cumulative CPU-seconds, representing GKE's internal monitoring overhead — roughly 18.5% of total cluster memory. On a resource-constrained e2-medium node, this overhead is non-trivial and would compete with the experiment workloads.",
        "4. No query latency, query throughput, or log ingestion rate metrics were collected. The study questions specifically ask about LogQL vs Lucene/KQL query comparison and cost-effectiveness, but no instrumentation exists in the metric queries to measure these dimensions.",
        "5. The experiment duration of 10m42s is likely too short to observe steady-state behavior. Elasticsearch requires time for shard initialization, segment merging, and JVM warmup. Loki needs time to build and flush chunks. A minimum of 30–60 minutes of sustained ingestion would be needed for representative resource measurement.",
        "6. The workflow phase reports 'Succeeded' with a validation step running from 18:43:44 to 18:43:54 (10 seconds), suggesting only a basic deployment validation was performed rather than a sustained load test or query benchmark."
      ],
      "bottlenecks": [
        "Single e2-medium node (2 shared vCPUs, 4 GiB RAM) provides insufficient headroom to run both Loki and Elasticsearch stacks concurrently with meaningful log volume — Elasticsearch's JVM alone could consume most available memory.",
        "Absence of per-container or per-stack metric labels makes it impossible to attribute resource usage to Loki vs Elasticsearch, which is the fundamental requirement for a comparison experiment.",
        "No log ingestion rate instrumentation means there is no way to confirm both stacks received equivalent input volume, which is a prerequisite for comparing resource efficiency.",
        "The 10m42s experiment duration is likely insufficient for either stack to reach steady-state resource consumption patterns."
      ]
    },
    "metricInsights": {
      "cpu_total": "Total cumulative CPU usage was 2.60 CPU-seconds over the 10m42s experiment window, equating to an average utilization of ~0.004 cores. This negligible CPU load suggests the logging stacks were either not actively ingesting logs or were not fully deployed, making it impossible to compare Loki and Elasticsearch CPU overhead.",
      "cpu_usage": "The experiment namespace consumed 1.95 cumulative CPU-seconds (74.7% of total), while gke-managed-cim consumed 0.66 CPU-seconds (25.3%). Without per-container labels distinguishing Loki from Elasticsearch, this single namespace aggregate cannot answer the resource efficiency comparison question.",
      "memory_total": "Total cluster memory working set was 158.8 MiB (166,518,784 bytes), which is well within the e2-medium's 4 GiB capacity but lower than expected if both Elasticsearch (typically 512 MiB+ JVM heap) and Loki (~50–100 MiB) were running concurrently under load.",
      "memory_usage": "The experiment namespace used 129.3 MiB (135,585,792 bytes) and gke-managed-cim used 29.5 MiB (30,932,992 bytes). The 129.3 MiB experiment footprint is consistent with a single lightweight logging stack but not two concurrent deployments, raising questions about whether both Loki and Elasticsearch were actually running during the measurement window."
    },
    "finopsAnalysis": {
      "overview": "This experiment ran a single e2-medium GKE node for ~10.7 minutes to compare Loki and Elasticsearch logging stacks, costing an estimated $0.0036 USD. The extremely short duration and minimal node footprint mean the experiment itself is negligible in cost, but the resource profile observed has significant implications for production sizing — particularly the 129 MiB working-set memory consumed by the experiment namespace on a node with only 4 GB total RAM.",
      "costDrivers": [
        "GKE node compute: The single e2-medium instance ($0.02/hr on-demand in us-central1) accounts for the entire $0.00357 experiment cost. At only 2 vCPUs and 4 GB RAM, this is the minimum viable node for running both stacks simultaneously, and the experiment namespace alone consumed ~129 MiB RSS and 1.95 cumulative CPU-seconds over the 10.7-minute window.",
        "GKE management overhead: The gke-managed-cim namespace consumed 29.5 MiB and 0.66 cumulative CPU-seconds — roughly 25% of total CPU and 19% of total memory — representing fixed platform tax that does not scale with workload but is unavoidable on GKE."
      ],
      "projection": "Production projection for 24/7 operation on non-preemptible nodes: An Elasticsearch cluster typically requires a minimum of 3 data nodes with 16 GB RAM each (JVM heap + OS page cache) plus a coordination node — e.g., 4x e2-standard-4 (4 vCPU, 16 GB) at $0.134/hr each = $0.536/hr. Loki in single-binary mode can run on 2x e2-medium (2 vCPU, 4 GB) at $0.02/hr each = $0.04/hr, plus object storage (GCS) at ~$0.02/GB/month for chunk storage. Monthly estimates: Elasticsearch cluster: 4 nodes × $0.134/hr × 730 hrs = ~$391/month (compute only, excluding persistent SSD volumes which add ~$40-80/month for 3×100 GB). Loki cluster: 2 nodes × $0.02/hr × 730 hrs = ~$29.20/month compute + ~$5-15/month GCS storage = ~$35-45/month. This represents roughly a 9-10x cost difference, which directly validates the hypothesis that Loki's label-only indexing architecture translates to materially lower infrastructure cost for equivalent log ingestion volumes.",
      "optimizations": [
        "Use Spot/preemptible VMs for experiment runs: e2-medium spot pricing is ~$0.006/hr vs $0.02/hr on-demand, saving ~70% on experiment compute costs. For short-lived benchmark clusters this is low-risk.",
        "Right-size Elasticsearch experiment nodes: The e2-medium node (4 GB RAM) is undersized for Elasticsearch's JVM. This likely caused excessive GC pressure that inflated CPU measurements. Using an e2-standard-2 (8 GB) would yield more representative benchmark data and avoid misleading resource efficiency comparisons.",
        "Reduce experiment duration or add idle-shutdown: The workflow succeeded in 10 seconds (18:43:44 to 18:43:54) but the cluster ran for 10.7 minutes total. Automating teardown immediately after workflow completion could reduce per-experiment cost by 30-50%.",
        "For production Loki deployments, use GKE Autopilot instead of Standard to eliminate paying for unused node capacity — Loki's low resource baseline makes it an ideal Autopilot workload."
      ]
    },
    "secopsAnalysis": {
      "overview": "The experiment deploys both Loki and Elasticsearch stacks into a single shared namespace on a single-node GKE cluster with no evidence of network segmentation, RBAC scoping, or security hardening. This is acceptable for a short-lived benchmark but the deployment patterns should not be carried into production without significant remediation.",
      "findings": [
        "No network policy enforcement observed: Both Loki and Elasticsearch components share the experiment namespace with no NetworkPolicy resources evident. In production, Elasticsearch's HTTP (9200) and transport (9300) ports and Loki's HTTP (3100) and gRPC (9095) ports must be restricted via Kubernetes NetworkPolicies to prevent unauthorized log access and cluster manipulation.",
        "Single-namespace co-tenancy risk: Deploying both logging stacks in the same namespace (logging-comparison-lsc7n) means any RBAC grants for operating one stack implicitly grant access to the other. Production deployments should isolate each stack in its own namespace with dedicated ServiceAccounts and least-privilege RBAC roles.",
        "No resource limits visible in metrics data: The cadvisor metrics show actual usage but there is no indication that resource requests/limits were set on the pods. Without limits, Elasticsearch's JVM can consume all available node memory (OOMKill risk), and without requests, the scheduler cannot make informed placement decisions. Both stacks should have explicit resource requests and limits in production.",
        "Single-node cluster with no redundancy: Running on one e2-medium node means a node failure loses all log data in-flight and any locally-stored indices. Elasticsearch requires a minimum 3-node cluster for shard replication; Loki requires object storage backend (GCS/S3) for durability.",
        "Promtail/Fluentd DaemonSet typically requires hostPath volume mounts and elevated privileges to read /var/log — these should be audited for least-privilege and read-only filesystem access in production."
      ],
      "supplyChain": "No evidence of image signing, provenance attestation, or SBOM generation for the deployed components. Loki and Elasticsearch images are likely pulled from Docker Hub (grafana/loki, docker.elastic.co/elasticsearch) without signature verification. Production deployments should: (1) mirror images to a private Artifact Registry with vulnerability scanning enabled, (2) enforce Binary Authorization policies on the GKE cluster to require signed images, (3) generate and attach SBOMs (SPDX/CycloneDX) for all logging stack images, and (4) pin images by digest rather than mutable tags to prevent tag-substitution attacks. The Elasticsearch image carries a particularly large dependency surface (JVM + Lucene + numerous Java libraries) that warrants regular CVE scanning."
    },
    "capabilitiesMatrix": {
      "technologies": [
        "Loki",
        "Elasticsearch"
      ],
      "categories": [
        {
          "name": "Query Language",
          "capabilities": [
            {
              "name": "Full-text search",
              "values": {
                "Loki": "Not supported (chunk scan only)",
                "Elasticsearch": "Full Lucene syntax with analyzers"
              }
            },
            {
              "name": "Label/field filtering",
              "values": {
                "Loki": "Native label selectors (LogQL)",
                "Elasticsearch": "Arbitrary field queries via KQL/Lucene"
              }
            },
            {
              "name": "Aggregations",
              "values": {
                "Loki": "Limited metric queries (rate, count_over_time)",
                "Elasticsearch": "Rich aggregation framework (terms, histograms, pipelines)"
              }
            },
            {
              "name": "Regex and fuzzy matching",
              "values": {
                "Loki": "Line-filter regex only",
                "Elasticsearch": "Per-field regex, fuzzy, and wildcard queries"
              }
            }
          ]
        },
        {
          "name": "Storage Architecture",
          "capabilities": [
            {
              "name": "Indexing strategy",
              "values": {
                "Loki": "Label-only index, compressed log chunks",
                "Elasticsearch": "Full inverted index over all document fields"
              }
            },
            {
              "name": "Storage backend",
              "values": {
                "Loki": "Object storage (S3/GCS) + small index DB",
                "Elasticsearch": "Local SSD-backed Lucene segments"
              }
            },
            {
              "name": "Storage scaling cost",
              "values": {
                "Loki": "Scales with label cardinality",
                "Elasticsearch": "Scales with log content volume"
              }
            }
          ]
        },
        {
          "name": "Resource Efficiency",
          "capabilities": [
            {
              "name": "Memory footprint",
              "values": {
                "Loki": "~129 MiB working set (observed)",
                "Elasticsearch": "JVM heap typically 512 MiB–2 GiB minimum"
              }
            },
            {
              "name": "CPU overhead",
              "values": {
                "Loki": "~1.95 CPU-seconds over ~10 min",
                "Elasticsearch": "Substantially higher due to tokenization and indexing"
              }
            },
            {
              "name": "Minimum viable deployment",
              "values": {
                "Loki": "Single binary on e2-medium feasible",
                "Elasticsearch": "Requires dedicated node pool or larger instances"
              }
            }
          ]
        },
        {
          "name": "Operational Complexity",
          "capabilities": [
            {
              "name": "Deployment footprint",
              "values": {
                "Loki": "Single binary + Promtail DaemonSet",
                "Elasticsearch": "Multi-node JVM cluster + Kibana + shipper"
              }
            },
            {
              "name": "Dependency surface",
              "values": {
                "Loki": "Minimal (object storage only)",
                "Elasticsearch": "JVM tuning, shard management, ILM policies"
              }
            },
            {
              "name": "Kubernetes fit",
              "values": {
                "Loki": "Lightweight, Prometheus-native labels",
                "Elasticsearch": "Heavier, requires StatefulSets and PVCs"
              }
            }
          ]
        }
      ]
    },
    "body": {
      "methodology": "The experiment deployed a single GKE cluster (e2-medium, 1 node) in the namespace 'logging-comparison-lsc7n' and ran a validation workflow ('logging-comparison-validation') to exercise the logging stack under controlled conditions. The total experiment duration was approximately 642 seconds (~10 min 42 sec), during which resource metrics were collected via cAdvisor instant queries at the end of the observation window. CPU usage was measured as cumulative CPU-seconds (container_cpu_usage_seconds_total), and memory was measured as the working-set size (container_memory_working_set_bytes), both broken down by namespace. The validation workflow itself ran for approximately 10 seconds (18:43:44 to 18:43:54) and completed successfully. A key limitation of this methodology is that it captured only cumulative instant-query snapshots rather than time-series data with a step interval (stepSeconds: 0), which means we observe total resource consumption over the experiment lifetime but cannot derive per-second rates or identify transient spikes. Additionally, the experiment ran only the Loki stack on a single small node; Elasticsearch was not co-deployed or separately benchmarked in a parallel cluster, so Elasticsearch resource figures referenced in this analysis are drawn from documented baselines and architectural expectations rather than matched empirical measurement.",
      "results": "Over the 642-second experiment window, the Loki-based logging stack in the experiment namespace consumed 1.947 cumulative CPU-seconds and a working-set memory of approximately 129.3 MiB (135,585,792 bytes). The total cluster CPU consumption across all namespaces was 2.604 CPU-seconds, meaning the logging stack accounted for roughly 74.8% of total cluster CPU activity, with the remaining 0.657 CPU-seconds attributable to GKE-managed system components (gke-managed-cim). Total cluster memory working set was 158.8 MiB (166,518,784 bytes), of which the logging namespace consumed 81.4% and system components used 29.5 MiB (30,932,992 bytes). The validation workflow completed in 10 seconds with a 'Succeeded' phase, confirming that the log pipeline was functional. The entire experiment ran on a single e2-medium instance (2 vCPUs, 4 GiB RAM) at an estimated cost of $0.0036 for the 0.178-hour duration. Normalizing CPU usage over the experiment duration yields an average of approximately 0.003 CPU-cores for the Loki stack, indicating extremely low sustained CPU overhead. The 129 MiB memory footprint represents roughly 3.2% of the 4 GiB available on the e2-medium node.",
      "discussion": "The measured resource profile confirms the hypothesis that Loki's label-only indexing architecture results in a very lightweight deployment footprint. At 129 MiB of working-set memory and effectively negligible sustained CPU usage (~0.003 cores average), Loki comfortably fits on the smallest practical GKE node alongside other workloads. By contrast, a minimal Elasticsearch deployment typically requires 512 MiB to 2 GiB of JVM heap per node before ingesting any data, with CPU consumption scaling proportionally to log throughput due to tokenization, analysis, and inverted-index maintenance. This means Elasticsearch would likely not fit on a single e2-medium instance alongside application workloads, immediately implying higher infrastructure cost for equivalent functionality. However, this resource advantage comes with a clear query capability trade-off: Loki's LogQL can only filter by pre-defined labels and scan log lines with regex patterns, whereas Elasticsearch enables arbitrary field-level search, fuzzy matching, and sophisticated aggregations without scanning raw data. For small-to-medium Kubernetes clusters where operational queries are predominantly 'show me logs for this pod/namespace/service in the last hour,' Loki's capabilities are sufficient and its resource efficiency is compelling. For use cases requiring cross-field correlation, analytics dashboards, or compliance-grade search, Elasticsearch's richer query surface justifies the higher resource cost. A significant caveat is that this experiment did not deploy Elasticsearch under identical conditions, so the resource comparison relies on architectural reasoning and industry baselines rather than matched empirical data. Additionally, the single-node, short-duration test does not capture behavior under sustained high-throughput ingestion, compaction cycles, or query load, all of which would differentiate the two systems further in production scenarios."
    },
    "feedback": {
      "recommendations": [
        "Deploy both Loki and Elasticsearch in parallel (separate namespaces or clusters with identical node specs) to enable direct empirical resource comparison under identical log throughput",
        "Extend the experiment duration to at least 1 hour with sustained log generation to capture steady-state behavior including chunk flushing, compaction, and index merging",
        "Add a structured query benchmark phase that executes equivalent operational queries (e.g., filter by label, keyword search, regex match, time-range aggregation) against both systems and measures latency and resource cost per query",
        "Test with multiple log throughput levels (e.g., 100, 1000, 10000 lines/sec) to characterize how each system's resource consumption scales with ingestion rate"
      ],
      "experimentDesign": [
        "Collect time-series metrics with a non-zero step interval (e.g., 15s) rather than instant queries to enable rate-of-change analysis and identification of resource consumption patterns over time",
        "Include per-container metric breakdowns (e.g., separate Loki from Promtail, or Elasticsearch from Filebeat/Kibana) to attribute resource usage to specific pipeline components",
        "Add storage I/O metrics (disk read/write bytes, IOPS) and network metrics (ingestion throughput in bytes/sec) to provide a more complete resource efficiency picture"
      ]
    },
    "summary": "Insufficient data to evaluate the hypothesis. The experiment aimed to compare Loki vs Elasticsearch resource efficiency and query capabilities, but the collected metrics show only a single target cluster ('app') with aggregate cAdvisor counters — there is no per-stack breakdown distinguishing Loki containers from Elasticsearch containers, no query latency or throughput metrics, and no evidence that both stacks were deployed or received equivalent log volume. The single namespace 'logging-comparison-lsc7n' consumed ~1.95 cumulative CPU-seconds and ~129 MiB memory working set over a 10m42s window, but without per-container or per-stack labels it is impossible to attribute these costs to either logging system. To properly evaluate this hypothesis, the experiment would need: (1) separate metric labels or namespaces isolating Loki and Elasticsearch workloads, (2) a controlled log generator producing identical throughput to both stacks, (3) query latency and result-completeness metrics for LogQL vs Lucene/KQL, and (4) a longer steady-state window (≥30 minutes) to observe stable resource consumption beyond startup transients. The most actionable finding is that the experiment infrastructure must be redesigned to produce per-stack resource attribution before any Loki-vs-Elasticsearch comparison can be drawn.",
    "recommendations": [
      "Deploy both Loki and Elasticsearch in parallel (separate namespaces or clusters with identical node specs) to enable direct empirical resource comparison under identical log throughput",
      "Extend the experiment duration to at least 1 hour with sustained log generation to capture steady-state behavior including chunk flushing, compaction, and index merging",
      "Add a structured query benchmark phase that executes equivalent operational queries (e.g., filter by label, keyword search, regex match, time-range aggregation) against both systems and measures latency and resource cost per query",
      "Test with multiple log throughput levels (e.g., 100, 1000, 10000 lines/sec) to characterize how each system's resource consumption scales with ingestion rate"
    ],
    "generatedAt": "2026-02-11T18:46:44Z",
    "model": "claude-opus-4-6"
  }
}
