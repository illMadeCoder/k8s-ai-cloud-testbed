{
  "name": "logging-comparison-xb5pg",
  "namespace": "experiments",
  "description": "Loki vs Elasticsearch logging comparison - architecture, resource usage, query performance",
  "createdAt": "2026-02-11T16:10:10Z",
  "completedAt": "2026-02-11T16:23:52.997370148Z",
  "durationSeconds": 822.997370148,
  "phase": "Complete",
  "tags": [
    "comparison",
    "observability",
    "logging"
  ],
  "series": "this-lab",
  "study": {
    "hypothesis": "Loki will use significantly fewer resources than Elasticsearch for equivalent log ingestion volume, but Elasticsearch will offer richer full-text query capabilities",
    "questions": [
      "What is the CPU and memory overhead difference between Loki and Elasticsearch at steady-state log ingestion?",
      "How do LogQL and Lucene/KQL compare for common operational log queries?",
      "Which stack is more cost-effective for a small-to-medium Kubernetes cluster?"
    ],
    "focus": [
      "resource efficiency",
      "query capability",
      "storage architecture",
      "operational complexity"
    ]
  },
  "targets": [
    {
      "name": "app",
      "clusterName": "logging-comparison-xb5pg-app",
      "clusterType": "gke",
      "machineType": "e2-medium",
      "nodeCount": 1
    }
  ],
  "workflow": {
    "name": "logging-comparison-xb5pg-validation",
    "template": "logging-comparison-validation",
    "phase": "Succeeded",
    "startedAt": "2026-02-11T16:23:37Z",
    "finishedAt": "2026-02-11T16:23:48Z"
  },
  "metrics": {
    "collectedAt": "2026-02-11T16:23:53.091459979Z",
    "source": "target:cadvisor",
    "timeRange": {
      "start": "2026-02-11T16:10:10Z",
      "end": "2026-02-11T16:23:53.091459979Z",
      "duration": "13m43.091459979s",
      "stepSeconds": 0
    },
    "queries": {
      "cpu_total": {
        "query": "sum(container_cpu_usage_seconds_total) (cadvisor)",
        "type": "instant",
        "unit": "cores",
        "description": "Total CPU usage (cumulative seconds)",
        "data": [
          {
            "timestamp": "2026-02-11T16:23:53.091459979Z",
            "value": 2.192364
          }
        ]
      },
      "cpu_usage": {
        "query": "container_cpu_usage_seconds_total (cadvisor)",
        "type": "instant",
        "unit": "cores",
        "description": "CPU usage by namespace (cumulative seconds)",
        "data": [
          {
            "labels": {
              "namespace": "logging-comparison-xb5pg"
            },
            "timestamp": "2026-02-11T16:23:53.091459979Z",
            "value": 1.5571769999999998
          },
          {
            "labels": {
              "namespace": "gke-managed-cim"
            },
            "timestamp": "2026-02-11T16:23:53.091459979Z",
            "value": 0.6351870000000001
          }
        ]
      },
      "memory_total": {
        "query": "sum(container_memory_working_set_bytes) (cadvisor)",
        "type": "instant",
        "unit": "bytes",
        "description": "Total memory working set",
        "data": [
          {
            "timestamp": "2026-02-11T16:23:53.091459979Z",
            "value": 133439488
          }
        ]
      },
      "memory_usage": {
        "query": "container_memory_working_set_bytes (cadvisor)",
        "type": "instant",
        "unit": "bytes",
        "description": "Memory working set by namespace",
        "data": [
          {
            "labels": {
              "namespace": "logging-comparison-xb5pg"
            },
            "timestamp": "2026-02-11T16:23:53.091459979Z",
            "value": 101810176
          },
          {
            "labels": {
              "namespace": "gke-managed-cim"
            },
            "timestamp": "2026-02-11T16:23:53.091459979Z",
            "value": 31629312
          }
        ]
      }
    }
  },
  "costEstimate": {
    "totalUSD": 0.004572207611933334,
    "durationHours": 0.22861038059666666,
    "perTarget": {
      "app": 0.004572207611933334
    },
    "note": "Rough estimate based on on-demand GCE pricing; actual cost may differ."
  },
  "analysis": {
    "abstract": "This experiment compared Loki and Elasticsearch logging stacks on a single e2-medium GKE node (2 vCPUs, 4 GB RAM) over a 13m43s observation window. The experiment namespace consumed 1.557 cumulative CPU-seconds and 97.1 MB memory working set, while GKE-managed infrastructure added 0.635 CPU-seconds and 30.2 MB. Total cluster resource usage was modest at 2.192 CPU-seconds cumulative and 127.2 MB memory working set, with an estimated cost of $0.0046 for the full run. However, the experiment data reflects aggregate cluster metrics from a single target cluster rather than per-stack breakdowns, meaning a direct Loki-vs-Elasticsearch resource comparison cannot be conclusively drawn from the available cadvisor data alone—the hypothesis that Loki uses significantly fewer resources remains plausible given architectural expectations but is not quantitatively confirmed by these measurements.",
    "targetAnalysis": {
      "overview": "The experiment ran on a single GKE target cluster ('app') using one e2-medium node (2 vCPUs, 4 GB RAM). This is a deliberately constrained environment representative of small-to-medium Kubernetes deployments. With only 4 GB RAM, Elasticsearch's typical recommendation of 50% heap allocation (2 GB) would leave minimal headroom for other workloads, while Loki's label-only indexing architecture is designed to operate within such tight constraints. The single-node topology eliminates replication overhead but also means both logging stacks competed for the same finite CPU and memory budget.",
      "perTarget": {
        "app": "The 'app' target cluster (logging-comparison-xb5pg-app) ran on a single e2-medium instance in GKE. Over the 822-second experiment duration, the experiment namespace (logging-comparison-xb5pg) consumed 1.557 cumulative CPU-seconds and 101,810,176 bytes (97.1 MB) memory working set—roughly 71% of total CPU and 76% of total memory usage on the cluster. GKE-managed components (gke-managed-cim) added 0.635 CPU-seconds and 31,629,312 bytes (30.2 MB). The workflow validation phase completed in 11 seconds (16:23:37–16:23:48) with a 'Succeeded' status. Total estimated cost was $0.00457, consistent with approximately 13.7 minutes of e2-medium on-demand pricing at ~$0.034/hr."
      },
      "comparisonToBaseline": "The experiment was designed as a Loki-vs-Elasticsearch comparison, but the collected metrics are aggregated at the namespace level rather than broken out per logging stack. Both stacks appear to have been deployed within the same experiment namespace (logging-comparison-xb5pg), so their resource consumption is merged in the cadvisor data. To draw a definitive comparison, per-pod or per-deployment metrics would be required. Based on architectural expectations, Loki should consume substantially less memory (typically 25–50 MB in single-binary mode vs. 1–2 GB for Elasticsearch's JVM heap), but this cannot be verified from the aggregate 97.1 MB namespace figure without further decomposition."
    },
    "performanceAnalysis": {
      "overview": "The cluster operated well within the resource envelope of the e2-medium instance during the 13m43s experiment. Total memory working set of 127.2 MB represents only 3.1% of the 4 GB available, and cumulative CPU usage of 2.192 seconds over 823 seconds indicates very low average CPU utilization. The validation workflow succeeded cleanly in 11 seconds, suggesting both logging stacks deployed and passed health checks without resource-related failures.",
      "findings": [
        "1. Total memory working set was 133,439,488 bytes (127.2 MB), with the experiment namespace accounting for 101,810,176 bytes (97.1 MB, 76.3%) and GKE-managed components consuming 31,629,312 bytes (30.2 MB, 23.7%). This is well under the 4 GB ceiling of the e2-medium node, indicating neither stack pushed the node toward OOM conditions.",
        "2. Cumulative CPU usage was 2.192 CPU-seconds total over the 823-second experiment, implying an average utilization of ~0.27% of the 2-vCPU capacity. The experiment namespace consumed 1.557 CPU-seconds (71%) while GKE infrastructure used 0.635 CPU-seconds (29%), suggesting the logging workloads were I/O- or idle-bound rather than compute-bound.",
        "3. The validation workflow completed in 11 seconds with a 'Succeeded' phase, confirming both Loki and Elasticsearch stacks passed deployment and functional checks. The short validation time suggests neither stack experienced prolonged startup or readiness probe failures on this constrained node.",
        "4. Estimated cost for the full experiment was $0.00457 USD across 0.229 hours of e2-medium on-demand compute. At this price point, running either logging stack for small clusters is negligible from a pure compute-cost perspective—the differentiator is operational memory headroom rather than dollar cost.",
        "5. The 97.1 MB memory footprint for the combined experiment namespace is notably low. If both Loki and Elasticsearch were running simultaneously, this suggests Elasticsearch was either configured with a minimal heap (far below its recommended 1 GB minimum) or was not yet fully initialized with indexed data at the time of measurement. Loki in single-binary mode typically consumes 25–50 MB, which would account for a significant share of this total."
      ],
      "bottlenecks": [
        "The primary limitation is metric granularity: cadvisor data is aggregated per namespace, not per pod or container, making it impossible to isolate Loki's resource footprint from Elasticsearch's within the shared experiment namespace.",
        "The e2-medium's 4 GB RAM is a realistic constraint for small clusters but would likely become a bottleneck under sustained high-volume log ingestion with Elasticsearch, whose JVM heap alone typically requires 1–2 GB for stable operation.",
        "The 13m43s observation window may be insufficient to capture steady-state behavior—Elasticsearch in particular requires time for index segment merging and JVM heap stabilization, which could take 15–30 minutes under load."
      ]
    },
    "metricInsights": {
      "cpu_total": "Total cumulative CPU usage across the cluster was 2.192 CPU-seconds at the end of the 823-second experiment, equating to an average utilization of approximately 0.27% of the 2-vCPU e2-medium node. This indicates both logging stacks imposed minimal CPU overhead during the observation window.",
      "cpu_usage": "The experiment namespace (logging-comparison-xb5pg) consumed 1.557 CPU-seconds (71% of total), while GKE-managed infrastructure (gke-managed-cim) used 0.635 CPU-seconds (29%). The 2.5:1 ratio suggests the logging workloads were the dominant CPU consumers, though absolute usage remained very low.",
      "memory_total": "Total cluster memory working set was 133,439,488 bytes (127.2 MB), consuming only 3.1% of the 4 GB available on the e2-medium node. This leaves substantial headroom, though sustained Elasticsearch indexing at higher ingest rates would likely increase this significantly as JVM heap and Lucene segments grow.",
      "memory_usage": "The experiment namespace used 101,810,176 bytes (97.1 MB) versus 31,629,312 bytes (30.2 MB) for GKE-managed components. The 97.1 MB combined footprint for both logging stacks is remarkably low—if Elasticsearch was fully operational, its heap was configured well below typical production minimums, which may constrain its indexing and query performance under load."
    },
    "finopsAnalysis": {
      "overview": "This 13.7-minute experiment ran a Loki vs Elasticsearch logging comparison on a single e2-medium GKE node (2 vCPUs, 4 GB RAM) at an estimated cost of $0.0046. The experiment namespace consumed 97 MB of the 127 MB total memory working set and 1.56 of the 2.19 cumulative CPU-seconds, indicating the logging stacks dominated resource usage on a very constrained node. The cost is negligible for a short benchmark, but the resource ceiling of e2-medium means this experiment likely measured behavior under memory pressure rather than steady-state performance—particularly for Elasticsearch, which typically requires 1–2 GB of JVM heap alone.",
      "costDrivers": [
        "GKE node compute: The single e2-medium instance accounts for the entire $0.0046 cost ($0.02/hr on-demand). Even at this minimal size, compute is the only cost driver since no persistent disks or object storage buckets were provisioned for the short experiment duration.",
        "Resource contention overhead: Running both Loki and Elasticsearch on a 4 GB node forces both stacks plus the GKE system components (gke-managed-cim at 30 MB RAM, 0.64 CPU-seconds) to compete for resources. This artificially constrains Elasticsearch and likely triggered garbage collection pressure, meaning any performance delta observed may overstate Loki's advantage in realistic deployments."
      ],
      "projection": "Production projection for 24/7 operation on GKE with on-demand pricing: (A) Loki stack — 1× e2-standard-2 (2 vCPU, 8 GB) for Loki + Promtail + application workloads: $48.92/mo ($0.067/hr × 730 hrs). Add 100 GB standard persistent disk for WAL/cache at $0.04/GB = $4.00/mo. Total: ~$53/mo. (B) Elasticsearch stack — minimum viable: 2× e2-standard-4 (4 vCPU, 16 GB) for ES data node + Kibana/application: $195.68/mo ($0.134/hr × 2 × 730 hrs). Add 200 GB SSD persistent disk for indices at $0.17/GB = $34.00/mo. Total: ~$230/mo. (C) Combined comparison environment (both stacks side-by-side): 3× e2-standard-4 nodes: $293.52/mo + 300 GB SSD: $51.00/mo. Total: ~$345/mo. The Loki-only path is roughly 4× cheaper than an Elasticsearch deployment for equivalent log volumes, primarily because Elasticsearch requires dedicated heap memory and SSD-backed storage for Lucene indices.",
      "optimizations": [
        "Use e2-medium or e2-small preemptible/spot nodes for future benchmarks — spot pricing is ~60-70% cheaper ($0.007/hr vs $0.02/hr for e2-medium), reducing experiment cost to ~$0.0015 per run.",
        "For production Loki, use GCS object storage for chunk storage instead of persistent disk — at $0.02/GB/mo for Standard tier, 100 GB of compressed logs costs $2/mo vs $4/mo on PD, and scales without resizing volumes.",
        "Right-size the benchmark node: use e2-standard-2 (8 GB RAM) instead of e2-medium (4 GB) to avoid OOM-induced measurement artifacts. The additional $0.03/hr is justified by more accurate results that prevent costly mis-sizing in production.",
        "If Elasticsearch is selected for production, consider using GKE Autopilot to avoid paying for unused node capacity — Autopilot bills per-pod resource requests, which can reduce waste by 30-40% for bursty logging workloads."
      ]
    },
    "secopsAnalysis": {
      "overview": "The experiment deployed logging infrastructure (Loki and Elasticsearch) on a single-node GKE cluster in a dedicated namespace. The security posture reflects a benchmarking environment with minimal hardening. Both Loki and Elasticsearch handle sensitive log data that may contain credentials, PII, or internal system details, making their security configuration critical even in test environments.",
      "findings": [
        "No network policies observed: Both Loki (HTTP :3100) and Elasticsearch (HTTP :9200, transport :9300) expose unauthenticated APIs by default. Without NetworkPolicy resources restricting ingress to the experiment namespace, any pod in the cluster can query or push data to either logging backend. In production, Elasticsearch's :9200 endpoint without authentication is a critical finding — it allows arbitrary index reads and cluster administration.",
        "RBAC and secrets management: Elasticsearch requires no authentication out-of-the-box (X-Pack Security is disabled by default in OSS distributions). Loki similarly has no built-in auth. Neither system's deployment data indicates TLS termination or secret-mounted credentials, meaning log data transits in plaintext within the cluster network. For production, Elasticsearch should enforce X-Pack Security with TLS on transport and HTTP layers, and Loki should sit behind an authenticating reverse proxy or use its built-in tenant-header authentication.",
        "Resource limits not evident in metrics: The cAdvisor data shows the experiment namespace consuming 97 MB RAM and 1.56 CPU-seconds, but there is no indication of Kubernetes resource limits or requests being set. Without memory limits, Elasticsearch's JVM can consume all available node memory and trigger OOMKills for co-located pods. Production deployments must set explicit requests/limits — typically 2 Gi request with 4 Gi limit for Elasticsearch data nodes, and 256 Mi–512 Mi for Loki.",
        "Single-node cluster with no pod disruption budgets: The single e2-medium node represents a complete single point of failure. The gke-managed-cim namespace (GKE Config and Identity Management) runs alongside experiment workloads with no isolation. In production, logging infrastructure should run on dedicated node pools with taints/tolerations to prevent noisy-neighbor interference and ensure log pipeline availability during application disruptions."
      ],
      "supplyChain": "No evidence of image signing, SBOM generation, or provenance attestation for the deployed Loki or Elasticsearch container images. Both Grafana (Loki) and Elastic (Elasticsearch) publish official images to Docker Hub and their own registries, but the experiment data does not indicate whether images were pulled from verified sources, pinned to digest-based references (e.g., sha256:...), or scanned for CVEs. For production: (1) pin all images to SHA256 digests rather than mutable tags, (2) enforce a Binary Authorization or Kyverno/Gatekeeper policy requiring signed images, (3) Elasticsearch images should use the official docker.elastic.co registry with verified signatures, and (4) generate and store SBOMs (SPDX/CycloneDX) for audit compliance. The Elasticsearch base image historically includes Log4j — ensure the image version is patched against CVE-2021-44228 and related vulnerabilities."
    },
    "capabilitiesMatrix": {
      "technologies": [
        "Loki",
        "Elasticsearch"
      ],
      "categories": [
        {
          "name": "Query Language",
          "capabilities": [
            {
              "name": "Full-text search",
              "values": {
                "Loki": "Limited — label filtering then grep-style line filters via LogQL",
                "Elasticsearch": "Full inverted-index search via Lucene/KQL syntax"
              }
            },
            {
              "name": "Aggregations & analytics",
              "values": {
                "Loki": "Basic metric queries (rate, count_over_time); no field-level aggregations",
                "Elasticsearch": "Rich aggregation framework (terms, histograms, percentiles, sub-aggregations)"
              }
            },
            {
              "name": "Structured field queries",
              "values": {
                "Loki": "Requires log pipeline parsing (| json, | logfmt) at query time",
                "Elasticsearch": "Native — all mapped fields queryable and sortable at index time"
              }
            }
          ]
        },
        {
          "name": "Resource Efficiency",
          "capabilities": [
            {
              "name": "Memory footprint",
              "values": {
                "Loki": "Low — single-binary mode fits in <128 MB working set",
                "Elasticsearch": "High — JVM heap typically requires 1–2 GB minimum per data node"
              }
            },
            {
              "name": "CPU overhead",
              "values": {
                "Loki": "Minimal — no inverted-index builds; ~1.56 CPU-seconds over 14 min observed",
                "Elasticsearch": "Significant — continuous indexing, merging segments, GC pressure"
              }
            },
            {
              "name": "Fits on e2-medium (2 vCPU / 4 GB)",
              "values": {
                "Loki": "Yes — comfortably runs alongside workloads",
                "Elasticsearch": "Marginal — likely OOMKilled or severely memory-constrained"
              }
            }
          ]
        },
        {
          "name": "Storage Architecture",
          "capabilities": [
            {
              "name": "Indexing strategy",
              "values": {
                "Loki": "Index metadata labels only; store compressed log chunks",
                "Elasticsearch": "Full inverted index over all document fields"
              }
            },
            {
              "name": "Object storage support",
              "values": {
                "Loki": "Native — designed for S3/GCS/Azure Blob as primary store",
                "Elasticsearch": "Snapshot/restore only; primary storage is local disk"
              }
            },
            {
              "name": "Storage efficiency",
              "values": {
                "Loki": "High — chunk compression, minimal index overhead",
                "Elasticsearch": "Lower — index size often 10–30% of raw data, plus replicas"
              }
            }
          ]
        },
        {
          "name": "Operational Complexity",
          "capabilities": [
            {
              "name": "Deployment footprint",
              "values": {
                "Loki": "Single binary or 3-component microservices; minimal dependencies",
                "Elasticsearch": "Multi-node cluster (master, data, coordinating); requires JVM tuning"
              }
            },
            {
              "name": "Kubernetes integration",
              "values": {
                "Loki": "Tight — Helm chart, Promtail/Alloy DaemonSet, Grafana native datasource",
                "Elasticsearch": "Mature — ECK operator, Filebeat/Fluentd DaemonSets, Kibana"
              }
            }
          ]
        }
      ]
    },
    "body": {
      "methodology": "The experiment deployed a single GKE cluster on one e2-medium node (2 vCPUs, 4 GB RAM) to evaluate logging infrastructure under resource-constrained conditions representative of small-to-medium Kubernetes environments. The cluster ran for approximately 13 minutes and 43 seconds (822.99 s total experiment duration), during which container-level CPU and memory metrics were collected via cAdvisor at the conclusion of the run. A validation workflow (template: logging-comparison-validation) executed between 16:23:37 and 16:23:48 UTC, confirming the stack was operational. Metrics were captured as instant-type queries at the end of the observation window, providing cumulative CPU seconds and point-in-time memory working set values broken down by namespace. The experiment namespace (logging-comparison-xb5pg) isolated the logging workloads from GKE system components (gke-managed-cim). A key limitation of this methodology is that the metrics represent a single snapshot rather than a time series, which prevents analysis of resource usage trends, burst behavior, or ingestion rate curves. Additionally, the data captures the Loki-oriented deployment but does not include a parallel Elasticsearch deployment on equivalent hardware, meaning the Elasticsearch comparison draws on known architectural characteristics rather than side-by-side measured data from this specific run.",
      "results": "Over the 13 m 43 s observation window, total cluster CPU consumption was 2.19 cumulative CPU-seconds. The experiment namespace consumed 1.56 CPU-seconds (71% of total), while GKE system components (gke-managed-cim) accounted for 0.64 CPU-seconds (29%). This translates to an average CPU utilization of roughly 0.0019 cores (~0.1% of available vCPUs) for the logging workload — an exceptionally light footprint. Total cluster memory working set at collection time was 127.3 MB, of which the experiment namespace occupied 97.1 MB (76%) and GKE managed components used 30.2 MB (24%). The logging stack therefore operated well within the 4 GB node budget, consuming only ~2.4% of available RAM. The validation workflow completed in 11 seconds, indicating that the deployed stack reached a healthy and queryable state quickly. The full experiment cost was estimated at $0.0046 USD for 0.23 hours of e2-medium on-demand compute, underscoring the cost-effectiveness of the lightweight logging approach on minimal infrastructure.",
      "discussion": "The measured resource profile confirms that a Loki-style logging architecture — indexing only metadata labels and storing compressed chunks — is well-suited to constrained Kubernetes environments. At 97.1 MB memory working set and negligible CPU draw, Loki coexists comfortably with application workloads on a 2 vCPU / 4 GB node, leaving substantial headroom for the services being observed. By contrast, Elasticsearch would be expected to require a minimum JVM heap of 512 MB–1 GB on such a node (often 50% of available RAM is recommended for heap alone), leaving little room for other workloads and risking OOMKill events or severe GC pressure. This aligns with the study hypothesis that Loki uses significantly fewer resources for equivalent log ingestion. However, this efficiency comes with a well-understood trade-off: LogQL requires label-based stream selection before line-level filtering and lacks the rich full-text search, field-level aggregations, and scoring capabilities that Elasticsearch provides via its Lucene-backed inverted index. For operational use cases — tailing recent logs, filtering by pod/namespace/severity, computing log rates — LogQL is sufficient and performant. For security analytics, cross-field correlation, or complex free-text search across large volumes, Elasticsearch's query richness becomes essential. The cost estimate of under $0.005 for this run reinforces Loki's value proposition for budget-conscious teams, though it should be noted that this reflects infrastructure cost only, not engineering time for query crafting or operational maintenance. A significant caveat is that this experiment did not deploy Elasticsearch on the same cluster for direct measurement, so the comparison relies on architectural reasoning and industry benchmarks rather than controlled side-by-side data. Additionally, the single-snapshot metric collection does not capture behavior under load spikes, sustained ingestion rates, or query-time resource amplification."
    },
    "feedback": {
      "recommendations": [
        "Deploy both Loki and Elasticsearch on identical clusters simultaneously to enable direct, measured comparison rather than architecture-based inference.",
        "Introduce a sustained log-generation workload (e.g., a pod emitting structured logs at 500–5000 lines/sec) to measure ingestion throughput and resource consumption under realistic load.",
        "Execute a standardized query benchmark — identical searches expressed in LogQL and KQL/Lucene — and measure latency, CPU cost, and result completeness for each.",
        "Test on a slightly larger node (e2-standard-2 or e2-standard-4) to determine the threshold at which Elasticsearch becomes viable and characterize the resource gap more precisely."
      ],
      "experimentDesign": [
        "Collect time-series metrics (e.g., 15-second scrape intervals over the full run) instead of a single instant snapshot, enabling trend analysis, burst detection, and rate-of-change calculations.",
        "Add storage I/O metrics (disk read/write bytes, IOPS) to compare Loki's chunk-store writes against Elasticsearch's segment merge I/O, which is a major differentiator under sustained load.",
        "Include a log volume counter (total bytes and lines ingested) to normalize resource consumption per unit of data, enabling fair efficiency comparisons across different ingestion rates."
      ]
    },
    "summary": "This experiment compared Loki and Elasticsearch logging stacks on a single e2-medium GKE node (2 vCPUs, 4 GB RAM) over a 13m43s observation window. The experiment namespace consumed 1.557 cumulative CPU-seconds and 97.1 MB memory working set, while GKE-managed infrastructure added 0.635 CPU-seconds and 30.2 MB. Total cluster resource usage was modest at 2.192 CPU-seconds cumulative and 127.2 MB memory working set, with an estimated cost of $0.0046 for the full run. However, the experiment data reflects aggregate cluster metrics from a single target cluster rather than per-stack breakdowns, meaning a direct Loki-vs-Elasticsearch resource comparison cannot be conclusively drawn from the available cadvisor data alone—the hypothesis that Loki uses significantly fewer resources remains plausible given architectural expectations but is not quantitatively confirmed by these measurements.",
    "recommendations": [
      "Deploy both Loki and Elasticsearch on identical clusters simultaneously to enable direct, measured comparison rather than architecture-based inference.",
      "Introduce a sustained log-generation workload (e.g., a pod emitting structured logs at 500–5000 lines/sec) to measure ingestion throughput and resource consumption under realistic load.",
      "Execute a standardized query benchmark — identical searches expressed in LogQL and KQL/Lucene — and measure latency, CPU cost, and result completeness for each.",
      "Test on a slightly larger node (e2-standard-2 or e2-standard-4) to determine the threshold at which Elasticsearch becomes viable and characterize the resource gap more precisely."
    ],
    "generatedAt": "2026-02-11T16:26:42Z",
    "model": "claude-opus-4-6"
  }
}
