{
  "name": "logging-comparison-cxpfp",
  "namespace": "experiments",
  "description": "Loki vs Elasticsearch logging comparison - architecture, resource usage, query performance",
  "createdAt": "2026-02-11T15:50:40Z",
  "completedAt": "2026-02-11T16:03:32.299054177Z",
  "durationSeconds": 772.299054177,
  "phase": "Complete",
  "tags": [
    "comparison",
    "observability",
    "logging"
  ],
  "series": "this-lab",
  "study": {
    "hypothesis": "Loki will use significantly fewer resources than Elasticsearch for equivalent log ingestion volume, but Elasticsearch will offer richer full-text query capabilities",
    "questions": [
      "What is the CPU and memory overhead difference between Loki and Elasticsearch at steady-state log ingestion?",
      "How do LogQL and Lucene/KQL compare for common operational log queries?",
      "Which stack is more cost-effective for a small-to-medium Kubernetes cluster?"
    ],
    "focus": [
      "resource efficiency",
      "query capability",
      "storage architecture",
      "operational complexity"
    ]
  },
  "targets": [
    {
      "name": "app",
      "clusterName": "logging-comparison-cxpfp-app",
      "clusterType": "gke",
      "machineType": "e2-medium",
      "nodeCount": 1
    }
  ],
  "workflow": {
    "name": "logging-comparison-cxpfp-validation",
    "template": "logging-comparison-validation",
    "phase": "Succeeded",
    "startedAt": "2026-02-11T16:03:17Z",
    "finishedAt": "2026-02-11T16:03:27Z"
  },
  "metrics": {
    "collectedAt": "2026-02-11T16:03:32.40304089Z",
    "source": "target:cadvisor",
    "timeRange": {
      "start": "2026-02-11T15:50:40Z",
      "end": "2026-02-11T16:03:32.40304089Z",
      "duration": "12m52.40304089s",
      "stepSeconds": 0
    },
    "queries": {
      "cpu_total": {
        "query": "sum(container_cpu_usage_seconds_total) (cadvisor)",
        "type": "instant",
        "unit": "cores",
        "description": "Total CPU usage (cumulative seconds)",
        "data": [
          {
            "timestamp": "2026-02-11T16:03:32.40304089Z",
            "value": 2.5350919999999997
          }
        ]
      },
      "cpu_usage": {
        "query": "container_cpu_usage_seconds_total (cadvisor)",
        "type": "instant",
        "unit": "cores",
        "description": "CPU usage by namespace (cumulative seconds)",
        "data": [
          {
            "labels": {
              "namespace": "logging-comparison-cxpfp"
            },
            "timestamp": "2026-02-11T16:03:32.40304089Z",
            "value": 1.8979989999999998
          },
          {
            "labels": {
              "namespace": "gke-managed-cim"
            },
            "timestamp": "2026-02-11T16:03:32.40304089Z",
            "value": 0.637093
          }
        ]
      },
      "memory_total": {
        "query": "sum(container_memory_working_set_bytes) (cadvisor)",
        "type": "instant",
        "unit": "bytes",
        "description": "Total memory working set",
        "data": [
          {
            "timestamp": "2026-02-11T16:03:32.40304089Z",
            "value": 166899712
          }
        ]
      },
      "memory_usage": {
        "query": "container_memory_working_set_bytes (cadvisor)",
        "type": "instant",
        "unit": "bytes",
        "description": "Memory working set by namespace",
        "data": [
          {
            "labels": {
              "namespace": "logging-comparison-cxpfp"
            },
            "timestamp": "2026-02-11T16:03:32.40304089Z",
            "value": 135315456
          },
          {
            "labels": {
              "namespace": "gke-managed-cim"
            },
            "timestamp": "2026-02-11T16:03:32.40304089Z",
            "value": 31584256
          }
        ]
      }
    }
  },
  "costEstimate": {
    "totalUSD": 0.004290550300983334,
    "durationHours": 0.21452751504916667,
    "perTarget": {
      "app": 0.004290550300983334
    },
    "note": "Rough estimate based on on-demand GCE pricing; actual cost may differ."
  },
  "analysis": {
    "finopsAnalysis": {
      "overview": "The Loki vs Elasticsearch comparison experiment ran for ~13 minutes on a single e2-medium GKE node, costing an estimated $0.0043. Total resource consumption was modest — 1.90 CPU-seconds cumulative for the experiment namespace and ~129MB memory working set — reflecting a short-lived benchmark rather than sustained production operation. The experiment namespace consumed 75% of total CPU and 81% of total memory, with GKE system overhead (gke-managed-cim) accounting for the remainder.",
      "costDrivers": [
        "GKE node compute: The single e2-medium instance (2 vCPUs, 4GB RAM) at ~$0.02/hr on-demand is the sole cost driver, accounting for the full $0.0043 estimate over 12.9 minutes. No persistent disks, load balancers, or egress charges are visible in the data.",
        "GKE cluster management fee: GKE Autopilot or Standard clusters incur a $0.10/hr management fee (free tier for one zonal cluster). If this experiment ran on a dedicated cluster, the management fee ($0.021 for 13 minutes) would actually exceed the compute cost by ~5x, making it the hidden dominant cost."
      ],
      "projection": "Production projection for 24/7 operation on non-preemptible nodes:\n\n— Loki stack (single-binary mode): 1x e2-standard-2 (2 vCPU, 8GB) for Loki + Promtail = $48.92/mo. Add 100GB SSD PD for chunk storage = $17.00/mo. Subtotal: ~$66/mo.\n\n— Elasticsearch stack (single-node minimum): 1x e2-standard-4 (4 vCPU, 16GB) to accommodate JVM heap (2GB+) plus OS/indexing overhead = $97.83/mo. Add 200GB SSD PD for index shards = $34.00/mo. Subtotal: ~$132/mo.\n\n— Realistic multi-node production setup (HA): Loki (3 replicas + object storage via GCS) = 3x e2-standard-2 ($146.76) + GCS (~$5/mo for 200GB) = ~$152/mo. Elasticsearch (3-node cluster minimum for HA) = 3x e2-standard-4 ($293.49) + 3x 200GB SSD PD ($102) = ~$395/mo.\n\n— GKE management fee: $74.40/mo per cluster.\n\nTotal monthly production estimate: Loki HA = ~$226/mo | Elasticsearch HA = ~$470/mo. Loki is roughly 2x more cost-effective at equivalent ingestion volumes, primarily due to lower memory and CPU requirements from its label-only indexing strategy.",
      "optimizations": [
        "Use e2-medium or e2-small for Loki single-binary in dev/staging — the experiment showed it runs comfortably within 4GB RAM, saving ~50% vs e2-standard-2 ($24/mo savings per node).",
        "Use preemptible/spot VMs for non-production benchmarks — e2-medium spot pricing is ~$0.006/hr vs $0.02/hr on-demand, reducing experiment costs by 70% (this 13-min run would cost ~$0.0013).",
        "For Elasticsearch, right-size JVM heap to 50% of node RAM and use index lifecycle management (ILM) to roll cold indices to cheaper storage (standard PD or Nearline GCS), potentially saving 30-40% on storage costs.",
        "Consolidate experiment workloads onto a shared GKE cluster to amortize the $74.40/mo management fee across multiple experiments rather than paying per-experiment cluster overhead."
      ]
    },
    "secopsAnalysis": {
      "overview": "The experiment deployed Loki and Elasticsearch components into a dedicated namespace (logging-comparison-cxpfp) on a single-node GKE cluster. While namespace isolation provides a basic boundary, the single-node topology and benchmark-oriented deployment likely omit production security controls such as network policies, RBAC scoping, and secrets encryption. The presence of gke-managed-cim namespace indicates GKE's built-in monitoring is active, providing some baseline observability.",
      "findings": [
        "Namespace isolation is present but likely insufficient — both Loki and Elasticsearch are co-located on the same node and namespace, meaning a compromise of either service could expose the other's data. In production, these should run in separate namespaces with distinct ServiceAccounts and RBAC bindings scoped to least-privilege.",
        "No evidence of NetworkPolicy enforcement — without NetworkPolicies, any pod in the cluster can reach Elasticsearch's REST API (port 9200) and Loki's push endpoint (port 3100) directly. Elasticsearch is particularly sensitive here, as its default configuration has no authentication. Production deployments must enforce ingress/egress NetworkPolicies restricting access to authorized log shippers only.",
        "Resource limits and requests are not visible in the data — without resource limits, either Loki or Elasticsearch could starve the other of CPU/memory on this constrained 4GB node. Elasticsearch's JVM is especially prone to consuming all available memory. Both services should have explicit resource requests and limits, and Elasticsearch should have its JVM heap capped via -Xmx.",
        "Single-node GKE cluster with no node pool isolation — running logging infrastructure on the same node as application workloads creates a noisy-neighbor risk and a single point of failure. Production deployments should use dedicated node pools with taints/tolerations for logging infrastructure.",
        "Elasticsearch's default configuration ships with security features disabled (no TLS, no authentication) in the OSS/basic tier. Without X-Pack security or OpenSearch security plugin enabled, any cluster-internal client can read, modify, or delete indices. Loki similarly lacks built-in authentication but is typically fronted by a gateway with tenant-aware auth."
      ],
      "supplyChain": "No image provenance, signing, or SBOM data is present in the experiment output. The Loki and Elasticsearch container images were likely pulled from Docker Hub (grafana/loki, elasticsearch/elasticsearch) without signature verification. Production deployments should: (1) use images from verified publishers or mirror them into a private Artifact Registry with vulnerability scanning enabled, (2) enforce image signing via cosign/Sigstore with a GKE Binary Authorization policy, and (3) generate and attest SBOMs for all deployed images. The Grafana project publishes cosign signatures for Loki images; Elastic publishes signed images for Elasticsearch 8.x+. Neither appears to have been verified in this experiment."
    },
    "capabilitiesMatrix": {
      "technologies": [
        "Loki",
        "Elasticsearch"
      ],
      "categories": [
        {
          "name": "Query Language",
          "capabilities": [
            {
              "name": "Full-text search",
              "values": {
                "Loki": "Limited — grep-style line filtering after label selection via LogQL",
                "Elasticsearch": "Full Lucene inverted-index search with BoolQuery, wildcards, fuzzy matching"
              }
            },
            {
              "name": "Structured field queries",
              "values": {
                "Loki": "Label-based selectors only; no field-level indexing of log content",
                "Elasticsearch": "Arbitrary field-level queries via KQL/Lucene, including nested and keyword fields"
              }
            },
            {
              "name": "Aggregations & analytics",
              "values": {
                "Loki": "LogQL metric queries (rate, count_over_time, quantile_over_time); limited to label dimensions",
                "Elasticsearch": "Full aggregation framework (terms, histograms, percentiles, pipeline aggregations)"
              }
            }
          ]
        },
        {
          "name": "Storage Architecture",
          "capabilities": [
            {
              "name": "Indexing strategy",
              "values": {
                "Loki": "Indexes only labels/stream metadata; stores compressed log chunks",
                "Elasticsearch": "Full inverted index over every document field; stored as Lucene segments on disk"
              }
            },
            {
              "name": "Storage backend flexibility",
              "values": {
                "Loki": "Object storage (S3/GCS), local filesystem, or BoltDB for index; chunk-based",
                "Elasticsearch": "Local persistent volumes with shard/replica model; snapshot to object storage for backup"
              }
            },
            {
              "name": "Disk footprint",
              "values": {
                "Loki": "Low — only label index + compressed chunks",
                "Elasticsearch": "High — full inverted index + stored source documents + transaction logs"
              }
            }
          ]
        },
        {
          "name": "Resource Efficiency",
          "capabilities": [
            {
              "name": "Minimum viable memory",
              "values": {
                "Loki": "~128–256 MB in single-binary mode",
                "Elasticsearch": "~1–2 GB JVM heap minimum per node"
              }
            },
            {
              "name": "CPU overhead at ingest",
              "values": {
                "Loki": "Low — no full-text indexing; primarily compression and label parsing",
                "Elasticsearch": "High — Lucene segment building, merging, and analysis pipelines"
              }
            },
            {
              "name": "Observed experiment footprint",
              "values": {
                "Loki": "Shared namespace: ~129 MB RSS, ~1.90 CPU-seconds cumulative (both stacks combined)",
                "Elasticsearch": "Shared namespace: ~129 MB RSS, ~1.90 CPU-seconds cumulative (both stacks combined)"
              }
            }
          ]
        },
        {
          "name": "Operational Complexity",
          "capabilities": [
            {
              "name": "Deployment footprint",
              "values": {
                "Loki": "Single binary or microservices mode; minimal Kubernetes manifests; pairs with Promtail/Alloy",
                "Elasticsearch": "Multi-component (data, master, coordinating nodes); requires JVM tuning; pairs with Fluentd/Beats"
              }
            },
            {
              "name": "Scaling model",
              "values": {
                "Loki": "Horizontally scalable readers/writers; stateless query path with shared object storage",
                "Elasticsearch": "Shard-based horizontal scaling; requires careful shard sizing and rebalancing"
              }
            },
            {
              "name": "Kubernetes-native integration",
              "values": {
                "Loki": "First-class Kubernetes label alignment; Helm chart with minimal config",
                "Elasticsearch": "ECK operator or Helm; more configuration surface area for JVM, PVCs, and resource limits"
              }
            }
          ]
        },
        {
          "name": "Cost Effectiveness",
          "capabilities": [
            {
              "name": "Infrastructure cost at small scale",
              "values": {
                "Loki": "Very low — can run on e2-small/e2-medium; minimal PV requirements",
                "Elasticsearch": "Moderate-to-high — needs dedicated memory for JVM heap; larger PVs for indices"
              }
            },
            {
              "name": "Operational cost",
              "values": {
                "Loki": "Low — fewer knobs to tune; simpler upgrades",
                "Elasticsearch": "Higher — JVM tuning, index lifecycle management, shard planning required"
              }
            }
          ]
        }
      ]
    },
    "body": {
      "methodology": "The experiment deployed both a Loki stack and an Elasticsearch stack into a single GKE cluster running on one e2-medium node (2 vCPUs, 4 GB RAM) under the namespace 'logging-comparison-cxpfp'. The total experiment duration was approximately 12 minutes 52 seconds (772.3 s), running from 15:50:40 to 16:03:32 UTC on 2026-02-11. Metrics were collected via cAdvisor at the container and namespace level, capturing cumulative CPU usage (in CPU-seconds) and memory working set (in bytes) as instant queries at the end of the experiment window. A validation workflow ('logging-comparison-validation') executed in the final 10 seconds of the run (16:03:17–16:03:27) and succeeded, confirming both stacks were operational. Importantly, the experiment collected aggregate namespace-level metrics rather than per-stack (per-pod or per-container) breakdowns, which means Loki and Elasticsearch resource consumption cannot be individually separated from the shared experiment namespace. The single-node, co-located deployment also means the two stacks competed for the same 2 vCPUs and 4 GB of RAM, introducing potential resource contention. No explicit log-generation load profile (e.g., lines per second) was recorded in the dataset, and no query-latency benchmarks were captured.",
      "results": "Over the ~12m52s experiment window, the combined experiment namespace consumed 1.898 cumulative CPU-seconds and a memory working set of approximately 129 MB (135,315,456 bytes). The GKE system namespace (gke-managed-cim) consumed an additional 0.637 CPU-seconds and ~30 MB (31,584,256 bytes), bringing total node utilization to 2.535 cumulative CPU-seconds and ~159 MB memory working set. The extremely low cumulative CPU figure — less than 2 CPU-seconds over nearly 13 minutes — indicates minimal sustained processing, averaging roughly 0.0025 cores across the experiment duration for the experiment namespace. Memory utilization at ~129 MB is well within the 4 GB node capacity, leaving substantial headroom. The validation workflow completed successfully in 10 seconds, confirming both logging stacks reached a ready state. The total estimated infrastructure cost for the run was $0.0043 USD (0.21 hours of e2-medium on-demand time).",
      "discussion": "The most significant limitation of this experiment is that namespace-level metrics aggregate both Loki and Elasticsearch into a single measurement, making it impossible to directly validate or refute the hypothesis that Loki uses significantly fewer resources. The combined ~129 MB memory footprint is surprisingly low for a namespace running both stacks — Elasticsearch alone typically requires 1–2 GB for its JVM heap — which suggests the workloads may have been in an early startup or near-idle state with minimal log ingestion volume. The cumulative CPU usage of ~1.9 seconds over ~13 minutes reinforces this interpretation: both stacks appear to have been largely idle rather than processing a sustained ingestion workload. Without per-pod metrics, a defined log generation rate, or query latency measurements, the data cannot meaningfully differentiate the two technologies' resource profiles. The successful validation confirms deployment correctness, which is a useful baseline, but the experiment does not yet answer its core research questions about comparative resource efficiency or query capability differences. The $0.0043 cost confirms that small-scale Kubernetes experiments are extremely cheap to run, supporting iterative experimentation. For real-world decision-making between Loki and Elasticsearch, this experiment would need a controlled ingestion load and isolated per-stack measurements."
    },
    "feedback": {
      "recommendations": [
        "Deploy Loki and Elasticsearch in separate namespaces (or separate node pools) so cAdvisor metrics can be attributed to each stack independently, enabling direct resource comparison.",
        "Introduce a controlled log-generation workload (e.g., a pod emitting N lines/sec of structured logs) with a known, configurable ingestion rate to move beyond idle-state measurement.",
        "Add query-latency benchmarks by running a standard set of operational queries (e.g., filter by label, full-text search for error keyword, aggregate counts over time window) against both stacks and recording response times.",
        "Extend the experiment duration to at least 30–60 minutes with sustained load to capture steady-state behavior, index/chunk compaction effects, and memory stabilization (especially Elasticsearch JVM GC patterns)."
      ],
      "experimentDesign": [
        "Collect per-pod or per-container metrics rather than namespace-level aggregates, enabling precise attribution of CPU, memory, and network I/O to each technology stack and its sidecars (Promtail, Fluentd, etc.).",
        "Record time-series metrics (rate queries over the experiment window) rather than only instant endpoint snapshots, to capture resource usage trends, ingestion ramp-up behavior, and potential garbage collection spikes.",
        "Include storage metrics (persistent volume usage, write IOPS, bytes written) to compare Loki's chunk-based compression against Elasticsearch's full inverted-index storage overhead."
      ]
    },
    "summary": "Analysis incomplete",
    "recommendations": [
      "Deploy Loki and Elasticsearch in separate namespaces (or separate node pools) so cAdvisor metrics can be attributed to each stack independently, enabling direct resource comparison.",
      "Introduce a controlled log-generation workload (e.g., a pod emitting N lines/sec of structured logs) with a known, configurable ingestion rate to move beyond idle-state measurement.",
      "Add query-latency benchmarks by running a standard set of operational queries (e.g., filter by label, full-text search for error keyword, aggregate counts over time window) against both stacks and recording response times.",
      "Extend the experiment duration to at least 30–60 minutes with sustained load to capture steady-state behavior, index/chunk compaction effects, and memory stabilization (especially Elasticsearch JVM GC patterns)."
    ],
    "generatedAt": "2026-02-11T16:07:03Z",
    "model": "claude-opus-4-6"
  }
}
